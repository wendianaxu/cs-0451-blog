<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.189">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2023-05-14">
<meta name="description" content="Classify music by genre using neural networks in PyTorch.">

<title>CSCI 0451 Blog - Diana Xu - Deep Music Genre Classification</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<style>
    .quarto-title-block .quarto-title-banner {
      color: white;
background-image: url(../../img/landscape.png);
background-size: cover;
    }
    </style>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
        <script type="text/javascript">
        window.PlotlyConfig = {MathJaxConfig: 'local'};
        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: "STIX-Web"}});}
        if (typeof require !== 'undefined') {
        require.undef("plotly");
        requirejs.config({
            paths: {
                'plotly': ['https://cdn.plot.ly/plotly-2.18.2.min']
            }
        });
        require(['plotly'], function(Plotly) {
            window._Plotly = Plotly;
        });
        }
        </script>
        


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">CSCI 0451 Blog - Diana Xu</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html">About</a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/"><i class="bi bi-github" role="img">
</i> 
 </a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com"><i class="bi bi-twitter" role="img">
</i> 
 </a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Deep Music Genre Classification</h1>
                  <div>
        <div class="description">
          <p>Classify music by genre using neural networks in PyTorch.</p>
        </div>
      </div>
                </div>
  </div>
    
  
  <div class="quarto-title-meta">

      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">May 14, 2023</p>
      </div>
    </div>
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="deep-music-genre-classification" class="level1">
<h1>Deep Music Genre Classification</h1>
<p>In this blog post, I will use PyTorch to train neural networks that classify music genre based on lyrics and some engineered features that describe song attributes.</p>
<section id="data-preparation" class="level2">
<h2 class="anchored" data-anchor-id="data-preparation">Data Preparation</h2>
<p>To start off, I will read the data and turn them into <code>Dataset</code> objects that can be accessed by PyTorch.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># download dataset</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>url <span class="op">=</span> <span class="st">"https://raw.githubusercontent.com/PhilChodrow/PIC16B/master/datasets/tcc_ceds_music.csv"</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(url)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>engineered_features <span class="op">=</span> [<span class="st">'dating'</span>, <span class="st">'violence'</span>, <span class="st">'world/life'</span>, <span class="st">'night/time'</span>,<span class="st">'shake the audience'</span>,<span class="st">'family/gospel'</span>, <span class="st">'romantic'</span>, <span class="st">'communication'</span>,<span class="st">'obscene'</span>, <span class="st">'music'</span>, <span class="st">'movement/places'</span>, <span class="st">'light/visual perceptions'</span>,<span class="st">'family/spiritual'</span>, <span class="st">'like/girls'</span>, <span class="st">'sadness'</span>, <span class="st">'feelings'</span>, <span class="st">'danceability'</span>,<span class="st">'loudness'</span>, <span class="st">'acousticness'</span>, <span class="st">'instrumentalness'</span>, <span class="st">'valence'</span>, <span class="st">'energy'</span>] </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-outputid="ff17d7bf-7dc1-473e-c1ec-b53b07ce0ef8">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>df.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="4">


  <div id="df-df444f4e-0132-4448-8889-30227f094ee1">
    <div class="colab-df-container">
      <div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>Unnamed: 0</th>
      <th>artist_name</th>
      <th>track_name</th>
      <th>release_date</th>
      <th>genre</th>
      <th>lyrics</th>
      <th>len</th>
      <th>dating</th>
      <th>violence</th>
      <th>world/life</th>
      <th>...</th>
      <th>sadness</th>
      <th>feelings</th>
      <th>danceability</th>
      <th>loudness</th>
      <th>acousticness</th>
      <th>instrumentalness</th>
      <th>valence</th>
      <th>energy</th>
      <th>topic</th>
      <th>age</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>mukesh</td>
      <td>mohabbat bhi jhoothi</td>
      <td>1950</td>
      <td>pop</td>
      <td>hold time feel break feel untrue convince spea...</td>
      <td>95</td>
      <td>0.000598</td>
      <td>0.063746</td>
      <td>0.000598</td>
      <td>...</td>
      <td>0.380299</td>
      <td>0.117175</td>
      <td>0.357739</td>
      <td>0.454119</td>
      <td>0.997992</td>
      <td>0.901822</td>
      <td>0.339448</td>
      <td>0.137110</td>
      <td>sadness</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>4</td>
      <td>frankie laine</td>
      <td>i believe</td>
      <td>1950</td>
      <td>pop</td>
      <td>believe drop rain fall grow believe darkest ni...</td>
      <td>51</td>
      <td>0.035537</td>
      <td>0.096777</td>
      <td>0.443435</td>
      <td>...</td>
      <td>0.001284</td>
      <td>0.001284</td>
      <td>0.331745</td>
      <td>0.647540</td>
      <td>0.954819</td>
      <td>0.000002</td>
      <td>0.325021</td>
      <td>0.263240</td>
      <td>world/life</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>6</td>
      <td>johnnie ray</td>
      <td>cry</td>
      <td>1950</td>
      <td>pop</td>
      <td>sweetheart send letter goodbye secret feel bet...</td>
      <td>24</td>
      <td>0.002770</td>
      <td>0.002770</td>
      <td>0.002770</td>
      <td>...</td>
      <td>0.002770</td>
      <td>0.225422</td>
      <td>0.456298</td>
      <td>0.585288</td>
      <td>0.840361</td>
      <td>0.000000</td>
      <td>0.351814</td>
      <td>0.139112</td>
      <td>music</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>10</td>
      <td>pérez prado</td>
      <td>patricia</td>
      <td>1950</td>
      <td>pop</td>
      <td>kiss lips want stroll charm mambo chacha merin...</td>
      <td>54</td>
      <td>0.048249</td>
      <td>0.001548</td>
      <td>0.001548</td>
      <td>...</td>
      <td>0.225889</td>
      <td>0.001548</td>
      <td>0.686992</td>
      <td>0.744404</td>
      <td>0.083935</td>
      <td>0.199393</td>
      <td>0.775350</td>
      <td>0.743736</td>
      <td>romantic</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>12</td>
      <td>giorgos papadopoulos</td>
      <td>apopse eida oneiro</td>
      <td>1950</td>
      <td>pop</td>
      <td>till darling till matter know till dream live ...</td>
      <td>48</td>
      <td>0.001350</td>
      <td>0.001350</td>
      <td>0.417772</td>
      <td>...</td>
      <td>0.068800</td>
      <td>0.001350</td>
      <td>0.291671</td>
      <td>0.646489</td>
      <td>0.975904</td>
      <td>0.000246</td>
      <td>0.597073</td>
      <td>0.394375</td>
      <td>romantic</td>
      <td>1.0</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 31 columns</p>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-df444f4e-0132-4448-8889-30227f094ee1')" title="Convert this dataframe to an interactive table." style="display:none;">
        
  <svg xmlns="http://www.w3.org/2000/svg" height="24px" viewbox="0 0 24 24" width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"></path>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"></path><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"></path>
  </svg>
      </button>
      
  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-df444f4e-0132-4448-8889-30227f094ee1 button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-df444f4e-0132-4448-8889-30227f094ee1');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>
  
</div>
</div>
<p>I will look at three genres: hip hop, country, and rock. Since the <code>genre</code> labels are strings, I will encode them with integers.</p>
<div class="cell" data-outputid="46cf39bd-e24b-4cf0-d0d6-14c65c3c4186">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>genres <span class="op">=</span> {</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">"hip hop"</span>: <span class="dv">0</span>, </span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">"country"</span>: <span class="dv">1</span>, </span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">"rock"</span>: <span class="dv">2</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> df[df[<span class="st">"genre"</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: x <span class="kw">in</span> genres.keys())]</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>df[<span class="st">"genre"</span>] <span class="op">=</span> df[<span class="st">"genre"</span>].<span class="bu">apply</span>(genres.get)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>df.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  df["genre"] = df["genre"].apply(genres.get)</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="2">


  <div id="df-265fa569-212b-4d81-b952-656e971b9906">
    <div class="colab-df-container">
      <div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>Unnamed: 0</th>
      <th>artist_name</th>
      <th>track_name</th>
      <th>release_date</th>
      <th>genre</th>
      <th>lyrics</th>
      <th>len</th>
      <th>dating</th>
      <th>violence</th>
      <th>world/life</th>
      <th>...</th>
      <th>sadness</th>
      <th>feelings</th>
      <th>danceability</th>
      <th>loudness</th>
      <th>acousticness</th>
      <th>instrumentalness</th>
      <th>valence</th>
      <th>energy</th>
      <th>topic</th>
      <th>age</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>7042</th>
      <td>20290</td>
      <td>lefty frizzell</td>
      <td>if you've got the money i've got the time</td>
      <td>1950</td>
      <td>1</td>
      <td>money time honky tonkin time night spot dance ...</td>
      <td>59</td>
      <td>0.022813</td>
      <td>0.001074</td>
      <td>0.001074</td>
      <td>...</td>
      <td>0.001074</td>
      <td>0.001074</td>
      <td>0.523448</td>
      <td>0.655488</td>
      <td>0.833333</td>
      <td>0.000095</td>
      <td>0.955688</td>
      <td>0.392373</td>
      <td>night/time</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>7043</th>
      <td>20292</td>
      <td>lefty frizzell</td>
      <td>i want to be with you always</td>
      <td>1950</td>
      <td>1</td>
      <td>lose blue heart stay go sing song wanna dear n...</td>
      <td>24</td>
      <td>0.002288</td>
      <td>0.002288</td>
      <td>0.002288</td>
      <td>...</td>
      <td>0.205663</td>
      <td>0.091285</td>
      <td>0.705405</td>
      <td>0.594980</td>
      <td>0.777108</td>
      <td>0.000229</td>
      <td>0.717642</td>
      <td>0.226202</td>
      <td>music</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>7044</th>
      <td>20293</td>
      <td>lefty frizzell</td>
      <td>how long will it take (to stop loving you)</td>
      <td>1950</td>
      <td>1</td>
      <td>long count star long climb mar long world stan...</td>
      <td>34</td>
      <td>0.001595</td>
      <td>0.001595</td>
      <td>0.119458</td>
      <td>...</td>
      <td>0.001595</td>
      <td>0.001595</td>
      <td>0.780136</td>
      <td>0.583109</td>
      <td>0.892570</td>
      <td>0.000052</td>
      <td>0.706307</td>
      <td>0.180155</td>
      <td>night/time</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>7045</th>
      <td>20297</td>
      <td>lefty frizzell</td>
      <td>look what thoughts will do</td>
      <td>1950</td>
      <td>1</td>
      <td>think love think love look thoughts today wear...</td>
      <td>44</td>
      <td>0.001253</td>
      <td>0.001253</td>
      <td>0.308536</td>
      <td>...</td>
      <td>0.001253</td>
      <td>0.039916</td>
      <td>0.716235</td>
      <td>0.609697</td>
      <td>0.734939</td>
      <td>0.000000</td>
      <td>0.703215</td>
      <td>0.249226</td>
      <td>world/life</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>7046</th>
      <td>20300</td>
      <td>lefty frizzell</td>
      <td>treasure untold</td>
      <td>1950</td>
      <td>1</td>
      <td>dream eye blue love forever long dear want nea...</td>
      <td>36</td>
      <td>0.001698</td>
      <td>0.001698</td>
      <td>0.140714</td>
      <td>...</td>
      <td>0.001698</td>
      <td>0.001698</td>
      <td>0.703238</td>
      <td>0.648848</td>
      <td>0.685743</td>
      <td>0.000000</td>
      <td>0.384790</td>
      <td>0.219195</td>
      <td>romantic</td>
      <td>1.0</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 31 columns</p>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-265fa569-212b-4d81-b952-656e971b9906')" title="Convert this dataframe to an interactive table." style="display:none;">
        
  <svg xmlns="http://www.w3.org/2000/svg" height="24px" viewbox="0 0 24 24" width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"></path>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"></path><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"></path>
  </svg>
      </button>
      
  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-265fa569-212b-4d81-b952-656e971b9906 button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-265fa569-212b-4d81-b952-656e971b9906');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>
  
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create Dataset class using features of interest</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> Dataset, DataLoader</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>engineered_feature_indices <span class="op">=</span> [df.columns.get_loc(col_name) <span class="cf">for</span> col_name <span class="kw">in</span> engineered_features]</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> TextDataFromDF(Dataset):</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, df):</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.df <span class="op">=</span> df</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, index): <span class="co"># get lyrics, engineered features, and genre labels</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>        lyrics <span class="op">=</span> <span class="va">self</span>.df.iloc[index, <span class="dv">5</span>]</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>        engineered <span class="op">=</span> <span class="va">self</span>.df.iloc[index, engineered_feature_indices].tolist()</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>        labels <span class="op">=</span> <span class="va">self</span>.df.iloc[index, <span class="dv">4</span>]</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> lyrics, engineered, labels</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.df)                </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># train-test split</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>df_train, df_test <span class="op">=</span> train_test_split(df,shuffle <span class="op">=</span> <span class="va">True</span>, test_size <span class="op">=</span> <span class="fl">0.2</span>)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>train_data <span class="op">=</span> TextDataFromDF(df_train)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>test_data  <span class="op">=</span> TextDataFromDF(df_test)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Each element of the dataset is a tuple containing the lyrics, engineered features, and integer labels.</p>
<div class="cell" data-outputid="bc2270a2-bb34-49cd-e9c4-7d1e32901922">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>train_data[<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="5">
<pre><code>('feet feet fight missiles spear thirtyone seventeen soldier thousand years catholic hindu atheist buddhist baptist know shouldn kill know kill friend fight fight fight fight russians fight japan think fight democracy fight reds say peace decide live see write wall hitler condemn dachau stand give body weapon kill universal soldier blame order come away come brothers',
 [0.0015037595085359,
  0.7092493819987846,
  0.0015037595507576,
  0.0015037594168285,
  0.0015037594030349,
  0.0015037594155051,
  0.0015037594066574,
  0.177928468124219,
  0.001503759427317,
  0.0015037594637539,
  0.0015037594607704,
  0.0015037594349133,
  0.0502747402292334,
  0.0015037594141537,
  0.0399910181175007,
  0.0015037594017228,
  0.4877071374417849,
  0.5141142989000845,
  0.780120261164921,
  0.0,
  0.414674361088211,
  0.208183478803342],
 2)</code></pre>
</div>
</div>
<section id="text-vectorization" class="level3">
<h3 class="anchored" data-anchor-id="text-vectorization">Text Vectorization</h3>
<p>Next, I will vectorize the lyrics text using similar approaches as our text classification lecture notes.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create tokenizer and vocabulary</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchtext.data.utils <span class="im">import</span> get_tokenizer</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchtext.vocab <span class="im">import</span> build_vocab_from_iterator</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> get_tokenizer(<span class="st">'basic_english'</span>)</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> yield_tokens(data_iter): </span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>  <span class="co">'''</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a><span class="co">  loop through our dataset and tokenize lyrics</span></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a><span class="co">  '''</span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> lyrics, _, _ <span class="kw">in</span> data_iter:</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>      <span class="cf">yield</span> tokenizer(lyrics)</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>vocab <span class="op">=</span> build_vocab_from_iterator(yield_tokens(train_data), specials<span class="op">=</span>[<span class="st">"&lt;unk&gt;"</span>], min_freq <span class="op">=</span> <span class="dv">50</span>) <span class="co"># only include tokens that appeared at least 50 times</span></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>vocab.set_default_index(vocab[<span class="st">"&lt;unk&gt;"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>To check if the tokenizer and vocabulary are working correctly:</p>
<div class="cell" data-outputid="31d827fa-2c66-4faa-9dca-48a600135e9e">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>tokenized <span class="op">=</span> tokenizer(train_data[<span class="dv">100</span>][<span class="dv">0</span>])</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(tokenized)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(vocab(tokenized))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>['peepbo', 'peachblow', 'pandour', 'pompadour', 'pale', 'leaf', 'pink', 'sweet', 'deer', 'peepbo', 'animal', 'peepbo', 'deer', 'peepbo', 'animal', 'peepbo', 'deer', 'peepbo', 'animal', 'peepbo', 'deer', 'peepbo', 'animal', 'peepbo', 'peepbo', 'peachblow', 'pandour', 'pompadour', 'pale', 'leaf', 'pink', 'sweet', 'deer', 'peepbo', 'animal', 'peepbo', 'deer', 'peepbo', 'animal', 'peepbo', 'deer', 'peepbo', 'animal', 'peepbo', 'deer', 'peepbo', 'animal', 'peepbo', 'peepbo', 'peachblow', 'pandour', 'pompadour', 'pale', 'leaf', 'pink', 'sweet', 'deer', 'peepbo', 'animal', 'peepbo', 'deer', 'peepbo', 'animal', 'peepbo', 'deer', 'peepbo', 'animal', 'peepbo', 'deer', 'peepbo', 'animal', 'peepbo', 'predaria', 'predo', 'pradari', 'peepbo', 'peachblow', 'pandour', 'pompadour', 'pale', 'leaf', 'pink', 'sweet', 'peepbo', 'peachblow', 'pandour', 'pompadour', 'pale', 'leaf', 'pink', 'sweet', 'deer', 'peepbo', 'animal', 'peepbo', 'deer', 'peepbo', 'animal', 'peepbo', 'deer', 'peepbo', 'animal', 'peepbo', 'deer', 'peepbo', 'animal', 'peepbo', 'peepbo', 'peachblow', 'pandour']
[0, 0, 0, 0, 0, 0, 0, 66, 0, 0, 798, 0, 0, 0, 798, 0, 0, 0, 798, 0, 0, 0, 798, 0, 0, 0, 0, 0, 0, 0, 0, 66, 0, 0, 798, 0, 0, 0, 798, 0, 0, 0, 798, 0, 0, 0, 798, 0, 0, 0, 0, 0, 0, 0, 0, 66, 0, 0, 798, 0, 0, 0, 798, 0, 0, 0, 798, 0, 0, 0, 798, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 66, 0, 0, 0, 0, 0, 0, 0, 66, 0, 0, 798, 0, 0, 0, 798, 0, 0, 0, 798, 0, 0, 0, 798, 0, 0, 0, 0]</code></pre>
</div>
</div>
</section>
<section id="batch-collation" class="level3">
<h3 class="anchored" data-anchor-id="batch-collation">Batch Collation</h3>
<p>For the last step of data preparation, I will create <code>DataLoader</code>s that pass batches of data to the training loop.</p>
<p>Before creating the <code>DataLoader</code>s, I will represent the lyrics with integers, and pad them to make them have uniform lengths.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># determine max length of the lyrics</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>max_len <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> data <span class="kw">in</span> train_data:</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>  length <span class="op">=</span> <span class="bu">len</span>(data[<span class="dv">0</span>].split())</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> length <span class="op">&gt;</span> max_len:</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>    max_len <span class="op">=</span> length</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># represent lyrics with integers and pad them</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>num_tokens <span class="op">=</span> <span class="bu">len</span>(vocab.get_itos())</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> text_pipeline(x):</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>    tokens <span class="op">=</span> vocab(tokenizer(x))</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> torch.zeros(max_len, dtype<span class="op">=</span>torch.int64) <span class="op">+</span> num_tokens</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>    y[<span class="dv">0</span>:<span class="bu">len</span>(tokens)] <span class="op">=</span> torch.tensor(tokens,dtype<span class="op">=</span>torch.int64)</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> y</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">"cuda"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> collate_batch(batch):</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>    lyrics_list, label_list <span class="op">=</span> [], [],</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>    engineered_tuple <span class="op">=</span> ()</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> (_lyrics, _engineered, _label) <span class="kw">in</span> batch:</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>      <span class="co"># add lyrics to list</span></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>      processed_lyrics <span class="op">=</span> text_pipeline(_lyrics)</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>      lyrics_list.append(processed_lyrics)</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>      <span class="co"># add engineered features to tuple</span></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>      engineered_tuple <span class="op">+=</span> (_engineered, )</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>      <span class="co"># add label to list</span></span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>      label_list.append(_label)</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>    lyrics_tensor <span class="op">=</span> torch.stack(lyrics_list)</span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>    engineered_tensor <span class="op">=</span> torch.tensor(engineered_tuple, dtype<span class="op">=</span>torch.float32)</span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>    label_tensor <span class="op">=</span> torch.tensor(label_list, dtype<span class="op">=</span>torch.int64)</span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> lyrics_tensor.to(device), engineered_tensor.to(device), label_tensor.to(device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>train_loader <span class="op">=</span> DataLoader(train_data, batch_size<span class="op">=</span><span class="dv">16</span>, shuffle<span class="op">=</span><span class="va">True</span>, collate_fn<span class="op">=</span>collate_batch)</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>test_loader <span class="op">=</span> DataLoader(test_data, batch_size<span class="op">=</span><span class="dv">16</span>, shuffle<span class="op">=</span><span class="va">True</span>, collate_fn<span class="op">=</span>collate_batch)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Each batch of data returns three tensors: the lyrics, engineered features, and labels.</p>
<div class="cell" data-outputid="6751f6f4-8e52-4739-cf92-f8c49e6c6e02">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="bu">next</span>(<span class="bu">iter</span>(train_loader))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="13">
<pre><code>(tensor([[ 190,    5,  190,  ..., 1372, 1372, 1372],
         [  35,  137,   62,  ..., 1372, 1372, 1372],
         [  84,   27,   27,  ..., 1372, 1372, 1372],
         ...,
         [   0,   27,  257,  ..., 1372, 1372, 1372],
         [  13,    3,    0,  ..., 1372, 1372, 1372],
         [  36,   57, 1041,  ..., 1372, 1372, 1372]], device='cuda:0'),
 tensor([[1.2563e-01, 8.2237e-04, 8.2237e-04, 7.9477e-02, 7.4051e-02, 8.2237e-04,
          1.8140e-02, 1.6188e-01, 8.2237e-04, 8.2237e-04, 8.2237e-04, 8.2237e-04,
          8.2237e-04, 8.2237e-04, 7.4127e-02, 4.5682e-01, 5.1045e-01, 7.6297e-01,
          7.2289e-01, 1.3158e-05, 7.5371e-01, 6.0359e-01],
         [1.3158e-03, 1.3158e-03, 1.2600e-01, 1.3158e-03, 1.3158e-03, 1.3158e-03,
          2.4707e-01, 1.3158e-03, 1.3158e-03, 1.3158e-03, 1.3158e-03, 1.3158e-03,
          1.3158e-03, 1.3158e-03, 6.0587e-01, 1.3158e-03, 3.4907e-01, 7.5292e-01,
          2.6606e-01, 2.6923e-05, 8.4852e-01, 6.2661e-01],
         [5.9137e-04, 5.4297e-01, 5.9137e-04, 5.9137e-04, 1.5789e-01, 4.0467e-02,
          5.9137e-04, 5.9137e-04, 5.9137e-04, 5.9137e-04, 5.9137e-04, 5.9137e-04,
          5.9137e-04, 5.9137e-04, 9.2487e-02, 5.9137e-04, 4.7688e-01, 6.6959e-01,
          1.1144e-02, 6.1741e-05, 3.2296e-01, 8.5185e-01],
         [6.2902e-02, 3.0960e-03, 3.0960e-03, 3.0960e-03, 3.0960e-03, 3.0960e-03,
          3.2338e-01, 3.0960e-03, 3.0960e-03, 3.0960e-03, 3.0960e-03, 8.1015e-02,
          1.3279e-01, 3.0960e-03, 1.8003e-01, 3.0960e-03, 3.2091e-01, 6.1941e-01,
          9.1767e-01, 4.1093e-03, 4.3941e-01, 3.2430e-01],
         [1.8149e-03, 1.9740e-01, 1.8149e-03, 1.8149e-03, 1.8149e-03, 1.8149e-03,
          1.8149e-03, 1.8149e-03, 1.8149e-03, 1.8149e-03, 3.3243e-01, 1.8149e-03,
          3.8736e-02, 1.8149e-03, 4.0422e-01, 1.8149e-03, 4.0214e-01, 4.4807e-01,
          7.0884e-01, 9.7368e-03, 1.7972e-01, 6.6265e-01],
         [5.7208e-04, 5.7208e-04, 5.7208e-04, 5.7208e-04, 5.7208e-04, 1.2899e-02,
          1.3940e-02, 7.4754e-02, 4.6015e-01, 5.7208e-04, 3.5104e-02, 5.5986e-02,
          5.7208e-04, 5.7208e-04, 2.9759e-01, 5.7208e-04, 6.7075e-01, 7.0010e-01,
          7.0481e-02, 0.0000e+00, 8.1451e-01, 7.1971e-01],
         [1.8797e-03, 3.7041e-01, 1.8797e-03, 8.4183e-02, 1.8797e-03, 1.8797e-03,
          3.2794e-02, 1.5335e-01, 1.8797e-03, 1.8797e-03, 1.8797e-03, 1.8233e-01,
          1.8797e-03, 1.8797e-03, 1.8797e-03, 6.3712e-02, 3.2308e-01, 6.6167e-01,
          9.1456e-03, 6.7510e-03, 2.1476e-01, 9.4995e-01],
         [2.5063e-03, 2.5063e-03, 2.5063e-03, 2.5063e-03, 2.5063e-03, 2.5063e-03,
          7.8521e-02, 4.6253e-01, 2.5063e-03, 2.5063e-03, 2.5063e-03, 2.5063e-03,
          6.0003e-02, 2.5063e-03, 3.1478e-01, 2.5063e-03, 6.0576e-01, 5.0617e-01,
          8.9859e-01, 3.4312e-05, 4.8784e-01, 1.3311e-01],
         [8.9206e-04, 8.9206e-04, 8.9206e-04, 8.9206e-04, 8.9206e-04, 8.2017e-02,
          9.4692e-02, 6.1993e-02, 5.8138e-02, 8.9206e-04, 2.1777e-01, 8.9206e-04,
          8.9206e-04, 8.9206e-04, 4.2052e-01, 8.9206e-04, 4.4330e-01, 7.4281e-01,
          7.7711e-05, 1.2348e-01, 6.3108e-01, 7.3573e-01],
         [1.0121e-03, 1.0121e-03, 1.0121e-03, 1.0121e-03, 1.0121e-03, 1.0121e-03,
          1.0121e-03, 1.0121e-03, 4.1138e-01, 1.0121e-03, 3.1253e-01, 1.0121e-03,
          1.0121e-03, 1.0121e-03, 1.5268e-01, 1.0823e-01, 4.3247e-01, 5.3480e-01,
          2.3494e-01, 0.0000e+00, 7.2692e-01, 6.5364e-01],
         [2.3923e-03, 2.3923e-03, 2.3923e-03, 2.3923e-03, 4.7847e-02, 9.7432e-02,
          2.3923e-03, 2.3923e-03, 5.6130e-02, 2.3923e-03, 2.3923e-03, 9.4145e-02,
          2.3923e-03, 2.3923e-03, 5.7453e-01, 2.3923e-03, 6.7183e-01, 7.2410e-01,
          4.9196e-02, 2.0445e-02, 9.7012e-01, 8.3983e-01],
         [1.5949e-03, 1.5949e-03, 1.5949e-03, 1.5949e-03, 1.5949e-03, 1.5949e-03,
          6.0752e-02, 1.5949e-03, 1.5949e-03, 4.3576e-01, 3.8856e-01, 1.5949e-03,
          1.5949e-03, 1.5949e-03, 1.5949e-03, 2.9081e-02, 3.2850e-01, 7.1048e-01,
          1.3253e-01, 8.6943e-03, 4.5486e-01, 7.1971e-01],
         [1.4620e-03, 3.9148e-01, 1.4620e-03, 1.4620e-03, 1.4620e-03, 1.4620e-03,
          1.4620e-03, 1.4620e-03, 1.4620e-03, 1.4620e-03, 1.4620e-03, 1.1630e-01,
          7.2153e-02, 1.4620e-03, 3.9813e-01, 1.4620e-03, 4.9637e-01, 6.0718e-01,
          9.1968e-01, 0.0000e+00, 2.1373e-01, 1.4512e-01],
         [1.5480e-03, 1.5480e-03, 1.5480e-03, 8.1524e-02, 1.5480e-03, 1.5480e-03,
          3.1767e-02, 9.7475e-02, 1.5480e-03, 1.5480e-03, 1.5480e-03, 1.5480e-03,
          1.5480e-03, 1.5480e-03, 7.0748e-01, 1.5480e-03, 5.7327e-01, 5.8839e-01,
          8.8153e-01, 2.1559e-02, 5.8574e-01, 2.8927e-01],
         [1.7544e-03, 1.7544e-03, 3.9870e-01, 2.8864e-01, 1.7544e-03, 1.7544e-03,
          1.7544e-03, 1.7544e-03, 1.7544e-03, 1.7544e-03, 1.3114e-01, 1.5521e-01,
          1.7544e-03, 1.7544e-03, 1.7544e-03, 1.7544e-03, 4.5197e-01, 6.3493e-01,
          8.0321e-01, 1.3462e-04, 5.8883e-01, 3.7335e-01],
         [1.0965e-03, 1.0965e-03, 5.9253e-02, 1.0965e-03, 1.0965e-03, 8.3288e-02,
          1.0965e-03, 1.0965e-03, 1.0965e-03, 1.0965e-03, 1.2537e-01, 1.0965e-03,
          1.0965e-03, 1.1992e-01, 4.6606e-01, 1.3185e-01, 5.4186e-01, 7.6104e-01,
          2.7107e-02, 4.5445e-03, 4.8475e-01, 9.7097e-01]], device='cuda:0'),
 tensor([1, 1, 2, 2, 2, 1, 2, 1, 2, 2, 2, 1, 1, 1, 1, 2], device='cuda:0'))</code></pre>
</div>
</div>
</section>
</section>
<section id="training-models" class="level2">
<h2 class="anchored" data-anchor-id="training-models">Training Models</h2>
<p>I will train three neural networks on the data: 1. A model that only uses the lyrics as features. 2. A model that only uses the engineered features. 3. A model taht uses both the lyrics and the engineered features.</p>
<p>Before building the models, I will define the training and testing loops.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co"># training loop</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train(dataloader, feature_choice <span class="op">=</span> <span class="st">"l"</span>, k_epochs <span class="op">=</span> <span class="dv">1</span>, print_every <span class="op">=</span> <span class="dv">50</span>):</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># select model based on feature choice</span></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> feature_choice <span class="op">==</span> <span class="st">"l"</span>: <span class="co"># lyrics only model</span></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>      model <span class="op">=</span> lyrics_model</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> feature_choice <span class="op">==</span> <span class="st">"e"</span>: <span class="co"># engineered features only</span></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>      model <span class="op">=</span> engineered_model</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> feature_choice <span class="op">==</span> <span class="st">"b"</span>: <span class="co"># both lyrics and engineered features</span></span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>      model <span class="op">=</span> both_model</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> torch.optim.Adam(model.parameters(), lr<span class="op">=</span>learning_rate)</span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>    loss_fn <span class="op">=</span> torch.nn.CrossEntropyLoss()</span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a>    log_interval <span class="op">=</span> <span class="dv">300</span></span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(k_epochs): </span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a>      running_loss <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a>      <span class="cf">for</span> idx, (lyrics, engineered, label) <span class="kw">in</span> <span class="bu">enumerate</span>(dataloader):</span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a>          <span class="co"># for calculating accuracy</span></span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a>          correct, total <span class="op">=</span> <span class="dv">0</span>, <span class="dv">0</span></span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-24"><a href="#cb19-24" aria-hidden="true" tabindex="-1"></a>          <span class="co"># zero gradients</span></span>
<span id="cb19-25"><a href="#cb19-25" aria-hidden="true" tabindex="-1"></a>          optimizer.zero_grad()</span>
<span id="cb19-26"><a href="#cb19-26" aria-hidden="true" tabindex="-1"></a>          <span class="co"># form prediction on batch, using chosen features and models</span></span>
<span id="cb19-27"><a href="#cb19-27" aria-hidden="true" tabindex="-1"></a>          <span class="cf">if</span> feature_choice <span class="op">==</span> <span class="st">"l"</span>: <span class="co"># lyrics only model</span></span>
<span id="cb19-28"><a href="#cb19-28" aria-hidden="true" tabindex="-1"></a>            predicted_label <span class="op">=</span> model(lyrics)</span>
<span id="cb19-29"><a href="#cb19-29" aria-hidden="true" tabindex="-1"></a>          <span class="cf">elif</span> feature_choice <span class="op">==</span> <span class="st">"e"</span>: <span class="co"># engineered features only</span></span>
<span id="cb19-30"><a href="#cb19-30" aria-hidden="true" tabindex="-1"></a>            predicted_label <span class="op">=</span> model(engineered)</span>
<span id="cb19-31"><a href="#cb19-31" aria-hidden="true" tabindex="-1"></a>          <span class="cf">elif</span> feature_choice <span class="op">==</span> <span class="st">"b"</span>: <span class="co"># both lyrics and engineered features</span></span>
<span id="cb19-32"><a href="#cb19-32" aria-hidden="true" tabindex="-1"></a>            predicted_label <span class="op">=</span> model(lyrics, engineered)</span>
<span id="cb19-33"><a href="#cb19-33" aria-hidden="true" tabindex="-1"></a>          </span>
<span id="cb19-34"><a href="#cb19-34" aria-hidden="true" tabindex="-1"></a>          <span class="co"># evaluate loss on prediction</span></span>
<span id="cb19-35"><a href="#cb19-35" aria-hidden="true" tabindex="-1"></a>          loss <span class="op">=</span> loss_fn(predicted_label, label)</span>
<span id="cb19-36"><a href="#cb19-36" aria-hidden="true" tabindex="-1"></a>          <span class="co"># compute gradient</span></span>
<span id="cb19-37"><a href="#cb19-37" aria-hidden="true" tabindex="-1"></a>          loss.backward()</span>
<span id="cb19-38"><a href="#cb19-38" aria-hidden="true" tabindex="-1"></a>          <span class="co"># take an optimization step</span></span>
<span id="cb19-39"><a href="#cb19-39" aria-hidden="true" tabindex="-1"></a>          optimizer.step()</span>
<span id="cb19-40"><a href="#cb19-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-41"><a href="#cb19-41" aria-hidden="true" tabindex="-1"></a>          <span class="co"># update running loss</span></span>
<span id="cb19-42"><a href="#cb19-42" aria-hidden="true" tabindex="-1"></a>          running_loss <span class="op">+=</span> loss.item()</span>
<span id="cb19-43"><a href="#cb19-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-44"><a href="#cb19-44" aria-hidden="true" tabindex="-1"></a>          <span class="co"># for printing accuracy</span></span>
<span id="cb19-45"><a href="#cb19-45" aria-hidden="true" tabindex="-1"></a>          correct <span class="op">+=</span> (predicted_label.argmax(<span class="dv">1</span>) <span class="op">==</span> label).<span class="bu">sum</span>().item()</span>
<span id="cb19-46"><a href="#cb19-46" aria-hidden="true" tabindex="-1"></a>          total   <span class="op">+=</span> label.size(<span class="dv">0</span>)</span>
<span id="cb19-47"><a href="#cb19-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-48"><a href="#cb19-48" aria-hidden="true" tabindex="-1"></a>          <span class="cf">if</span> idx <span class="op">%</span> print_every <span class="op">==</span> print_every <span class="op">-</span> <span class="dv">1</span>:    </span>
<span id="cb19-49"><a href="#cb19-49" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f'[epoch: </span><span class="sc">{</span>epoch <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">, batches: </span><span class="sc">{</span>idx <span class="op">+</span> <span class="dv">1</span><span class="sc">:5d}</span><span class="ss">], loss: </span><span class="sc">{</span>running_loss <span class="op">/</span> print_every<span class="sc">:.3f}</span><span class="ss">, accuracy:</span><span class="sc">{</span>correct<span class="op">/</span>total<span class="sc">:.3f}</span><span class="ss">'</span>)</span>
<span id="cb19-50"><a href="#cb19-50" aria-hidden="true" tabindex="-1"></a>            running_loss <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb19-51"><a href="#cb19-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-52"><a href="#cb19-52" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Finished Training"</span>) </span>
<span id="cb19-53"><a href="#cb19-53" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb19-54"><a href="#cb19-54" aria-hidden="true" tabindex="-1"></a>    </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># testing loop</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> test(dataloader, feature_choice <span class="op">=</span> <span class="st">"l"</span>):</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>    correct, total <span class="op">=</span> <span class="dv">0</span>, <span class="dv">0</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> idx, (lyrics, engineered, label) <span class="kw">in</span> <span class="bu">enumerate</span>(dataloader):</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>            <span class="co"># form prediction on batch, using chosen features and models</span></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> feature_choice <span class="op">==</span> <span class="st">"l"</span>: <span class="co"># lyrics only model</span></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>              predicted_label <span class="op">=</span> lyrics_model(lyrics)</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>            <span class="cf">elif</span> feature_choice <span class="op">==</span> <span class="st">"e"</span>: <span class="co"># engineered features only</span></span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>              predicted_label <span class="op">=</span> engineered_model(engineered)</span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>            <span class="cf">elif</span> feature_choice <span class="op">==</span> <span class="st">"b"</span>: <span class="co"># both lyrics and engineered features</span></span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>              predicted_label <span class="op">=</span> both_model(lyrics, engineered)</span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>            correct <span class="op">+=</span> (predicted_label.argmax(<span class="dv">1</span>) <span class="op">==</span> label).<span class="bu">sum</span>().item()</span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a>            total   <span class="op">+=</span> label.size(<span class="dv">0</span>)</span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'Test accuracy: </span><span class="sc">{</span><span class="dv">100</span> <span class="op">*</span> correct <span class="op">//</span> total<span class="sc">}</span><span class="ss"> %'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="lyrics-only" class="level3">
<h3 class="anchored" data-anchor-id="lyrics-only">Lyrics Only</h3>
<p>I will use a simple model with a word embedding layer to classify music based on only the lyrics.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> nn</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a><span class="co"># define the model</span></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LyricsModel(nn.Module):</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_size, embedding_dim, max_len, num_class):</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.embedding <span class="op">=</span> nn.Embedding(vocab_size<span class="op">+</span><span class="dv">1</span>, embedding_dim)</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc <span class="op">=</span> nn.Linear(max_len<span class="op">*</span>embedding_dim, num_class)</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.embedding(x)</span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.flatten(x, <span class="dv">1</span>)</span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.fc(x)</span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span>(x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># instantiate the model</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>embedding_dim <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>lyrics_model <span class="op">=</span> LyricsModel(<span class="bu">len</span>(vocab), embedding_dim, max_len, <span class="dv">3</span>).to(device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-outputid="10b5cedd-7746-4f28-d3ab-c75e976036f1">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co"># train the model</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>k_epochs <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>train(train_loader, <span class="st">"l"</span>, k_epochs, <span class="dv">200</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[epoch: 1, batches:   200], loss: 1.192, accuracy:0.500
[epoch: 1, batches:   400], loss: 1.039, accuracy:0.312
[epoch: 2, batches:   200], loss: 0.818, accuracy:0.562
[epoch: 2, batches:   400], loss: 0.797, accuracy:0.688
[epoch: 3, batches:   200], loss: 0.628, accuracy:0.688
[epoch: 3, batches:   400], loss: 0.649, accuracy:0.812
[epoch: 4, batches:   200], loss: 0.509, accuracy:0.500
[epoch: 4, batches:   400], loss: 0.542, accuracy:0.938
[epoch: 5, batches:   200], loss: 0.436, accuracy:0.750
[epoch: 5, batches:   400], loss: 0.497, accuracy:0.812
[epoch: 6, batches:   200], loss: 0.409, accuracy:0.750
[epoch: 6, batches:   400], loss: 0.456, accuracy:0.750
[epoch: 7, batches:   200], loss: 0.365, accuracy:0.875
[epoch: 7, batches:   400], loss: 0.410, accuracy:0.688
[epoch: 8, batches:   200], loss: 0.346, accuracy:0.875
[epoch: 8, batches:   400], loss: 0.383, accuracy:0.812
[epoch: 9, batches:   200], loss: 0.312, accuracy:0.875
[epoch: 9, batches:   400], loss: 0.382, accuracy:0.938
[epoch: 10, batches:   200], loss: 0.326, accuracy:0.875
[epoch: 10, batches:   400], loss: 0.363, accuracy:0.750
Finished Training</code></pre>
</div>
</div>
<div class="cell" data-outputid="2bc2a99e-ad20-4d67-a298-2812651c67c2">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="co"># test accuracy</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>test(test_loader, <span class="st">"l"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Test accuracy: 65 %</code></pre>
</div>
</div>
<p>The test accuracy is lower than the training accuracy. This suggests overfitting.</p>
<p>I will add two dropout layers to the network and see if this reduces overfitting.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LyricsModelDropout(nn.Module):</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_size, embedding_dim, max_len, num_class):</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.embedding <span class="op">=</span> nn.Embedding(vocab_size<span class="op">+</span><span class="dv">1</span>, embedding_dim)</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout1 <span class="op">=</span> nn.Dropout(<span class="fl">0.2</span>)</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc <span class="op">=</span> nn.Linear(<span class="dv">3</span>, num_class)</span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout2 <span class="op">=</span> nn.Dropout(<span class="fl">0.2</span>)</span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.embedding(x)</span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.dropout1(x)</span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.mean(axis <span class="op">=</span> <span class="dv">1</span>) <span class="co"># take the average across tokens for each embedding dimension</span></span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.flatten(x, <span class="dv">1</span>)</span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.fc(x)</span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.dropout2(x)</span>
<span id="cb27-17"><a href="#cb27-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span>(x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>lyrics_model <span class="op">=</span> LyricsModelDropout(<span class="bu">len</span>(vocab), embedding_dim, max_len, <span class="dv">3</span>).to(device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-outputid="323253d9-9491-43bd-e42f-b0ec27359a97">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="co"># train the model</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>k_epochs <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>train(train_loader, <span class="st">"l"</span>, k_epochs, <span class="dv">200</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[epoch: 1, batches:   200], loss: 0.953, accuracy:0.562
[epoch: 1, batches:   400], loss: 0.896, accuracy:0.562
[epoch: 2, batches:   200], loss: 0.854, accuracy:0.500
[epoch: 2, batches:   400], loss: 0.825, accuracy:0.625
[epoch: 3, batches:   200], loss: 0.743, accuracy:0.750
[epoch: 3, batches:   400], loss: 0.732, accuracy:0.500
[epoch: 4, batches:   200], loss: 0.691, accuracy:0.625
[epoch: 4, batches:   400], loss: 0.676, accuracy:0.750
[epoch: 5, batches:   200], loss: 0.643, accuracy:0.688
[epoch: 5, batches:   400], loss: 0.645, accuracy:0.688
[epoch: 6, batches:   200], loss: 0.617, accuracy:0.875
[epoch: 6, batches:   400], loss: 0.630, accuracy:0.750
[epoch: 7, batches:   200], loss: 0.608, accuracy:0.812
[epoch: 7, batches:   400], loss: 0.612, accuracy:0.875
[epoch: 8, batches:   200], loss: 0.600, accuracy:0.562
[epoch: 8, batches:   400], loss: 0.598, accuracy:0.750
[epoch: 9, batches:   200], loss: 0.591, accuracy:0.875
[epoch: 9, batches:   400], loss: 0.585, accuracy:0.812
[epoch: 10, batches:   200], loss: 0.574, accuracy:0.750
[epoch: 10, batches:   400], loss: 0.583, accuracy:0.688
Finished Training</code></pre>
</div>
</div>
<div class="cell" data-outputid="294851fd-e1ae-41c1-e8c1-2471fe15428b">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="co"># test accuracy</span></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>test(test_loader, <span class="st">"l"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Test accuracy: 67 %</code></pre>
</div>
</div>
<p>The dropout layers did reduce overfitting, and the test accuracy improved by 2%.</p>
<p>The base rate for our classification across four genres is 33.3%, so the accuracy of 67% suggests that the model is at least doing one time better than random guessing.</p>
</section>
<section id="engineered-only" class="level3">
<h3 class="anchored" data-anchor-id="engineered-only">Engineered Only</h3>
<p>Next, I will train some models that take in only the engineered features of the songs.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> EngineeredModel(nn.Module):</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_class):</span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Linear(<span class="dv">22</span>, <span class="dv">64</span>)</span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc2 <span class="op">=</span> nn.Linear(<span class="dv">64</span>, <span class="dv">32</span>)</span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc3 <span class="op">=</span> nn.Linear(<span class="dv">32</span>, num_class)</span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(<span class="va">self</span>.fc1(x))</span>
<span id="cb33-13"><a href="#cb33-13" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(<span class="va">self</span>.fc2(x))</span>
<span id="cb33-14"><a href="#cb33-14" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.fc3(x)</span>
<span id="cb33-15"><a href="#cb33-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span>(x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>engineered_model <span class="op">=</span> EngineeredModel(<span class="dv">3</span>).to(device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-outputid="b1d02d3f-1171-4d23-ef7f-939ff22fc8d2">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">0.0001</span></span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>k_epochs <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>train(train_loader, <span class="st">"e"</span>, k_epochs, <span class="dv">200</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[epoch: 1, batches:   200], loss: 1.096, accuracy:0.250
[epoch: 1, batches:   400], loss: 0.988, accuracy:0.812
[epoch: 2, batches:   200], loss: 0.860, accuracy:0.750
[epoch: 2, batches:   400], loss: 0.802, accuracy:0.750
[epoch: 3, batches:   200], loss: 0.749, accuracy:0.625
[epoch: 3, batches:   400], loss: 0.713, accuracy:0.688
[epoch: 4, batches:   200], loss: 0.711, accuracy:0.562
[epoch: 4, batches:   400], loss: 0.677, accuracy:0.688
[epoch: 5, batches:   200], loss: 0.671, accuracy:0.562
[epoch: 5, batches:   400], loss: 0.653, accuracy:0.750
[epoch: 6, batches:   200], loss: 0.648, accuracy:0.875
[epoch: 6, batches:   400], loss: 0.643, accuracy:0.750
[epoch: 7, batches:   200], loss: 0.623, accuracy:0.875
[epoch: 7, batches:   400], loss: 0.624, accuracy:0.562
[epoch: 8, batches:   200], loss: 0.608, accuracy:0.688
[epoch: 8, batches:   400], loss: 0.614, accuracy:0.875
[epoch: 9, batches:   200], loss: 0.612, accuracy:0.625
[epoch: 9, batches:   400], loss: 0.608, accuracy:0.562
[epoch: 10, batches:   200], loss: 0.603, accuracy:0.875
[epoch: 10, batches:   400], loss: 0.607, accuracy:0.750
Finished Training</code></pre>
</div>
</div>
<div class="cell" data-outputid="1fbb9bbd-6245-49ef-dd7c-95a5bf30958f">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>test(test_loader, <span class="st">"e"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Test accuracy: 74 %</code></pre>
</div>
</div>
<p>The accuracy that we got using engineered feature is higher than what we got with the lyrics. However, it seems that the model was struggling to improve accuracy during training.</p>
<p>Adding another fully connected layer improved the test accuracy for 2%:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> EngineeredModelMoreFc(nn.Module):</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_class):</span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Linear(<span class="dv">22</span>, <span class="dv">128</span>)</span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc2 <span class="op">=</span> nn.Linear(<span class="dv">128</span>, <span class="dv">64</span>)</span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc3 <span class="op">=</span> nn.Linear(<span class="dv">64</span>, <span class="dv">32</span>)</span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc4 <span class="op">=</span> nn.Linear(<span class="dv">32</span>, num_class)</span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(<span class="va">self</span>.fc1(x))</span>
<span id="cb39-12"><a href="#cb39-12" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(<span class="va">self</span>.fc2(x))</span>
<span id="cb39-13"><a href="#cb39-13" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(<span class="va">self</span>.fc3(x))</span>
<span id="cb39-14"><a href="#cb39-14" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.fc4(x)</span>
<span id="cb39-15"><a href="#cb39-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-16"><a href="#cb39-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span>(x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>engineered_model <span class="op">=</span> EngineeredModelMoreFc(<span class="dv">3</span>).to(device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-outputid="22924ad9-dc86-4141-e60e-b8d9562d203d">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>train(train_loader, <span class="st">"e"</span>, k_epochs, <span class="dv">200</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[epoch: 1, batches:   200], loss: 1.099, accuracy:0.562
[epoch: 1, batches:   400], loss: 0.925, accuracy:0.750
[epoch: 2, batches:   200], loss: 0.792, accuracy:0.688
[epoch: 2, batches:   400], loss: 0.715, accuracy:0.938
[epoch: 3, batches:   200], loss: 0.672, accuracy:0.875
[epoch: 3, batches:   400], loss: 0.661, accuracy:0.938
[epoch: 4, batches:   200], loss: 0.645, accuracy:0.750
[epoch: 4, batches:   400], loss: 0.614, accuracy:0.812
[epoch: 5, batches:   200], loss: 0.611, accuracy:0.750
[epoch: 5, batches:   400], loss: 0.616, accuracy:0.938
[epoch: 6, batches:   200], loss: 0.589, accuracy:0.812
[epoch: 6, batches:   400], loss: 0.599, accuracy:0.750
[epoch: 7, batches:   200], loss: 0.600, accuracy:0.562
[epoch: 7, batches:   400], loss: 0.588, accuracy:0.688
[epoch: 8, batches:   200], loss: 0.587, accuracy:0.688
[epoch: 8, batches:   400], loss: 0.595, accuracy:0.688
[epoch: 9, batches:   200], loss: 0.568, accuracy:0.875
[epoch: 9, batches:   400], loss: 0.589, accuracy:0.750
[epoch: 10, batches:   200], loss: 0.578, accuracy:0.688
[epoch: 10, batches:   400], loss: 0.584, accuracy:0.688
Finished Training</code></pre>
</div>
</div>
<div class="cell" data-outputid="dedeb7c4-1947-428a-d15e-0f30e88a3087">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>test(test_loader, <span class="st">"e"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Test accuracy: 76 %</code></pre>
</div>
</div>
<p>Using only one layer, on the other hand, decreased test accuracy:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="co"># one hidden layer with 32 outputs</span></span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> EngineeredModel1Fc(nn.Module):</span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_class):</span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Linear(<span class="dv">22</span>, <span class="dv">32</span>)</span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc2 <span class="op">=</span> nn.Linear(<span class="dv">32</span>, num_class)</span>
<span id="cb45-8"><a href="#cb45-8" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb45-9"><a href="#cb45-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb45-10"><a href="#cb45-10" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(<span class="va">self</span>.fc1(x))</span>
<span id="cb45-11"><a href="#cb45-11" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.fc2(x)</span>
<span id="cb45-12"><a href="#cb45-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span>(x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>engineered_model <span class="op">=</span> EngineeredModel1Fc(<span class="dv">3</span>).to(device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-outputid="e11675d6-9ee9-4d23-f695-f6f092de0ec6">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a>train(train_loader, <span class="st">"e"</span>, k_epochs, <span class="dv">200</span>) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[epoch: 1, batches:   200], loss: 1.119, accuracy:0.562
[epoch: 1, batches:   400], loss: 1.056, accuracy:0.438
[epoch: 2, batches:   200], loss: 0.964, accuracy:0.625
[epoch: 2, batches:   400], loss: 0.923, accuracy:0.500
[epoch: 3, batches:   200], loss: 0.889, accuracy:0.438
[epoch: 3, batches:   400], loss: 0.853, accuracy:0.500
[epoch: 4, batches:   200], loss: 0.837, accuracy:0.625
[epoch: 4, batches:   400], loss: 0.811, accuracy:0.688
[epoch: 5, batches:   200], loss: 0.785, accuracy:0.688
[epoch: 5, batches:   400], loss: 0.785, accuracy:0.875
[epoch: 6, batches:   200], loss: 0.744, accuracy:0.750
[epoch: 6, batches:   400], loss: 0.753, accuracy:0.562
[epoch: 7, batches:   200], loss: 0.715, accuracy:0.438
[epoch: 7, batches:   400], loss: 0.726, accuracy:0.688
[epoch: 8, batches:   200], loss: 0.703, accuracy:0.688
[epoch: 8, batches:   400], loss: 0.704, accuracy:0.375
[epoch: 9, batches:   200], loss: 0.681, accuracy:0.688
[epoch: 9, batches:   400], loss: 0.690, accuracy:0.812
[epoch: 10, batches:   200], loss: 0.671, accuracy:0.812
[epoch: 10, batches:   400], loss: 0.666, accuracy:0.688
Finished Training</code></pre>
</div>
</div>
<div class="cell" data-outputid="46ebfd4d-cdc3-49d1-da2c-cf6f2b1104c7">
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a>test(test_loader, <span class="st">"e"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Test accuracy: 72 %</code></pre>
</div>
</div>
<p>It looks like accuracy increased as the number of layers increased. I will try another model with even more layers.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> EngineeredModelMoreFc(nn.Module):</span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_class):</span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Linear(<span class="dv">22</span>, <span class="dv">128</span>)</span>
<span id="cb51-6"><a href="#cb51-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc2 <span class="op">=</span> nn.Linear(<span class="dv">128</span>, <span class="dv">64</span>)</span>
<span id="cb51-7"><a href="#cb51-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc3 <span class="op">=</span> nn.Linear(<span class="dv">64</span>, <span class="dv">32</span>)</span>
<span id="cb51-8"><a href="#cb51-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc4 <span class="op">=</span> nn.Linear(<span class="dv">32</span>, <span class="dv">16</span>)</span>
<span id="cb51-9"><a href="#cb51-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc5 <span class="op">=</span> nn.Linear(<span class="dv">16</span>, num_class)</span>
<span id="cb51-10"><a href="#cb51-10" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb51-11"><a href="#cb51-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb51-12"><a href="#cb51-12" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(<span class="va">self</span>.fc1(x))</span>
<span id="cb51-13"><a href="#cb51-13" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(<span class="va">self</span>.fc2(x))</span>
<span id="cb51-14"><a href="#cb51-14" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(<span class="va">self</span>.fc3(x))</span>
<span id="cb51-15"><a href="#cb51-15" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(<span class="va">self</span>.fc4(x))</span>
<span id="cb51-16"><a href="#cb51-16" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.fc5(x)</span>
<span id="cb51-17"><a href="#cb51-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-18"><a href="#cb51-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span>(x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a>engineered_model <span class="op">=</span> EngineeredModelMoreFc(<span class="dv">3</span>).to(device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-outputid="5bbdb781-fb0d-4616-d5b3-d292b4a420bf">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a>train(train_loader, <span class="st">"e"</span>, k_epochs, <span class="dv">200</span>) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[epoch: 1, batches:   200], loss: 1.137, accuracy:0.250
[epoch: 1, batches:   400], loss: 0.958, accuracy:0.625
[epoch: 2, batches:   200], loss: 0.845, accuracy:0.562
[epoch: 2, batches:   400], loss: 0.769, accuracy:0.750
[epoch: 3, batches:   200], loss: 0.733, accuracy:0.562
[epoch: 3, batches:   400], loss: 0.669, accuracy:0.625
[epoch: 4, batches:   200], loss: 0.643, accuracy:0.938
[epoch: 4, batches:   400], loss: 0.636, accuracy:0.625
[epoch: 5, batches:   200], loss: 0.627, accuracy:0.875
[epoch: 5, batches:   400], loss: 0.626, accuracy:0.812
[epoch: 6, batches:   200], loss: 0.605, accuracy:0.625
[epoch: 6, batches:   400], loss: 0.624, accuracy:0.625
[epoch: 7, batches:   200], loss: 0.613, accuracy:0.812
[epoch: 7, batches:   400], loss: 0.612, accuracy:0.562
[epoch: 8, batches:   200], loss: 0.595, accuracy:0.812
[epoch: 8, batches:   400], loss: 0.594, accuracy:0.688
[epoch: 9, batches:   200], loss: 0.589, accuracy:0.812
[epoch: 9, batches:   400], loss: 0.594, accuracy:0.688
[epoch: 10, batches:   200], loss: 0.579, accuracy:0.688
[epoch: 10, batches:   400], loss: 0.591, accuracy:0.875
Finished Training</code></pre>
</div>
</div>
<div class="cell" data-outputid="6c03fea3-5b52-4d9f-9bfb-3b3617c65c0e">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a>test(test_loader, <span class="st">"e"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Test accuracy: 75 %</code></pre>
</div>
</div>
<p>We didn’t get a better accuracy with one more layer. It seems that the benefit of making the model more complex has leveled out.</p>
<p>In conclusion, the best performing model is the one with four fully connected layers, with the test accuracy of 76%.</p>
</section>
<section id="lyrics-engineered-features" class="level3">
<h3 class="anchored" data-anchor-id="lyrics-engineered-features">Lyrics + Engineered Features</h3>
<p>Finally, I will try a model that takes in both the lyrics and the engineered features. This model takes in the lyrics data and engineered features separately. The lyrics are passed through an embedding layer and a dropout layer. The engineered features go through three fully-connected layers. Finally, the lyrics and engineered feature outputs are combined into one tensor, and passed through two fully-connected layers and one dropout layer in between.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CombinedNet(nn.Module):</span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_size, embedding_dim, num_class):</span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.embedding <span class="op">=</span> nn.Embedding(vocab_size<span class="op">+</span><span class="dv">1</span>, embedding_dim)</span>
<span id="cb57-6"><a href="#cb57-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Linear(<span class="dv">22</span>, <span class="dv">64</span>)</span>
<span id="cb57-7"><a href="#cb57-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc2 <span class="op">=</span> nn.Linear(<span class="dv">64</span>, <span class="dv">32</span>)</span>
<span id="cb57-8"><a href="#cb57-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc3 <span class="op">=</span> nn.Linear(<span class="dv">32</span>, <span class="dv">16</span>)</span>
<span id="cb57-9"><a href="#cb57-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc4 <span class="op">=</span> nn.Linear(<span class="dv">613</span>, <span class="dv">32</span>)</span>
<span id="cb57-10"><a href="#cb57-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc5 <span class="op">=</span> nn.Linear(<span class="dv">32</span>, num_class)</span>
<span id="cb57-11"><a href="#cb57-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(<span class="fl">0.2</span>)</span>
<span id="cb57-12"><a href="#cb57-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb57-13"><a href="#cb57-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x_1, x_2):</span>
<span id="cb57-14"><a href="#cb57-14" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb57-15"><a href="#cb57-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># text pipeline</span></span>
<span id="cb57-16"><a href="#cb57-16" aria-hidden="true" tabindex="-1"></a>        x_1 <span class="op">=</span> <span class="va">self</span>.embedding(x_1)</span>
<span id="cb57-17"><a href="#cb57-17" aria-hidden="true" tabindex="-1"></a>        x_1 <span class="op">=</span> <span class="va">self</span>.dropout(x_1)</span>
<span id="cb57-18"><a href="#cb57-18" aria-hidden="true" tabindex="-1"></a>        x_1 <span class="op">=</span> torch.flatten(x_1, <span class="dv">1</span>)</span>
<span id="cb57-19"><a href="#cb57-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-20"><a href="#cb57-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># engineered features</span></span>
<span id="cb57-21"><a href="#cb57-21" aria-hidden="true" tabindex="-1"></a>        x_2 <span class="op">=</span> F.relu(<span class="va">self</span>.fc1(x_2))</span>
<span id="cb57-22"><a href="#cb57-22" aria-hidden="true" tabindex="-1"></a>        x_2 <span class="op">=</span> F.relu(<span class="va">self</span>.fc2(x_2))</span>
<span id="cb57-23"><a href="#cb57-23" aria-hidden="true" tabindex="-1"></a>        x_2 <span class="op">=</span> <span class="va">self</span>.fc3(x_2)</span>
<span id="cb57-24"><a href="#cb57-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-25"><a href="#cb57-25" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ensure that both x_1 and x_2 are 2-d tensors, flattening if necessary</span></span>
<span id="cb57-26"><a href="#cb57-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># then, combine them with: </span></span>
<span id="cb57-27"><a href="#cb57-27" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.cat((x_1, x_2), <span class="dv">1</span>)</span>
<span id="cb57-28"><a href="#cb57-28" aria-hidden="true" tabindex="-1"></a>        <span class="co"># pass x through a couple more fully-connected layers and return output</span></span>
<span id="cb57-29"><a href="#cb57-29" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(<span class="va">self</span>.fc4(x))</span>
<span id="cb57-30"><a href="#cb57-30" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.dropout(x)</span>
<span id="cb57-31"><a href="#cb57-31" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.fc5(x)</span>
<span id="cb57-32"><a href="#cb57-32" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span>(x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a>both_model <span class="op">=</span> CombinedNet(<span class="bu">len</span>(vocab), embedding_dim, <span class="dv">3</span>).to(device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-outputid="81ee180d-521c-40e9-e2c7-5b18acc39661">
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">0.001</span></span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a>k_epochs <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a>train(train_loader, <span class="st">"b"</span>, k_epochs, <span class="dv">200</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[epoch: 1, batches:   200], loss: 0.857, accuracy:0.562
[epoch: 1, batches:   400], loss: 0.727, accuracy:0.750
[epoch: 2, batches:   200], loss: 0.702, accuracy:0.750
[epoch: 2, batches:   400], loss: 0.665, accuracy:0.812
[epoch: 3, batches:   200], loss: 0.666, accuracy:0.625
[epoch: 3, batches:   400], loss: 0.652, accuracy:0.812
[epoch: 4, batches:   200], loss: 0.617, accuracy:0.938
[epoch: 4, batches:   400], loss: 0.586, accuracy:0.750
[epoch: 5, batches:   200], loss: 0.562, accuracy:0.812
[epoch: 5, batches:   400], loss: 0.562, accuracy:0.688
[epoch: 6, batches:   200], loss: 0.536, accuracy:0.812
[epoch: 6, batches:   400], loss: 0.548, accuracy:0.625
[epoch: 7, batches:   200], loss: 0.539, accuracy:0.938
[epoch: 7, batches:   400], loss: 0.527, accuracy:0.750
[epoch: 8, batches:   200], loss: 0.521, accuracy:0.688
[epoch: 8, batches:   400], loss: 0.518, accuracy:0.750
[epoch: 9, batches:   200], loss: 0.491, accuracy:0.812
[epoch: 9, batches:   400], loss: 0.506, accuracy:0.812
[epoch: 10, batches:   200], loss: 0.489, accuracy:1.000
[epoch: 10, batches:   400], loss: 0.501, accuracy:0.812
Finished Training</code></pre>
</div>
</div>
<div class="cell" data-outputid="3a2283d9-95cf-4556-f78b-3b92dba1536b">
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a>test(test_loader, <span class="st">"b"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Test accuracy: 74 %</code></pre>
</div>
</div>
<p>The combined model seems to have similar testing performance as the model with only engineered features. Although I added some dropout layers, there is still some overfitting. In the next model, I will try to add more dropout layers.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CombinedNetMoreDropout(nn.Module):</span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_size, embedding_dim, num_class):</span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb63-5"><a href="#cb63-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.embedding <span class="op">=</span> nn.Embedding(vocab_size<span class="op">+</span><span class="dv">1</span>, embedding_dim)</span>
<span id="cb63-6"><a href="#cb63-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Linear(<span class="dv">22</span>, <span class="dv">64</span>)</span>
<span id="cb63-7"><a href="#cb63-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc2 <span class="op">=</span> nn.Linear(<span class="dv">64</span>, <span class="dv">32</span>)</span>
<span id="cb63-8"><a href="#cb63-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc3 <span class="op">=</span> nn.Linear(<span class="dv">32</span>, <span class="dv">16</span>)</span>
<span id="cb63-9"><a href="#cb63-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc4 <span class="op">=</span> nn.Linear(<span class="dv">613</span>, <span class="dv">32</span>)</span>
<span id="cb63-10"><a href="#cb63-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc5 <span class="op">=</span> nn.Linear(<span class="dv">32</span>, num_class)</span>
<span id="cb63-11"><a href="#cb63-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(<span class="fl">0.2</span>)</span>
<span id="cb63-12"><a href="#cb63-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb63-13"><a href="#cb63-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x_1, x_2):</span>
<span id="cb63-14"><a href="#cb63-14" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb63-15"><a href="#cb63-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># text pipeline</span></span>
<span id="cb63-16"><a href="#cb63-16" aria-hidden="true" tabindex="-1"></a>        x_1 <span class="op">=</span> <span class="va">self</span>.embedding(x_1)</span>
<span id="cb63-17"><a href="#cb63-17" aria-hidden="true" tabindex="-1"></a>        x_1 <span class="op">=</span> <span class="va">self</span>.dropout(x_1)</span>
<span id="cb63-18"><a href="#cb63-18" aria-hidden="true" tabindex="-1"></a>        x_1 <span class="op">=</span> torch.flatten(x_1, <span class="dv">1</span>)</span>
<span id="cb63-19"><a href="#cb63-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-20"><a href="#cb63-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># engineered features</span></span>
<span id="cb63-21"><a href="#cb63-21" aria-hidden="true" tabindex="-1"></a>        x_2 <span class="op">=</span> F.relu(<span class="va">self</span>.fc1(x_2))</span>
<span id="cb63-22"><a href="#cb63-22" aria-hidden="true" tabindex="-1"></a>        x_2 <span class="op">=</span> <span class="va">self</span>.dropout(x_2)</span>
<span id="cb63-23"><a href="#cb63-23" aria-hidden="true" tabindex="-1"></a>        x_2 <span class="op">=</span> F.relu(<span class="va">self</span>.fc2(x_2))</span>
<span id="cb63-24"><a href="#cb63-24" aria-hidden="true" tabindex="-1"></a>        x_2 <span class="op">=</span> <span class="va">self</span>.dropout(x_2)</span>
<span id="cb63-25"><a href="#cb63-25" aria-hidden="true" tabindex="-1"></a>        x_2 <span class="op">=</span> <span class="va">self</span>.fc3(x_2)</span>
<span id="cb63-26"><a href="#cb63-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-27"><a href="#cb63-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ensure that both x_1 and x_2 are 2-d tensors, flattening if necessary</span></span>
<span id="cb63-28"><a href="#cb63-28" aria-hidden="true" tabindex="-1"></a>        <span class="co"># then, combine them with: </span></span>
<span id="cb63-29"><a href="#cb63-29" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.cat((x_1, x_2), <span class="dv">1</span>)</span>
<span id="cb63-30"><a href="#cb63-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># pass x through a couple more fully-connected layers and return output</span></span>
<span id="cb63-31"><a href="#cb63-31" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(<span class="va">self</span>.fc4(x))</span>
<span id="cb63-32"><a href="#cb63-32" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.dropout(x)</span>
<span id="cb63-33"><a href="#cb63-33" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.fc5(x)</span>
<span id="cb63-34"><a href="#cb63-34" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span>(x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a>both_model <span class="op">=</span> CombinedNet(<span class="bu">len</span>(vocab), embedding_dim, <span class="dv">3</span>).to(device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-outputid="a1295250-0f25-4247-9f8e-2328f0a904d1">
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a>k_epochs <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a>train(train_loader, <span class="st">"b"</span>, k_epochs, <span class="dv">200</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[epoch: 1, batches:   200], loss: 0.826, accuracy:0.875
[epoch: 1, batches:   400], loss: 0.701, accuracy:0.750
[epoch: 2, batches:   200], loss: 0.640, accuracy:0.750
[epoch: 2, batches:   400], loss: 0.607, accuracy:0.750
[epoch: 3, batches:   200], loss: 0.568, accuracy:0.875
[epoch: 3, batches:   400], loss: 0.568, accuracy:0.688
[epoch: 4, batches:   200], loss: 0.523, accuracy:0.625
[epoch: 4, batches:   400], loss: 0.530, accuracy:0.812
[epoch: 5, batches:   200], loss: 0.521, accuracy:0.625
[epoch: 5, batches:   400], loss: 0.504, accuracy:0.688
[epoch: 6, batches:   200], loss: 0.487, accuracy:0.750
[epoch: 6, batches:   400], loss: 0.493, accuracy:0.625
[epoch: 7, batches:   200], loss: 0.450, accuracy:0.875
[epoch: 7, batches:   400], loss: 0.475, accuracy:0.812
[epoch: 8, batches:   200], loss: 0.449, accuracy:0.625
[epoch: 8, batches:   400], loss: 0.467, accuracy:0.875
[epoch: 9, batches:   200], loss: 0.435, accuracy:0.750
[epoch: 9, batches:   400], loss: 0.459, accuracy:0.750
[epoch: 10, batches:   200], loss: 0.424, accuracy:0.750
[epoch: 10, batches:   400], loss: 0.431, accuracy:0.812
Finished Training</code></pre>
</div>
</div>
<div class="cell" data-outputid="cc1c7665-4481-43a5-cd16-f2ce35f6b32b">
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a>test(test_loader, <span class="st">"b"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Test accuracy: 74 %</code></pre>
</div>
</div>
</section>
<section id="summary" class="level3">
<h3 class="anchored" data-anchor-id="summary">Summary</h3>
<p>In summary, the best test accuracy achieved by our three types of models are: - Lyrics only: 67% - Engineered features only: 76% - Lyrics and engineered features: 74%</p>
<p>All of these models perform better than the base rate of 33.3%, but there is still room for improvement. In general, the engineered features are better predictors of genres than the lyrics. Looking at lyrics in addition to engineered features did not seem to help boost accuracy compared to using the features alone.</p>
</section>
</section>
<section id="visualzing-word-embeddings" class="level2">
<h2 class="anchored" data-anchor-id="visualzing-word-embeddings">Visualzing Word Embeddings</h2>
<p>In this section, I will visualize the word embeddings learned by my lyrics model.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb69"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a><span class="co"># extract the embedding matrix from the lyrics model</span></span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a>embedding_matrix <span class="op">=</span> lyrics_model.embedding.cpu().weight.data.numpy()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb70"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a><span class="co"># extract words from vocab</span></span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a>tokens <span class="op">=</span> vocab.get_itos()</span>
<span id="cb70-3"><a href="#cb70-3" aria-hidden="true" tabindex="-1"></a>tokens.append(<span class="st">" "</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-outputid="3fa7a51c-703f-4a26-db26-0ae4c453a3af">
<div class="sourceCode cell-code" id="cb71"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a><span class="co"># represent embedding in two dimensions</span></span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.decomposition <span class="im">import</span> PCA</span>
<span id="cb71-3"><a href="#cb71-3" aria-hidden="true" tabindex="-1"></a>pca <span class="op">=</span> PCA(n_components <span class="op">=</span> <span class="dv">2</span>)</span>
<span id="cb71-4"><a href="#cb71-4" aria-hidden="true" tabindex="-1"></a>weights <span class="op">=</span> pca.fit_transform(embedding_matrix)</span>
<span id="cb71-5"><a href="#cb71-5" aria-hidden="true" tabindex="-1"></a>weights</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="44">
<pre><code>array([[ 1.6995059 ,  0.86947083],
       [ 1.9406745 ,  2.9961193 ],
       [-0.12173966,  2.8156223 ],
       ...,
       [ 2.7143688 , -0.7970013 ],
       [-0.44954112, -2.4305842 ],
       [ 0.16685873,  0.17623016]], dtype=float32)</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb73"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a><span class="co"># turn into dataframe</span></span>
<span id="cb73-2"><a href="#cb73-2" aria-hidden="true" tabindex="-1"></a>embedding_df <span class="op">=</span> pd.DataFrame({</span>
<span id="cb73-3"><a href="#cb73-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">"word"</span>: tokens, </span>
<span id="cb73-4"><a href="#cb73-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">"x0"</span>: weights[:,<span class="dv">0</span>], </span>
<span id="cb73-5"><a href="#cb73-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">"x1"</span>: weights[:,<span class="dv">1</span>]</span>
<span id="cb73-6"><a href="#cb73-6" aria-hidden="true" tabindex="-1"></a>})</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-outputid="44f7f2a5-17f3-49c7-a21b-425c2a8d056c" data-execution_count="53">
<div class="sourceCode cell-code" id="cb74"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a><span class="co"># visualize</span></span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> plotly.express <span class="im">as</span> px </span>
<span id="cb74-3"><a href="#cb74-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> plotly.io <span class="im">as</span> pio</span>
<span id="cb74-4"><a href="#cb74-4" aria-hidden="true" tabindex="-1"></a>pio.renderers.default <span class="op">=</span> <span class="st">"plotly_mimetype+notebook_connected"</span></span>
<span id="cb74-5"><a href="#cb74-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb74-6"><a href="#cb74-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-7"><a href="#cb74-7" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> px.scatter(embedding_df, </span>
<span id="cb74-8"><a href="#cb74-8" aria-hidden="true" tabindex="-1"></a>                 x <span class="op">=</span> <span class="st">"x0"</span>, </span>
<span id="cb74-9"><a href="#cb74-9" aria-hidden="true" tabindex="-1"></a>                 y <span class="op">=</span> <span class="st">"x1"</span>, </span>
<span id="cb74-10"><a href="#cb74-10" aria-hidden="true" tabindex="-1"></a>                 size <span class="op">=</span> <span class="bu">list</span>(np.ones(<span class="bu">len</span>(embedding_df))),</span>
<span id="cb74-11"><a href="#cb74-11" aria-hidden="true" tabindex="-1"></a>                 size_max <span class="op">=</span> <span class="dv">10</span>,</span>
<span id="cb74-12"><a href="#cb74-12" aria-hidden="true" tabindex="-1"></a>                 hover_name <span class="op">=</span> <span class="st">"word"</span>)</span>
<span id="cb74-13"><a href="#cb74-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-14"><a href="#cb74-14" aria-hidden="true" tabindex="-1"></a>fig.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

<div>                            <div id="b01ca6a7-a493-4c88-8a08-2664d6d3a2b7" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                require(["plotly"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("b01ca6a7-a493-4c88-8a08-2664d6d3a2b7")) {                    Plotly.newPlot(                        "b01ca6a7-a493-4c88-8a08-2664d6d3a2b7",                        [{"hovertemplate":"<b>%{hovertext}</b><br><br>x0=%{x}<br>x1=%{y}<br>size=%{marker.size}<extra></extra>","hovertext":["<unk>","know","like","time","come","go","heart","away","life","cause","feel","leave","yeah","live","right","want","night","break","think","long","tell","baby","look","world","need","gonna","good","fall","hold","hear","mind","home","dream","hand","eye","say","little","lose","walk","stand","believe","turn","play","better","take","sing","head","wanna","face","tear","blue","fuck","stay","change","tonight","love","start","girl","inside","things","light","song","place","hard","word","lonely","sweet","cold","give","fight","bring","money","remember","wrong","try","watch","thing","days","stop","true","gotta","black","fool","wait","wish","kill","make","shit","soul","get","miss","people","forget","line","kiss","hell","pain","dead","real","lyric","mean","care","burn","close","commercial","hurt","talk","today","arm","hide","high","bitch","sleep","years","shoot","cry","roll","call","somebody","forever","woman","lie","lord","sound","save","free","touch","help","till","door","grow","work","rain","wall","smile","open","see","goodbye","learn","blood","nigga","game","keep","drink","bout","party","wind","ready","deep","best","listen","everybody","morning","death","road","beat","apart","die","blow","nothin","music","matter","fear","reason","catch","throw","strong","white","run","body","tomorrow","write","alive","grind","sure","maybe","dark","guess","wonder","young","step","devil","memory","follow","friends","dance","speak","begin","understand","couldn","late","hate","promise","spend","pass","damn","friend","fade","pull","smoke","hang","comin","kind","ring","easy","hearts","heaven","niggas","rest","round","tire","crazy","house","ride","thank","truth","dear","street","goin","reach","gettin","scar","straight","drop","memories","afraid","bone","bear","voice","past","slip","darling","drive","soon","sick","moment","livin","stick","cross","nights","kick","blame","room","lookin","star","land","blind","shame","build","pray","steal","radio","feet","tryin","moon","city","wear","piece","stone","power","hole","breath","plan","waste","fast","laugh","trust","floor","scream","control","shake","songs","meet","belong","breathe","fine","water","someday","bleed","share","pick","send","pride","wouldn","talkin","tight","shine","lead","mama","picture","alright","check","summer","happen","lips","knees","yesterday","river","anymore","band","doubt","felt","feelin","chain","trouble","doin","million","dirty","girls","sign","brain","pretty","lonesome","mouth","end","streets","ways","second","choose","count","gold","raise","rule","great","country","peace","clear","push","rise","ball","loud","walkin","train","search","read","slow","lock","child","different","earth","swear","boys","thousand","runnin","sight","flow","wild","number","darkness","near","teach","evil","hair","warm","lady","forgive","story","outside","fell","sell","window","answer","finally","lovin","deal","mistake","flame","move","rhyme","worry","year","knock","wasn","daddy","shoe","finger","mother","sorrow","hop","track","jump","kid","ghost","pay","ask","ones","skin","wide","dust","sorry","future","shin","bury","half","longer","guitar","safe","lay","shut","thinkin","lover","point","letter","somethin","tree","beautiful","block","tongue","minute","bless","bind","whoa","mirror","phone","single","whisper","crack","fill","sayin","okay","school","crowd","brother","bust","crawl","taste","style","ahead","slowly","wake","queen","bottle","fuckin","shoulder","grave","simple","book","heat","race","perfect","show","waitin","remain","realize","wife","welcome","poor","children","cover","paper","darlin","twist","babe","cool","loose","nice","paint","small","repeat","state","cloud","makin","question","everyday","shout","record","thoughts","sense","pack","survive","quit","dress","pretend","stranger","bright","edge","father","mess","silence","outta","hours","playin","regret","wing","hour","glass","middle","brand","desire","corner","escape","human","rock","texas","club","return","sittin","family","green","strange","weak","woah","fly","luck","wine","insane","soldier","surprise","tune","drinkin","fate","treat","haunt","rockin","sink","fake","grab","ladies","clean","clock","warn","bite","tie","draw","south","heal","sit","space","prove","smell","tryna","bend","business","cowboy","motherfucker","freeze","spot","freedom","funny","steel","swing","brothers","color","closer","shadow","awake","dope","drug","couple","ocean","bang","drown","lovers","mountain","pour","sand","spin","wheel","cash","dare","neck","singin","america","disappear","have","honky","mornin","strike","anybody","climb","happiness","battle","master","saturday","serve","soft","spit","fail","gun","instead","magic","wave","clothe","gimme","heartache","roses","truck","bell","travel","crime","field","holy","let","fallin","fee","figure","imagine","kinda","movin","sky","stronger","suck","midnight","snow","spirit","tough","dirt","drift","misery","beer","lifetime","news","week","lean","precious","quick","buy","cryin","heavy","surround","even","echo","pocket","satisfy","short","tonk","crash","fame","momma","steady","thrill","whiskey","beg","booty","bridge","golden","tender","hello","highway","sail","clap","folks","quiet","ridin","shall","fact","guide","load","metal","sweat","card","decide","harder","hood","lift","prayer","ship","carry","hangin","hoe","takin","wander","choice","drag","string","whip","christmas","oooh","proud","tall","winter","fair","instrumental","judge","machine","mighty","table","california","flesh","hit","silver","name","niggaz","slide","women","wreck","remind","ease","freak","seat","suppose","bank","cheat","crown","dancin","hungry","ohoh","brave","cost","early","special","spell","boat","wrap","funky","diamonds","find","mend","respect","demons","force","hook","seek","spread","strength","angels","paradise","slave","suffer","wonderful","beneath","curse","higher","holdin","twice","wire","chill","older","trap","ache","callin","everytime","givin","hollow","leavin","murder","pussy","american","claim","favorite","knife","recall","secret","amaze","difference","double","later","stare","boss","lesson","loneliness","problem","dollar","hallelujah","jealous","weed","flower","lovely","pound","workin","fresh","hurry","note","stage","suddenly","bullet","erase","hill","weather","weight","bird","cast","foot","path","probably","final","gather","noise","rid","scene","store","tennessee","anger","doctor","rag","sin","southern","stuff","underneath","weep","church","fever","friday","garden","horse","release","shatter","surrender","animal","bitter","distance","flash","praise","seven","swallow","watchin","prison","spring","surely","thunder","trick","danger","heartaches","refuse","rope","shape","shouldn","stain","sweetheart","company","doors","enemy","hiphop","jail","mister","teardrops","bar","choke","coffee","hat","mountains","poison","pump","silent","vain","beast","chair","dime","fault","flip","bigger","certain","drum","fiddle","heartbreak","history","list","message","season","tellin","york","aren","cigarette","daylight","dont","goodnight","laughter","pillow","shelter","clown","offer","undo","bass","beauty","bore","cars","explain","nail","plain","shed","speed","switch","test","trade","trigger","admit","crew","savior","souls","stack","teeth","wound","appear","bomb","calm","chest","circle","scratch","ash","bother","chance","press","roar","trail","valley","weary","buzz","cell","chase","coast","fist","sister","stumble","view","boot","gift","hardly","sunday","tick","waltz","wed","asleep","class","foolish","join","put","rap","tide","float","grip","lyin","marry","nation","passion","stupid","will","cure","faster","madness","rough","wash","date","lick","rush","struggle","ticket","worst","boom","destroy","endless","groove","havin","problems","rebel","rome","strip","yellow","attack","daughter","east","excuse","fantasy","grand","numb","roam","toss","victim","worse","barely","desert","ears","hustle","iron","joke","nose","romance","softly","swim","wipe","bling","bloody","charm","crush","leather","santa","screen","standin","telephone","tremble","act","advice","bill","busy","dog","hammer","last","moonlight","secrets","smooth","breeze","burnin","cop","cut","glow","piss","quarter","smokin","suicide","whistle","action","cage","deserve","nature","reality","rhythm","self","settle","snake","spark","deeper","ghetto","island","lately","melt","moan","moments","notice","shotgun","solo","suit","unite","weekend","disguise","expect","gods","naked","pity","seed","vein","windows","alabama","breakin","comfort","motion","part","plenty","rip","set","uncle","downtown","flat","grey","guy","hotel","neon","proof","worlds","aside","cheap","handle","hollywood","order","punk","roof","stories","vision","zone","darkest","keepin","monster","orleans","ought","redneck","restless","sippin","condition","creep","flag","gently","guilty","inch","large","lightning","march","mood","roads","sugar","trippin","worship","blowin","gang","heartbeat","neighbor","north","plane","rainbow","smart","wise","destiny","fire","form","gentle","hearted","jungle","mystery","pills","revolution","treasure","wicked","win","youth","attention","awhile","county","deceive","dumb","embrace","exactly","fold","monday","park","salvation","sinner","angry","arrive","beach","deny","grass","guard","page","plus","ruin","shore","super","tangle","age","blast","borrow","cling","cock","direction","gain","homies","jeans","liar","liquor","match","needle","resist","reveal","silly","slap","strangers","team","thats","finish","heel","homie","memphis","mile","squeeze","tail","thee","trash","bull","create","dice","distant","harm","papa","porch","sake","seal","strap","stretch","sunset","type","damage","hardest","kitchen","main","mexico","nerve","pure","relax","sadness","score","screw","tempt","toe","unknown","crank","innocent","kingdom","movie","shots","simply","smash","solid","whore","witch","bullets","cadillac","cheer","command","feed","fish","houston","image","knockin","pleasure","pop","root","saddle","satan","shade","stereo","torture","weren","chicken","confuse","cotton","dough","explode","gate","holla","ignore","pool","protect","riot","shirt","shop","station","agree","apple","brick","brush","cocaine","cook","electric","forward","horn","kings","maker","separate","seventh","situation","spill","sweep","throat","turnin","weakness","ashamed","boogie","bunch","capture","crumble","demon","divine","farm","gamble","grin","guarantee","holiday","pardon","photograph","pimp","pistol","replace","sacrifice","saint","shinin","side","square","tiny","ugly","bein","cruel","goodbyes","heavenly","linger","mississippi","ohhhh","pin","rage","sheet","slam","suitcase","victory","actin","bible","blaze","bread","careful","complain","cowboys","dangerous","habit","haters","heroes","hopin","layin","oohooh","police","rent","tumble","winner","dreamin","flood","frame","hunger","hurricane","key","minutes","monkey","plant","prepare","shift","stress","teacher","tequila","tower","witness","attitude","bruise","candle","cheek","confess","dame","eternity","farther","fence","forgiveness","funk","innocence","invisible","jailhouse","joint","legs","motherfuckers","preacher","rappers","shelf","sunlight","sweeter","truly","wonderin","yell","yiggy","blink","bloom","brighter","chevrolet","desperate","dollars","knowin","motherfucking","natural","nightmare","prayers","pressure","reflection","sleepless","thirty","underground","upside","woods","back","birthday","blade","blunt","bullshit","complete","hero","hurtin","junior","lack","mass","range","sacred","snap","tread"," "],"legendgroup":"","marker":{"color":"#636efa","size":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0],"sizemode":"area","sizeref":0.01,"symbol":"circle"},"mode":"markers","name":"","showlegend":false,"x":[1.6995059251785278,1.9406745433807373,-0.12173966318368912,0.035872578620910645,-0.11843281239271164,0.07545451074838638,-3.4063520431518555,1.9739305973052979,1.4660042524337769,1.3782153129577637,1.6171702146530151,1.1612000465393066,1.500238060951233,2.0111823081970215,0.06929023563861847,0.867279052734375,0.41998007893562317,0.5313323736190796,-0.9315704703330994,0.3335350751876831,1.3772425651550293,-0.5170290470123291,0.35182279348373413,1.3020060062408447,2.105351448059082,-2.7871716022491455,-0.013027130626142025,0.4771774411201477,0.11784818768501282,-0.4310609698295593,-0.22309766709804535,-0.7340279817581177,-2.3974156379699707,-0.06574535369873047,2.668562889099121,-1.3242902755737305,-2.0156490802764893,-1.5032930374145508,-0.20221281051635742,-0.02734849974513054,0.8039650321006775,1.3887234926223755,-0.3134401738643646,1.2921552658081055,-0.44684019684791565,0.16643348336219788,3.3321800231933594,1.6518287658691406,2.850158929824829,-1.5229374170303345,-3.9679503440856934,3.5636234283447266,0.5423137545585632,0.38365787267684937,1.5315862894058228,-1.6855887174606323,-0.6202724575996399,-0.33202579617500305,4.188085556030273,-0.2730904519557953,-1.5046749114990234,-2.586074113845825,-0.8927977681159973,0.8368072509765625,0.5009186863899231,-0.1719793677330017,-0.8601940870285034,0.5071600079536438,-0.1695549488067627,1.3929270505905151,2.0591232776641846,1.8454748392105103,2.768495559692383,-1.6364859342575073,0.8571687340736389,0.7787779569625854,-1.1309833526611328,0.7632659673690796,2.712526559829712,0.4033794105052948,2.195047378540039,0.5357837677001953,0.08143798261880875,2.561142683029175,0.19735082983970642,0.2326047569513321,-0.734858512878418,2.654890537261963,-0.6319705247879028,0.548969566822052,-0.035350922495126724,0.08518940955400467,0.20451772212982178,1.0818310976028442,-0.2679976224899292,1.1106557846069336,2.2418856620788574,4.799577713012695,-0.3519485890865326,36.77808380126953,4.804645538330078,0.3015158474445343,0.8927000164985657,-1.470966100692749,39.140193939208984,-4.588335990905762,0.4682883620262146,-1.5855119228363037,-1.8654323816299438,0.43552881479263306,1.4090384244918823,-0.27007874846458435,1.3177063465118408,-0.7363564968109131,3.1522154808044434,-1.3206743001937866,0.288497656583786,1.0422760248184204,0.4196038842201233,0.3066492974758148,-1.9118047952651978,1.5621364116668701,-8.368370056152344,1.2372924089431763,1.900536060333252,-0.5236776471138,2.255605936050415,-1.2941067218780518,-2.5880441665649414,-0.5063183307647705,-2.074275255203247,0.8938148617744446,-1.6822142601013184,1.7352025508880615,-2.5872230529785156,2.358368396759033,-0.6202980875968933,-0.22087416052818298,-3.226506471633911,0.1360837072134018,2.8274195194244385,0.4061172902584076,-3.117807388305664,-2.5383450984954834,-2.695401906967163,2.660336971282959,-0.7844515442848206,-0.6987929344177246,-0.46184271574020386,-0.8587643504142761,1.585351586341858,1.2801376581192017,2.276262044906616,2.492042303085327,-6.420569896697998,1.1503889560699463,4.595770359039307,-4.152090549468994,1.9207215309143066,-1.7367854118347168,-0.03275741636753082,-0.857168972492218,5.028644561767578,-0.3773770332336426,0.237786203622818,2.383795738220215,-0.6724620461463928,1.0930479764938354,-1.72630774974823,3.0752899646759033,-0.12807197868824005,1.241741418838501,2.4300544261932373,1.000551700592041,-1.7272531986236572,1.0163930654525757,-0.033757489174604416,-6.573124885559082,-4.010196685791016,3.157745838165283,0.8450120091438293,0.6749720573425293,-6.56807804107666,2.952453374862671,-0.813368022441864,2.1997814178466797,1.5479532480239868,3.553422212600708,-1.0948399305343628,-1.3868298530578613,-0.4533539116382599,2.174217939376831,2.561765432357788,0.04436023533344269,-2.842045307159424,0.3365553319454193,-2.976529836654663,3.5421974658966064,1.0195327997207642,-1.6126904487609863,-1.791494607925415,-0.41334110498428345,0.184705451130867,-1.6057442426681519,-3.3158040046691895,-0.6115790009498596,-2.8427822589874268,0.6820458769798279,1.8858835697174072,-0.4515085518360138,0.06675953418016434,-3.9260473251342773,-1.8438119888305664,-0.7591304183006287,-4.143612861633301,-0.903163492679596,-1.8379336595535278,0.8541869521141052,-1.824211597442627,-0.34857553243637085,-3.655796527862549,1.7475954294204712,4.278853416442871,0.5312531590461731,-7.816947937011719,2.006187677383423,0.7035778760910034,1.3318010568618774,2.464722156524658,-0.6920268535614014,0.44401106238365173,-3.8916232585906982,-1.1869250535964966,1.0338066816329956,4.026699542999268,-0.07306934148073196,-3.1707100868225098,-0.09635458141565323,-6.393095016479492,1.534996747970581,2.6862456798553467,-0.5859373807907104,-3.689532995223999,-2.0066471099853516,0.4278252422809601,0.3823868930339813,1.4702634811401367,2.5225212574005127,-0.17747168242931366,1.0184112787246704,-0.3024462163448334,-2.1158103942871094,-0.3586027920246124,-3.147642135620117,-2.6326823234558105,0.3244806230068207,-2.1961464881896973,0.31568825244903564,-2.1226272583007812,1.7119814157485962,-0.5777193903923035,-0.08914031088352203,-0.9385486841201782,4.704444408416748,-0.12993189692497253,-2.9284465312957764,3.5992088317871094,0.0049493457190692425,5.035700798034668,3.0046615600585938,0.8816647529602051,-2.4777402877807617,0.5467808842658997,0.8769200444221497,1.4100602865219116,-0.33130431175231934,-2.9633562564849854,-0.093485027551651,5.825380325317383,-2.7511653900146484,-4.791096210479736,2.632962942123413,-3.2865116596221924,-2.3611042499542236,1.848691463470459,-2.208487033843994,-0.9056057929992676,-3.5119574069976807,-1.0627245903015137,-0.07441435009241104,2.769138813018799,0.7033182978630066,0.6965830326080322,1.2633693218231201,-4.795852184295654,1.7854418754577637,1.6976436376571655,-2.7465548515319824,-0.496946781873703,0.0757678896188736,-5.588857173919678,-1.2741221189498901,-2.0075736045837402,0.37662801146507263,-5.162201404571533,-0.3098166882991791,-0.739450216293335,-0.8355267643928528,0.31070613861083984,-0.015425012446939945,3.738757371902466,-5.325382709503174,-8.065062522888184,3.089003801345825,-2.1365530490875244,6.004992485046387,1.32464599609375,-0.44686219096183777,3.1863670349121094,-0.8909366726875305,1.046112060546875,-0.24353839457035065,2.6705703735351562,-0.4922071099281311,-8.475784301757812,-0.6616876721382141,1.9412999153137207,6.068378448486328,4.348984718322754,2.7614059448242188,-1.392552137374878,-5.19753360748291,-4.619576454162598,-2.5293123722076416,-1.0252017974853516,-5.349100589752197,0.3475153148174286,3.1650164127349854,3.2870726585388184,3.1913299560546875,-4.118500709533691,0.35151299834251404,1.3972790241241455,-0.8728285431861877,2.7636077404022217,-0.060398880392313004,0.5721859335899353,-1.6537728309631348,-0.8874960541725159,-1.7025657892227173,-1.5226496458053589,2.1470916271209717,-2.411776542663574,-4.993805885314941,-0.7767878174781799,0.29230913519859314,2.26544189453125,2.9617652893066406,-2.0846974849700928,1.5077704191207886,-3.152696132659912,1.5698740482330322,0.6361508369445801,-5.641232013702393,0.23349761962890625,2.205982208251953,-1.692823886871338,-1.3847856521606445,2.491567611694336,-0.919653594493866,2.8502280712127686,1.133082389831543,-0.4168859124183655,-1.7240301370620728,0.7467068433761597,2.754992961883545,3.0271244049072266,-0.7058278918266296,-3.8943593502044678,-2.031270980834961,1.2901049852371216,-2.849646806716919,0.060271427035331726,-1.5420818328857422,1.6261307001113892,-0.1570456176996231,2.9912707805633545,-0.41289272904396057,1.4848302602767944,0.8693858981132507,2.7500743865966797,0.9150644540786743,2.2640888690948486,-3.175119400024414,-2.155123233795166,-8.39661979675293,1.7680811882019043,-4.425253868103027,2.167419195175171,-2.8900747299194336,0.6912316083908081,0.5071381330490112,2.727445125579834,0.8921741247177124,-3.1575798988342285,1.3469316959381104,-1.5422015190124512,1.981420636177063,-0.20090481638908386,-0.426531046628952,-5.768795490264893,3.6660077571868896,-0.06498932838439941,-1.3363456726074219,1.7949848175048828,-6.4595947265625,4.867516040802002,-3.3391666412353516,-4.1696600914001465,1.8222638368606567,0.5418640971183777,0.6144477128982544,-2.1811375617980957,-0.11647741496562958,2.7292373180389404,2.5281150341033936,1.7001811265945435,0.5887854695320129,-2.77983021736145,0.9191842079162598,2.171107769012451,-3.040644407272339,3.8984975814819336,-4.210373878479004,2.6815762519836426,-0.848577618598938,1.6257710456848145,4.114508628845215,3.009347677230835,-0.9387663006782532,-1.3824036121368408,-2.847609043121338,3.859579086303711,2.945044994354248,-9.164511680603027,1.300627589225769,-0.5293194651603699,4.974598407745361,2.4239726066589355,-2.4765069484710693,-9.211256980895996,3.4450321197509766,2.9498469829559326,1.1525923013687134,2.080005645751953,6.007797718048096,-0.21514570713043213,-2.597137689590454,-4.4456787109375,5.486353397369385,-0.7912349104881287,1.24520742893219,-1.6844189167022705,4.113361358642578,4.345670223236084,1.0931649208068848,5.110175609588623,6.480611801147461,-3.516763687133789,3.202293634414673,-3.068220376968384,-2.848881244659424,-4.967443466186523,1.124337077140808,-3.2939391136169434,4.357579708099365,5.0164690017700195,5.201258182525635,5.205567359924316,0.8027799129486084,-1.108290672302246,-1.6953990459442139,-2.994168519973755,0.10504266619682312,3.247816801071167,-2.9668242931365967,-6.230284214019775,-1.3829467296600342,3.298330783843994,0.32426968216896057,7.88744592666626,2.780705451965332,2.6726653575897217,-9.750960350036621,1.3441342115402222,2.9784836769104004,-6.454412460327148,-1.011377215385437,0.5141196250915527,0.06423743814229965,4.911319255828857,3.086454153060913,-2.4965732097625732,-1.906519889831543,-1.3679028749465942,1.392791509628296,-1.8550912141799927,1.404288649559021,0.12790635228157043,-4.644371509552002,0.9954899549484253,4.274538040161133,4.0543718338012695,-1.0285086631774902,-1.3404136896133423,1.1633714437484741,5.182363033294678,-0.3477315902709961,3.678056240081787,-4.0758843421936035,0.7991302609443665,5.287332057952881,2.9002373218536377,1.9355766773223877,-3.570335626602173,-1.2774442434310913,-1.4765783548355103,3.387446403503418,-2.9462015628814697,0.8870668411254883,-3.179392099380493,-3.85860538482666,1.1339268684387207,-12.473369598388672,-0.8218761682510376,7.3448638916015625,2.8328967094421387,-1.8524724245071411,2.8694510459899902,-4.024280548095703,1.9409339427947998,-0.02143755927681923,2.851604461669922,1.5318686962127686,-1.0873517990112305,4.707732200622559,3.7012155055999756,3.4496009349823,-0.10284242779016495,0.5193830728530884,-0.4880211353302002,1.3659842014312744,-3.6136066913604736,-5.393479824066162,-3.736149311065674,-2.894697666168213,2.8198421001434326,-0.4436703622341156,-0.41837215423583984,0.08797668665647507,2.706101179122925,-0.9028229117393494,2.0974924564361572,0.8568336963653564,2.705299139022827,-8.009699821472168,-2.825281858444214,-3.764047145843506,-0.4615664482116699,-3.0258495807647705,-8.41796875,0.25518494844436646,3.821559429168701,-1.198124885559082,2.0630931854248047,-2.0712759494781494,3.5881848335266113,2.5413763523101807,-1.2494313716888428,-1.5611714124679565,-2.0972466468811035,4.559410572052002,-0.3372495174407959,1.8374238014221191,-4.735352993011475,-1.7785130739212036,-8.615769386291504,1.3252756595611572,-3.4786314964294434,4.373807430267334,0.8802254796028137,1.804547667503357,-1.452553629875183,-3.9773547649383545,4.924982070922852,2.2651846408843994,-0.01835945062339306,-4.410627365112305,-1.768602728843689,-2.7210655212402344,-2.5443923473358154,6.69824743270874,-2.645437002182007,-4.107673168182373,3.339603900909424,0.12448587268590927,0.45876482129096985,-1.8044887781143188,-1.6829558610916138,-15.086213111877441,4.266053199768066,-1.8836705684661865,0.7337573766708374,-2.707731008529663,-2.9006543159484863,3.979456663131714,-3.6118903160095215,-2.64762806892395,2.190504789352417,0.4376874268054962,-2.079092502593994,0.2413463145494461,-6.205280303955078,-4.899164199829102,-1.7247586250305176,-8.805187225341797,2.788132905960083,1.8578596115112305,-0.8086163997650146,2.983276844024658,-1.5881822109222412,-8.67251205444336,-1.8649412393569946,-0.5514096021652222,-1.7540079355239868,-0.5115605592727661,-1.598174810409546,1.0744030475616455,0.498583048582077,-1.0457864999771118,3.541865348815918,-6.253194808959961,3.4289937019348145,0.02772931568324566,-2.9087345600128174,0.11234971880912781,0.747025191783905,-1.276953935623169,3.775360345840454,3.4522664546966553,-4.05693244934082,7.0216569900512695,-1.7567641735076904,-0.8596468567848206,2.7523844242095947,-3.970747947692871,0.4480971693992615,-2.3760504722595215,-4.230010032653809,2.802311897277832,-3.5110585689544678,-2.0010907649993896,3.481215476989746,1.4568262100219727,-1.365818738937378,4.099803447723389,0.0989290326833725,3.5297272205352783,-3.568366527557373,0.7245394587516785,0.20768237113952637,-0.9782758355140686,-16.671131134033203,1.2022359371185303,2.5189449787139893,1.30103600025177,-0.26166248321533203,-0.5171424746513367,2.256800413131714,1.9883668422698975,-4.933732032775879,-2.818802833557129,5.082000255584717,1.2759621143341064,1.4432698488235474,2.5766501426696777,2.2489547729492188,-2.0406038761138916,3.603762626647949,-3.0953032970428467,2.038203239440918,-3.97316837310791,-1.0772182941436768,-2.4835474491119385,-2.417823314666748,-0.8263788819313049,4.63776159286499,0.5100556015968323,2.6560311317443848,1.7053385972976685,-3.034966230392456,0.18243764340877533,-1.3456045389175415,-0.7201623320579529,3.8071951866149902,2.296431064605713,-4.47169828414917,-2.7256901264190674,4.261474132537842,-1.699303150177002,7.453990459442139,2.4580748081207275,5.408223628997803,5.166528224945068,0.7186464071273804,-6.483591556549072,3.8140461444854736,3.2689270973205566,2.339170217514038,-3.9687998294830322,5.885075092315674,0.18131199479103088,-1.898805856704712,-2.265512704849243,-3.768742799758911,-0.28547635674476624,-4.595541000366211,2.948078155517578,4.089010715484619,2.6296002864837646,-2.8104248046875,0.037699297070503235,-2.7409274578094482,3.2102556228637695,-8.508746147155762,3.0455760955810547,6.572522163391113,-0.5119708180427551,0.4660626947879791,1.828127384185791,1.1432305574417114,-6.747279644012451,8.194195747375488,-1.4216333627700806,-1.9284874200820923,-0.9277052879333496,1.849745512008667,3.177178144454956,0.658642053604126,-0.8656023144721985,-2.5535714626312256,-0.21191468834877014,-0.12776903808116913,0.1955832540988922,-0.8576517701148987,-2.8963570594787598,-3.063178777694702,3.808685302734375,-2.4563281536102295,1.3075913190841675,-1.295256495475769,-3.3245508670806885,-1.6936590671539307,2.5406527519226074,2.920380115509033,-1.0161659717559814,-3.0508525371551514,-3.886314868927002,1.9381732940673828,6.5693678855896,-2.2481367588043213,0.016288997605443,-3.4732272624969482,0.9846822023391724,-0.024672577157616615,2.5236220359802246,-1.0887387990951538,-0.41568389534950256,0.4529587924480438,2.551992416381836,-0.8119215965270996,-6.342216968536377,1.224023461341858,2.4053704738616943,0.590916097164154,1.2074627876281738,-2.897150754928589,-2.3919787406921387,5.360910415649414,3.374244451522827,-2.0640289783477783,1.4166961908340454,-4.671737194061279,-0.22397631406784058,-4.695302486419678,-0.7816755175590515,-0.21366670727729797,4.041713714599609,8.57574462890625,-2.4645631313323975,-3.725598096847534,-0.9287117123603821,2.2512481212615967,0.4616882801055908,3.1673743724823,1.1682777404785156,-0.28271177411079407,-3.2676610946655273,3.247633695602417,-1.0983366966247559,0.8699164986610413,2.448610544204712,-9.246732711791992,1.8513290882110596,2.721064567565918,2.765667200088501,1.6644260883331299,-0.36083847284317017,-6.876705646514893,1.7002041339874268,-2.273397922515869,4.46538782119751,3.3581573963165283,-2.7166290283203125,0.004415108822286129,-10.772294998168945,-2.0978522300720215,2.9655263423919678,-4.12235689163208,-3.886996030807495,0.09002570062875748,3.0690407752990723,2.6588757038116455,-1.4110078811645508,1.2913053035736084,5.43192195892334,-3.021822929382324,-5.407227516174316,-0.01316614169627428,0.38449475169181824,2.4502170085906982,-6.957235813140869,-1.3392151594161987,-5.5236735343933105,-3.808528423309326,3.99908447265625,3.2185404300689697,1.9091720581054688,1.8122421503067017,-6.42324161529541,2.888608455657959,-2.339097023010254,-2.2247374057769775,0.022939080372452736,-1.2008719444274902,1.462268590927124,6.9454545974731445,-5.74730920791626,-1.5226095914840698,-3.1787312030792236,-0.04451589286327362,0.6287513375282288,-0.17754073441028595,0.4245130717754364,1.1525691747665405,-0.6851840615272522,2.8243794441223145,-0.9190956950187683,-6.617860317230225,-3.142723560333252,0.08116678148508072,2.3167724609375,3.8631436824798584,-1.4287580251693726,-1.9870710372924805,-1.2096036672592163,-1.5902907848358154,2.2906246185302734,2.24743390083313,-4.715816020965576,5.899677753448486,3.0892322063446045,-1.4770218133926392,4.861459255218506,4.224470615386963,5.8648576736450195,1.6607509851455688,2.7869887351989746,1.3897881507873535,-3.2133724689483643,0.4400586485862732,4.512856960296631,0.12181000411510468,-3.47007155418396,-2.638246536254883,-9.881497383117676,1.7925547361373901,4.5901312828063965,-0.5193463563919067,2.9502780437469482,5.115223407745361,-0.1169518232345581,0.0667777955532074,3.017918348312378,-2.290902614593506,-1.8571250438690186,0.1500205397605896,-3.114410400390625,1.1626670360565186,-0.2892932593822479,-8.402587890625,1.2040668725967407,-2.4970710277557373,-7.7495198249816895,1.5668443441390991,-0.06433313339948654,3.0947394371032715,4.296990394592285,4.367506980895996,4.695237159729004,-2.1200766563415527,-7.853061199188232,1.7816219329833984,2.117906332015991,4.031605243682861,-3.8608930110931396,3.1727943420410156,1.8049410581588745,2.713435649871826,-1.0043448209762573,-6.987863540649414,-4.1461405754089355,1.2395458221435547,2.601715564727783,0.9478702545166016,0.22486759722232819,-0.10816200822591782,1.501002311706543,4.684719085693359,0.44607678055763245,-0.5196811556816101,-2.5189332962036133,5.314509868621826,-2.3397624492645264,2.194441556930542,0.3276628255844116,-2.0174496173858643,2.932821750640869,0.7612712979316711,1.115049123764038,-1.0659273862838745,4.554991245269775,-1.78782320022583,5.460937023162842,2.402953863143921,-1.6482514142990112,3.680861711502075,-0.29047760367393494,-3.464385986328125,-0.7445377111434937,4.429631233215332,-3.088209629058838,-0.7778899073600769,1.7531925439834595,7.373058319091797,-0.8604730367660522,-3.471385955810547,-1.3156323432922363,-4.3807291984558105,2.585681676864624,0.7861326932907104,3.1088287830352783,1.1680538654327393,5.656093120574951,-2.086625814437866,0.7185921669006348,-2.649348735809326,2.334327220916748,-6.307701587677002,0.1708165556192398,2.0969078540802,-0.8652932047843933,0.8879024386405945,2.3343892097473145,-1.2474734783172607,-0.8677831888198853,-0.38268178701400757,4.043359279632568,-1.7371629476547241,-4.7838826179504395,-4.167636871337891,1.3033071756362915,6.6473612785339355,-2.1839146614074707,2.8069419860839844,1.750462532043457,-2.618161916732788,2.247741937637329,-2.8572282791137695,8.997452735900879,-2.1198418140411377,4.555812358856201,0.7217230200767517,0.5667783617973328,-0.18862652778625488,1.5261895656585693,-5.055846691131592,-2.1891844272613525,2.2650933265686035,6.883592128753662,0.38323694467544556,-2.7493820190429688,-0.07383427768945694,-0.44772517681121826,-1.0256054401397705,0.23860742151737213,-1.339273452758789,-2.836268186569214,-2.364452362060547,-0.47943204641342163,2.666266679763794,-0.6260800957679749,-0.09426075220108032,-2.2745606899261475,3.6609649658203125,4.27014684677124,1.1405243873596191,-2.3021490573883057,1.340903639793396,-2.8668053150177,-1.6745610237121582,-0.3252062499523163,1.4025802612304688,3.3400139808654785,-3.3456814289093018,0.8802928924560547,3.8347160816192627,-1.3482383489608765,-6.294110298156738,-3.6130239963531494,-5.145914554595947,-0.21459056437015533,0.3526163101196289,-3.9942502975463867,-5.402000904083252,-0.5157928466796875,1.0300440788269043,-0.22198715806007385,-1.3055044412612915,-2.3128325939178467,3.613431930541992,0.7212842106819153,7.635899543762207,0.18950481712818146,-1.8654085397720337,3.3838071823120117,-0.32461339235305786,-0.28265616297721863,-1.8816940784454346,7.496301651000977,-1.6311336755752563,-0.2337043136358261,-8.906037330627441,1.65891695022583,-2.1926913261413574,-3.892504930496216,3.4647419452667236,1.0443239212036133,-1.318305492401123,-3.302673101425171,1.2756439447402954,2.4627838134765625,6.753573894500732,1.1596530675888062,-4.354905128479004,-2.8025624752044678,-1.9193817377090454,1.012284517288208,2.929067611694336,-3.033433675765991,-2.987562656402588,2.8227739334106445,-0.7338098883628845,-4.115737438201904,1.1891568899154663,-7.091860294342041,-1.524877667427063,-3.815249443054199,2.5801892280578613,-1.249523401260376,1.8726578950881958,-2.346129894256592,0.40986377000808716,2.2648260593414307,-0.0070009371265769005,-1.604937195777893,7.217586517333984,-1.3271516561508179,-0.585703432559967,2.7658274173736572,-2.8924245834350586,-1.8236340284347534,-4.139923572540283,-3.5787601470947266,3.5206470489501953,6.932949542999268,3.143165111541748,-3.6213912963867188,-5.5444207191467285,-3.384427547454834,-0.706948459148407,0.7318337559700012,-5.718910217285156,1.0625590085983276,8.258378028869629,-2.5068159103393555,1.5195633172988892,-4.486362934112549,1.782439112663269,-1.9256840944290161,1.396248698234558,1.5169998407363892,-4.003109931945801,1.645303726196289,1.053481101989746,-2.7927913665771484,1.2880898714065552,-1.2910795211791992,-6.6060261726379395,5.1051483154296875,-0.8177725672721863,4.308004856109619,1.6388636827468872,-1.5370237827301025,5.9691081047058105,-5.683413505554199,-0.7604551315307617,-1.9509674310684204,-0.678397536277771,-0.3605725169181824,6.408292293548584,1.3007891178131104,3.229736089706421,-0.9570712447166443,1.1275265216827393,-2.716759204864502,0.15387114882469177,2.1760458946228027,-3.0838232040405273,-3.3855714797973633,4.536764144897461,-4.251428127288818,-7.9719014167785645,-3.551424026489258,-5.996895790100098,3.0596272945404053,-0.19970963895320892,0.4997033178806305,2.0367841720581055,0.8607282042503357,-9.913108825683594,-2.807054281234741,1.2500841617584229,0.6727224588394165,-0.6728910803794861,1.419755458831787,2.930925130844116,-1.581751823425293,3.8288912773132324,1.1097956895828247,2.913640260696411,-4.69462776184082,-3.5492634773254395,1.9166339635849,-0.015635421499609947,4.230494022369385,0.5483354926109314,5.095211505889893,-4.527791500091553,-0.4480118453502655,4.039353847503662,-1.8291231393814087,2.6732468605041504,4.890900135040283,-1.3427057266235352,0.5752979516983032,4.642094612121582,4.1250457763671875,-1.0806390047073364,4.500986576080322,4.155993461608887,-0.6355438232421875,-1.5608930587768555,0.6178154945373535,0.12741559743881226,11.70093822479248,1.3333852291107178,-5.8946452140808105,4.825130939483643,0.22063493728637695,4.5068535804748535,-3.599403142929077,-3.7300939559936523,-5.142202377319336,3.6744167804718018,-1.3519822359085083,0.4909569025039673,0.7586514949798584,-4.682286262512207,-3.3563072681427,4.669466972351074,-0.8612692952156067,-1.8583552837371826,5.890474796295166,-1.2969101667404175,4.671620845794678,3.3609697818756104,-3.0943243503570557,0.0850691944360733,5.176321983337402,1.6364648342132568,-4.760904788970947,-1.3927812576293945,0.06727278232574463,5.345211505889893,3.299010753631592,-4.412222385406494,0.3972754180431366,-5.44537878036499,5.753441333770752,1.1788733005523682,2.7933688163757324,1.7618402242660522,1.8501427173614502,2.7886834144592285,1.9961310625076294,1.4692939519882202,1.9166531562805176,6.766496181488037,4.73324728012085,-6.350697994232178,-2.856508731842041,-4.09020471572876,-0.09816814213991165,-4.783427715301514,0.2726486623287201,6.703761100769043,5.264937877655029,-0.8373537659645081,-6.363037109375,-8.615543365478516,-3.11746883392334,1.5858529806137085,4.917567729949951,-2.0579371452331543,-2.788478374481201,3.5335347652435303,0.49653899669647217,5.38320255279541,8.404579162597656,3.035414934158325,-8.541099548339844,0.3884785771369934,3.026559829711914,-1.5354043245315552,-2.3996427059173584,-5.08775520324707,5.034589767456055,-4.728330135345459,-0.5420781373977661,-2.9049994945526123,-4.106952667236328,-1.7770757675170898,0.035961780697107315,3.347050189971924,1.6944258213043213,1.7297232151031494,-1.4721808433532715,0.5725987553596497,-6.030242919921875,-4.593624591827393,2.011528491973877,0.48140713572502136,-5.571067810058594,-1.7762047052383423,-8.039122581481934,0.31280314922332764,-2.7216596603393555,-3.0390586853027344,0.8099053502082825,-7.564577579498291,-3.202829599380493,3.547805070877075,3.741595983505249,-4.011846542358398,0.7904600501060486,-4.681108474731445,-6.544406414031982,-1.3074042797088623,1.4292176961898804,-0.04492180421948433,2.0361669063568115,-1.53819739818573,0.5029553771018982,5.947168827056885,1.845304012298584,6.7937774658203125,-0.46513280272483826,-0.020479490980505943,-2.322838306427002,-5.7350239753723145,-1.788389801979065,-2.271500825881958,2.011357307434082,2.4835355281829834,-3.3818199634552,0.11539223045110703,-1.1664460897445679,1.7906434535980225,-3.3410778045654297,-3.643784761428833,-7.4690423011779785,-6.56914758682251,6.123844146728516,2.694255828857422,6.655964374542236,-1.8575884103775024,4.262815952301025,3.115356922149658,-1.8189769983291626,-3.4482924938201904,4.436944961547852,-2.0304441452026367,-0.28612029552459717,-2.3934855461120605,4.574805736541748,-3.332017183303833,2.3879947662353516,1.258185625076294,-0.26865413784980774,-6.655214786529541,2.5377423763275146,-5.189742088317871,-1.8855398893356323,-3.4074954986572266,-5.063748359680176,-1.4218288660049438,3.2837672233581543,1.0945936441421509,4.700606346130371,4.544920921325684,0.6564035415649414,-0.7848002314567566,-0.8242366313934326,4.203673362731934,-0.9846176505088806,-4.625539302825928,-0.8144856691360474,-0.04304980859160423,4.317598819732666,3.9520771503448486,2.2636241912841797,1.5220335721969604,-0.2777344286441803,-2.925720453262329,-1.3315706253051758,2.216937780380249,6.143080234527588,-2.5993552207946777,-1.2309356927871704,2.7143688201904297,-0.4495411217212677,0.1668587327003479],"xaxis":"x","y":[0.8694708347320557,2.996119260787964,2.815622329711914,-1.1954847574234009,1.5622974634170532,-1.845407485961914,-3.460927963256836,-1.3695660829544067,0.3746860921382904,1.2877014875411987,-1.1250147819519043,-0.2601541578769684,-0.6378770470619202,-0.5605581998825073,0.3482101857662201,-2.3418684005737305,-2.1951048374176025,0.22343704104423523,0.06662745028734207,-1.618050217628479,2.0272765159606934,0.2600097358226776,-0.7223504185676575,-0.009808179922401905,0.8008738160133362,-2.9954941272735596,-1.405261754989624,-0.02134554088115692,-0.8132045269012451,-1.5354334115982056,-0.7535904049873352,-1.2751202583312988,-2.8926267623901367,-0.8991247415542603,0.04121384024620056,-2.177772045135498,-2.1343276500701904,-1.4774116277694702,1.1914491653442383,-0.38005197048187256,1.0018349885940552,0.8460996150970459,0.5045467019081116,-0.8674473762512207,-1.6534688472747803,-1.2795734405517578,-2.076385974884033,1.3738858699798584,-0.9763845205307007,-0.491506427526474,-3.310237169265747,-0.19184669852256775,-0.6978062987327576,-3.6217873096466064,-1.7309409379959106,-1.6490182876586914,2.5052437782287598,1.3919312953948975,-0.10760810971260071,0.4442489743232727,0.5847231149673462,0.16820049285888672,-0.8080828785896301,1.2523857355117798,-0.7212403416633606,-1.1294671297073364,-4.266604900360107,-0.9085303544998169,-0.7719676494598389,0.48043563961982727,-0.5355837345123291,0.9365022778511047,-0.48173460364341736,-1.9479950666427612,2.223482370376587,0.7903945446014404,1.2181065082550049,1.8973480463027954,0.13481637835502625,-0.11286339908838272,1.2009174823760986,-1.0098801851272583,0.6590168476104736,-2.567831039428711,-1.5002325773239136,-1.361339807510376,-2.6289446353912354,3.857398271560669,-0.770554780960083,-0.33206456899642944,-0.691431999206543,-0.9696308374404907,1.7835915088653564,-0.058492712676525116,-3.060736656188965,-1.6545355319976807,-4.5171332359313965,-2.90091609954834,2.955726146697998,7.126415252685547,-0.2304159700870514,-4.410159587860107,-2.7498910427093506,0.2214759737253189,8.14641284942627,1.5776335000991821,-0.4164043366909027,-4.183259010314941,-2.4679250717163086,-3.124063014984131,0.9095832705497742,4.199721336364746,-1.1116158962249756,0.05666979402303696,-3.1618151664733887,-1.82895028591156,-0.6473932862281799,-0.33291876316070557,-0.490764319896698,-1.6971269845962524,-0.4793533682823181,-2.0106942653656006,0.8941130638122559,-1.5644510984420776,-0.6728526949882507,-1.0273220539093018,0.7914055585861206,-2.7771458625793457,-0.3011709153652191,-0.21443727612495422,1.2358654737472534,2.580787181854248,1.5307317972183228,-1.0579099655151367,-3.853879928588867,-0.4790601134300232,-2.3867361545562744,-0.6140120029449463,-3.7887213230133057,-3.6383321285247803,8.435462951660156,4.8930487632751465,-1.1817010641098022,0.7574959397315979,2.1291589736938477,2.966512441635132,-0.6178267002105713,-0.39338016510009766,-1.4908366203308105,0.4758494198322296,1.3039946556091309,-0.8321311473846436,0.8279938101768494,-2.693668842315674,-3.8239693641662598,4.316201686859131,2.629209280014038,0.26681438088417053,1.271407961845398,1.5863391160964966,1.4941567182540894,-1.1823490858078003,-1.5990465879440308,-1.2295129299163818,-1.4523766040802002,2.2730555534362793,0.7306851148605347,-0.6224253177642822,-3.7913620471954346,-0.1346360743045807,-0.506352424621582,0.8348580002784729,1.3999375104904175,-0.8797750473022461,-0.15546461939811707,-0.42871662974357605,-2.5870678424835205,-0.37786611914634705,-2.8268823623657227,2.561802387237549,1.9246468544006348,0.23516429960727692,-2.062471628189087,-1.5306432247161865,-1.577721118927002,0.3452131152153015,-0.13997530937194824,1.265523076057434,-1.4352748394012451,1.8360539674758911,0.029755352064967155,0.4076039791107178,-0.671973466873169,-0.4816071093082428,1.5988272428512573,1.1312096118927002,0.2430802285671234,2.1073224544525146,-2.747068166732788,2.193481683731079,-0.30173367261886597,0.11518801748752594,1.9439983367919922,0.5471686720848083,-0.35967686772346497,-0.9394145011901855,-1.5260270833969116,8.729360580444336,1.0198376178741455,-0.6822750568389893,-1.8133008480072021,-1.335668921470642,0.6262459754943848,3.6733837127685547,-0.694953978061676,-1.2566341161727905,2.4749245643615723,0.7895696759223938,-0.854773759841919,-0.6105740070343018,6.285892486572266,-0.3209371566772461,0.7560034990310669,2.1649792194366455,0.15781213343143463,-1.0035433769226074,-2.1913204193115234,-0.05490298196673393,-0.14679217338562012,-1.1572062969207764,-0.761897087097168,-0.7738943696022034,-1.2287311553955078,-0.12771622836589813,-1.4549115896224976,-0.9458336234092712,0.8413608074188232,-0.7882797718048096,-2.206298589706421,-1.1111291646957397,1.1953502893447876,-1.1308631896972656,-2.8289506435394287,4.457761287689209,2.3059937953948975,-2.4895637035369873,2.258392095565796,-0.32328781485557556,2.2468101978302,-0.8194273114204407,-3.8115394115448,0.36795729398727417,0.27432331442832947,4.159948825836182,-0.5097637176513672,-0.12617658078670502,-0.529747486114502,-2.4021642208099365,-5.260644912719727,-1.087950587272644,-4.847179889678955,-1.407091736793518,0.2416086494922638,-2.1087231636047363,0.7603581547737122,-0.39156273007392883,2.160339593887329,0.9344603419303894,-3.429111957550049,-2.219313383102417,0.06919344514608383,-3.0900096893310547,0.2352093756198883,-1.5673680305480957,2.7706453800201416,-3.1069209575653076,-4.489226818084717,-0.15670666098594666,-0.6594967842102051,-3.239375352859497,0.8559631705284119,1.822306513786316,-1.5670174360275269,-0.05015510693192482,2.538581132888794,2.09191632270813,1.1736406087875366,-1.000722885131836,0.6780276894569397,1.0269854068756104,1.8436230421066284,3.2709410190582275,-1.1041008234024048,2.8550469875335693,-5.392828464508057,-2.5652170181274414,-3.0121405124664307,-2.938840627670288,-5.883024215698242,-4.279268741607666,3.1927437782287598,-2.0498106479644775,0.9408297538757324,2.781146764755249,-3.7098898887634277,2.2415263652801514,-1.1283936500549316,-0.2865675091743469,2.2228317260742188,0.1494932770729065,-2.1471807956695557,1.3208354711532593,-0.6428682208061218,0.30156782269477844,1.650413155555725,2.1573073863983154,2.1991488933563232,-0.6373098492622375,-2.6660430431365967,0.06134524196386337,0.2594722509384155,0.3777429759502411,0.5688411593437195,-3.3093888759613037,0.8681575655937195,-0.04902981221675873,1.7672230005264282,-0.9109614491462708,-1.4888110160827637,-3.8773679733276367,-1.774387001991272,3.5374484062194824,1.1476526260375977,-3.018620491027832,1.3698540925979614,5.662445545196533,-1.9898871183395386,-0.19819380342960358,-1.712641954421997,-1.158553957939148,1.2166039943695068,-0.5496259331703186,1.406913161277771,1.0968098640441895,-0.6741600632667542,7.236550807952881,1.7534723281860352,0.34146401286125183,-1.570701003074646,-3.6775097846984863,1.820672869682312,-2.1785268783569336,-2.0172817707061768,-1.595584750175476,-1.1522769927978516,0.7660521864891052,-0.8220889568328857,-0.9813430309295654,-1.7694767713546753,2.4272186756134033,2.853292226791382,-0.2723845839500427,0.689851701259613,-0.7446726560592651,0.6236824989318848,3.3747146129608154,0.9007624983787537,1.321948766708374,5.600836277008057,1.712926983833313,-0.5095086097717285,0.8122243285179138,1.4101886749267578,1.7913867235183716,0.9792684316635132,0.7530383467674255,1.0453448295593262,3.812777519226074,-1.3182883262634277,4.012703895568848,-0.9658846259117126,0.9811657071113586,1.7758609056472778,0.035638440400362015,2.4261844158172607,-1.1060121059417725,-3.8341281414031982,-4.83658504486084,0.808627724647522,0.3059088885784149,-0.908168613910675,-1.6110060214996338,-0.6610037684440613,1.8972944021224976,2.1506900787353516,-3.6788816452026367,-0.5605013370513916,-1.9152311086654663,-0.4658626914024353,3.9070005416870117,-0.16573838889598846,0.4364311993122101,2.1516411304473877,4.797669410705566,2.8221912384033203,-1.2262544631958008,5.89559268951416,-2.215810537338257,-0.9504077434539795,-0.8525049686431885,4.03191614151001,-0.26355111598968506,-1.467753291130066,5.40432071685791,-2.9465932846069336,-1.7770966291427612,-3.9107911586761475,-2.7362756729125977,7.657130241394043,0.5641249418258667,1.247023582458496,3.1375935077667236,1.8719370365142822,3.148235559463501,-3.8335015773773193,-4.940777778625488,6.082406044006348,0.3180052638053894,-2.6722488403320312,1.4207768440246582,2.7197556495666504,2.6397509574890137,-2.018606185913086,-1.0744150876998901,-1.1487599611282349,0.7756173610687256,2.176572322845459,0.6078751683235168,-3.5665862560272217,-0.6659103035926819,1.92134428024292,3.163961410522461,0.5115735530853271,0.5192679166793823,-0.7340417504310608,-3.1756279468536377,-1.0235921144485474,-3.642071008682251,-0.04721621796488762,0.324858158826828,-1.1154124736785889,1.0335984230041504,-3.238539457321167,1.599147915840149,-1.0510749816894531,2.2892355918884277,-1.6936681270599365,0.3187503218650818,2.8078439235687256,-3.914705991744995,-0.8217913508415222,1.668858289718628,1.005751609802246,1.6401653289794922,-2.4018542766571045,2.061182975769043,0.21031267940998077,-1.6527976989746094,1.0228394269943237,-1.950645923614502,1.197016954421997,-0.3227798044681549,-0.4084412753582001,-3.246195077896118,0.03547853231430054,-4.09659481048584,-2.1871869564056396,2.4079318046569824,2.0480833053588867,2.5201382637023926,0.36535942554473877,2.564008951187134,-0.5308527946472168,0.5418288111686707,-1.9670802354812622,1.21895170211792,0.6874061822891235,0.8692610859870911,-2.6760995388031006,1.194308876991272,0.09214948862791061,-2.083911895751953,-0.22251902520656586,0.17657171189785004,5.319089889526367,1.4361445903778076,6.156388759613037,0.2928007245063782,-2.0989584922790527,-3.1527607440948486,-0.3967934846878052,-2.447906732559204,-2.480476140975952,-0.16698111593723297,-3.2178614139556885,-1.369267463684082,2.442538022994995,-1.8154720067977905,-0.1696503907442093,1.2723653316497803,-0.7839028239250183,2.6237807273864746,0.36620280146598816,0.4773859679698944,-2.3204097747802734,4.03815221786499,-0.41516315937042236,1.2819781303405762,-1.1519132852554321,2.31540846824646,1.2290085554122925,0.9486817717552185,-2.18288516998291,-2.6215476989746094,3.8922994136810303,-3.7265565395355225,-3.4776625633239746,-3.7086849212646484,-1.6432236433029175,0.36618706583976746,5.357827663421631,-2.017585039138794,0.07427804917097092,2.1310389041900635,-1.2376582622528076,-0.18714271485805511,2.1691184043884277,2.098464250564575,2.1627697944641113,-2.782449960708618,-2.6648480892181396,2.593906879425049,-3.5723602771759033,-0.16523241996765137,-3.8979570865631104,-5.097107410430908,6.495848178863525,0.7164801359176636,0.4673803448677063,-2.433201551437378,4.421757221221924,-0.47083255648612976,-1.2309514284133911,-1.3735495805740356,3.55976939201355,-1.975500226020813,-0.6468795537948608,1.405555009841919,3.9332985877990723,0.03837283328175545,-0.4393451511859894,-1.360207200050354,0.5388351082801819,-1.2274316549301147,0.5045048594474792,0.7134578227996826,-4.949018955230713,-4.950201034545898,-2.9008307456970215,-0.021723901852965355,-1.0577698945999146,-3.371073007583618,1.2969729900360107,-0.48016348481178284,0.7671273946762085,0.2484133243560791,0.5510039925575256,-0.7792974710464478,-0.04595155268907547,0.48485231399536133,-5.054408550262451,-0.8148693442344666,1.2116843461990356,-2.0050017833709717,1.3611149787902832,-0.33767950534820557,3.663893699645996,0.1047326996922493,-1.1535128355026245,-0.3573281466960907,2.2919390201568604,-0.42145010828971863,3.7163712978363037,0.3847408890724182,1.9646469354629517,0.40298226475715637,1.4451960325241089,2.822427272796631,-0.4152815043926239,-1.374292254447937,1.724871277809143,0.6642205715179443,-0.905770480632782,-2.2713072299957275,-2.0329177379608154,-0.39253148436546326,-1.2622121572494507,1.008161187171936,-1.2375601530075073,-0.2714633047580719,0.01818006858229637,2.374206066131592,1.7728325128555298,2.1564571857452393,-3.149298906326294,2.9575393199920654,2.3457977771759033,2.700953960418701,1.483498454093933,-0.22971655428409576,-2.6127052307128906,-0.48582202196121216,1.1243928670883179,-0.9055498838424683,3.6095547676086426,3.520113706588745,-1.698996901512146,-0.4183657169342041,3.201005458831787,1.3394542932510376,-1.3195217847824097,-1.8289467096328735,-2.276944160461426,1.8439505100250244,-0.5261495113372803,0.121628537774086,-2.838210105895996,0.9136218428611755,0.6451017260551453,-1.8326997756958008,1.9654154777526855,2.496654748916626,1.627010703086853,1.7852814197540283,0.24492648243904114,4.952192783355713,-2.465991497039795,-0.8217288255691528,-3.0926175117492676,3.3181471824645996,-2.261265993118286,-3.340059280395508,-2.3040049076080322,4.907275199890137,-3.7034573554992676,2.517284393310547,-0.5777310132980347,-1.146620273590088,1.2494781017303467,4.430436134338379,4.6443891525268555,-0.4669678807258606,-3.4992895126342773,-6.643031120300293,-4.177077770233154,1.1398050785064697,-2.8136394023895264,1.0755201578140259,1.5742955207824707,-2.719827651977539,-0.04547703638672829,3.591562032699585,3.250333786010742,2.153728723526001,-3.657949924468994,-3.243208169937134,2.3914151191711426,-3.3443808555603027,1.2026623487472534,0.8519225120544434,-1.7981804609298706,1.306283712387085,10.81544303894043,2.37799334526062,5.6448211669921875,-0.03230420500040054,-0.08565189689397812,2.2563960552215576,-1.583066701889038,0.05332815274596214,1.7129361629486084,-1.6253225803375244,1.3836578130722046,-1.4247974157333374,0.839956521987915,-2.5002377033233643,-2.2972819805145264,-1.824134349822998,2.0847086906433105,-0.036023419350385666,-0.06313811242580414,-0.03709246590733528,-3.4094457626342773,-2.0986759662628174,-0.3017728924751282,-2.4107186794281006,1.1489458084106445,0.7805357575416565,4.357479095458984,1.9526002407073975,-0.8868429064750671,2.62382435798645,-1.0415453910827637,0.3344433903694153,-2.0446016788482666,-1.5170021057128906,-2.121019124984741,-2.3164892196655273,-3.5278162956237793,0.5455809235572815,-0.9425843358039856,-1.4537633657455444,3.1327552795410156,1.0755592584609985,0.36173397302627563,0.37002062797546387,8.04270076751709,-1.123498558998108,0.8390033841133118,-1.4836792945861816,5.92299222946167,2.292310953140259,-1.2270619869232178,-1.1040277481079102,1.8662149906158447,2.4129438400268555,2.312509775161743,-2.9240312576293945,0.9207028150558472,0.021029211580753326,-1.522147297859192,-2.194134473800659,-0.6531227231025696,-1.0828684568405151,0.9254307746887207,0.006915336474776268,0.07744143158197403,3.0560383796691895,1.470015287399292,1.4518041610717773,-2.4860453605651855,1.8607827425003052,2.1707229614257812,0.6379968523979187,2.604449510574341,4.218001842498779,-0.9396244883537292,0.3629032373428345,4.9298224449157715,-0.5457403659820557,3.0513105392456055,-2.7773818969726562,3.0465734004974365,2.561295747756958,-2.70894455909729,-2.72621488571167,-0.40538257360458374,-0.483574241399765,-0.40438705682754517,2.652750253677368,-0.6611615419387817,-3.409391164779663,0.9063512086868286,-2.7419917583465576,3.0469934940338135,-1.0819669961929321,3.5234594345092773,-1.339093804359436,4.545552730560303,1.22939133644104,2.15297532081604,-3.3556151390075684,0.34934327006340027,-0.13339322805404663,1.6114673614501953,-1.5669387578964233,1.0714211463928223,4.195094108581543,-3.0755412578582764,-0.37957316637039185,-0.5136317014694214,-0.02132556401193142,0.2083469033241272,-1.6926071643829346,2.2245476245880127,1.9165807962417603,-2.6315338611602783,0.16455288231372833,-3.565002679824829,0.9809302091598511,-2.065999984741211,2.2095253467559814,-0.6795826554298401,-2.8175487518310547,-1.8957685232162476,2.027508020401001,-0.21696963906288147,0.002665229607373476,-3.9957964420318604,1.5699301958084106,-0.8283367156982422,-0.5361363291740417,-0.3105134963989258,-0.42128029465675354,-0.18846897780895233,-1.972064733505249,0.7137831449508667,-2.777719020843506,-1.0693919658660889,0.10627710074186325,-0.7595224380493164,-2.197047472000122,5.352449893951416,2.4630978107452393,-0.20070011913776398,2.2583839893341064,0.9011811017990112,-0.9693353176116943,-3.1396965980529785,0.21021249890327454,-3.308082342147827,-1.3222705125808716,-0.5408174395561218,-4.227806568145752,-2.446925401687622,-1.9043586254119873,-1.0129519701004028,1.5475269556045532,-2.834747076034546,6.27354097366333,5.6397881507873535,-0.07095362991094589,-0.1038321852684021,1.8699451684951782,-1.8237335681915283,-0.5184738636016846,-1.4718446731567383,-1.3928767442703247,-0.9601389765739441,0.18983043730258942,3.7808661460876465,-2.8149337768554688,-1.061095118522644,-2.34610915184021,3.7756659984588623,1.3796517848968506,-1.0598540306091309,-2.629054546356201,-0.7339480519294739,-4.9692606925964355,-0.37107735872268677,-2.027651071548462,2.9463632106781006,-3.1712632179260254,-1.2179526090621948,3.973966598510742,1.116291880607605,0.8070346713066101,-0.6706216335296631,0.651174008846283,1.3359322547912598,2.143768310546875,2.09460186958313,2.8578197956085205,2.5245447158813477,3.0200605392456055,3.863293409347534,-0.16735903918743134,-1.0790237188339233,4.827416896820068,-0.2227851152420044,2.245046615600586,0.10696754604578018,-1.8831945657730103,-0.0033194436691701412,-1.4021326303482056,2.679131269454956,0.036231521517038345,3.23441481590271,0.13620291650295258,2.094756841659546,-0.8992837071418762,-3.0349719524383545,-3.6431374549865723,-0.8099656105041504,-2.6153717041015625,-0.29565995931625366,0.41858533024787903,1.582600474357605,2.246112585067749,-2.3196873664855957,-1.449666976928711,-0.9045727252960205,-2.849287509918213,3.0552375316619873,1.4163419008255005,-3.377077579498291,-2.9081003665924072,-0.8692387342453003,0.4608389735221863,0.06549084931612015,-0.5704502463340759,2.428644895553589,-0.8279678225517273,-0.5812512040138245,0.6977270245552063,1.9143012762069702,-0.8085545897483826,-4.900876045227051,2.4051499366760254,0.3512952923774719,-0.38580983877182007,-1.2675155401229858,0.6264578104019165,0.8411076068878174,-1.356038212776184,-1.6531813144683838,-4.027482986450195,2.356688976287842,3.495650053024292,1.362724781036377,1.302780032157898,-0.47246938943862915,4.79101037979126,-1.6556788682937622,-0.953707218170166,2.5449609756469727,2.874830722808838,-3.7846803665161133,-4.830830097198486,-0.06590887904167175,0.9145386815071106,3.2040557861328125,-3.050123929977417,-0.2374897301197052,-1.7106353044509888,-0.48698896169662476,-0.7467425465583801,-0.8805660009384155,4.001055717468262,2.019695281982422,-3.8324427604675293,0.22356732189655304,-2.863629102706909,1.3032987117767334,2.291715621948242,-0.7372957468032837,-0.6035779118537903,0.1411333829164505,-0.7073177695274353,0.5288699865341187,0.3338455557823181,-2.4001758098602295,0.4183981418609619,2.463954448699951,-4.575450420379639,-1.6322381496429443,1.606119155883789,-2.156505823135376,2.6302199363708496,-5.655728816986084,1.6520934104919434,3.967597007751465,2.869858503341675,0.5410991907119751,0.96283358335495,3.6737453937530518,1.2003124952316284,-1.5187636613845825,-0.1959417760372162,-1.425016164779663,-0.8002532124519348,0.118509940803051,-0.14931422472000122,0.6226131916046143,-0.3901490271091461,-1.6376773118972778,-2.0112082958221436,2.1446986198425293,-1.2325565814971924,0.6355416178703308,2.7665469646453857,-0.9902048707008362,2.2416882514953613,0.0744960755109787,2.183070182800293,4.856414318084717,-3.2504069805145264,-0.6454969644546509,-2.1039137840270996,-0.5132392048835754,-1.177716851234436,-2.2303690910339355,0.5890149474143982,3.0083024501800537,-1.0412733554840088,-0.7276728749275208,-0.1306009441614151,2.1579067707061768,-0.7484848499298096,4.12955904006958,0.19678834080696106,-1.8554677963256836,-2.7362353801727295,0.155549094080925,0.09597976505756378,1.4078606367111206,-0.621146559715271,-3.5198686122894287,1.8104804754257202,0.2575634717941284,0.5730908513069153,-3.669326066970825,0.8081802725791931,-0.34164032340049744,-1.27984619140625,-2.9332616329193115,1.305494785308838,0.8166778087615967,0.5677902698516846,-1.2942863702774048,3.1393258571624756,-2.2926485538482666,-1.8996329307556152,1.8831162452697754,1.410807728767395,-0.1380738615989685,3.183180093765259,-0.734872043132782,-0.9600180387496948,-0.4844793379306793,-0.1038428544998169,2.3098671436309814,0.5001188516616821,-4.259586811065674,-0.01164485327899456,1.7587270736694336,-2.009204864501953,0.3589305281639099,-0.792252242565155,-6.422672748565674,1.712473750114441,-2.845693588256836,-0.8960685133934021,-0.6786363124847412,-0.8779922723770142,2.7613067626953125,2.1101839542388916,1.5542418956756592,-1.7914438247680664,0.09622030705213547,-1.438734531402588,-2.500824451446533,1.5313544273376465,1.157492995262146,-1.0870071649551392,1.137083888053894,-1.4336938858032227,-1.1875132322311401,-2.5788025856018066,-0.26490330696105957,2.569779634475708,-1.3234714269638062,-3.4917004108428955,-0.32829952239990234,-0.2513826787471771,-1.355506181716919,0.5495710968971252,-1.8892459869384766,2.692176342010498,2.471764326095581,0.7255761623382568,2.236447811126709,0.786490797996521,0.9188079833984375,-1.4831372499465942,1.5996103286743164,1.3768094778060913,-0.4823136329650879,-0.21030963957309723,1.2522228956222534,0.8091447949409485,-0.006362860091030598,-1.1168708801269531,-0.09749945253133774,-1.1711955070495605,-1.0831358432769775,0.11787191033363342,2.9350059032440186,-0.7843350768089294,-3.6963932514190674,-0.13558602333068848,0.44358885288238525,1.040592908859253,-2.569768190383911,1.2976408004760742,-3.632274866104126,-1.8685822486877441,1.4387431144714355,-0.8136664032936096,1.3484492301940918,1.1881564855575562,-1.7502703666687012,1.892024278640747,-1.019708275794983,0.26222339272499084,1.2286932468414307,-2.5824294090270996,-0.26509204506874084,-1.4453439712524414,6.637756824493408,-3.090714693069458,0.2374068945646286,3.3133280277252197,0.1417398452758789,0.7374356389045715,0.010822809301316738,-0.5753375887870789,-0.07205817848443985,-1.9374840259552002,-0.07789357006549835,2.4914424419403076,0.9710296392440796,-0.01990341767668724,-1.6205388307571411,0.5977770090103149,1.349372386932373,-2.521728992462158,-4.347507476806641,-2.554969310760498,0.9076154232025146,0.7550259828567505,-1.8839164972305298,3.3444080352783203,5.050083637237549,1.0732332468032837,-4.771300792694092,6.4261555671691895,-2.381176233291626,-1.4493458271026611,0.09957166016101837,-3.736647367477417,2.8079419136047363,-1.2484928369522095,0.1294020563364029,-0.33818957209587097,-0.9257047772407532,-2.012500524520874,-0.34669893980026245,-1.0811738967895508,1.7855409383773804,0.00985149759799242,-1.1412538290023804,2.264160394668579,0.7181172370910645,0.020859574899077415,-0.44704487919807434,0.12502209842205048,0.07573827356100082,1.9426242113113403,-1.0711936950683594,-0.753379225730896,0.22615505754947662,-0.47344598174095154,-0.0598166361451149,-2.50193190574646,-1.8198262453079224,0.5616495609283447,-1.0836142301559448,2.6003763675689697,-0.1775333136320114,-2.0448670387268066,-3.780880928039551,-0.05183057859539986,-0.8886125087738037,2.9275062084198,3.004265069961548,1.0035638809204102,1.2448810338974,1.363071084022522,-2.2009785175323486,-2.317371129989624,0.6871606111526489,-0.05959778651595116,-0.6526194214820862,-2.8804123401641846,-0.450784832239151,2.045975685119629,0.7061522006988525,1.3131026029586792,-2.331728935241699,1.4584548473358154,1.0349880456924438,-2.5370876789093018,-0.12821060419082642,1.7189940214157104,-1.6051170825958252,1.270205020904541,-1.4745644330978394,-0.8988402485847473,2.616957426071167,-0.24851490557193756,5.030877113342285,-4.557457447052002,2.430690050125122,2.111121654510498,-1.619437575340271,-0.24825316667556763,-3.0273988246917725,0.7439892888069153,0.16659310460090637,1.1194998025894165,2.748769998550415,0.43646517395973206,-3.720662832260132,0.5012239813804626,-2.148696184158325,0.34768378734588623,1.2085206508636475,-3.7667653560638428,-0.3529111444950104,0.17604288458824158,-3.755547285079956,-2.2200870513916016,-2.73280930519104,-2.6864874362945557,0.20741593837738037,1.9487411975860596,0.3143402636051178,-1.671457052230835,1.770768404006958,0.8840028047561646,0.6147453784942627,2.4787776470184326,2.237668037414551,1.218251347541809,-0.18562230467796326,-0.723547637462616,0.13505670428276062,-2.5839672088623047,3.2070186138153076,2.0924079418182373,-0.35129299759864807,-1.3443816900253296,3.144615888595581,-0.6722713708877563,3.534775495529175,4.918873310089111,-1.1887528896331787,-1.263795018196106,4.7574615478515625,2.3240811824798584,1.1837449073791504,-4.4831695556640625,-1.3020495176315308,0.2544240653514862,1.2294131517410278,-1.1710251569747925,-0.01947459951043129,2.7090578079223633,0.4695219099521637,-0.641140341758728,2.0910847187042236,0.03844102472066879,0.10225827246904373,0.7590773105621338,1.9986364841461182,-3.1333017349243164,0.9991056323051453,1.7270113229751587,2.0697295665740967,0.9106760025024414,4.253509521484375,-2.013463258743286,1.9838626384735107,0.8198035955429077,-0.9452731013298035,-0.5759653449058533,0.8339107632637024,-1.9817287921905518,-0.04223639890551567,0.8726548552513123,0.5341706275939941,1.2183769941329956,-1.1668463945388794,-0.9726966619491577,-3.1408023834228516,3.809648036956787,-0.47476881742477417,1.587283730506897,2.278883695602417,0.6091713309288025,0.06680948287248611,1.7817573547363281,-2.4747426509857178,-0.6456933617591858,1.9480737447738647,0.5793362855911255,1.9670528173446655,3.1358609199523926,-3.580613374710083,-3.8909332752227783,2.272921562194824,-2.8282902240753174,-2.2235867977142334,0.46681007742881775,2.6663591861724854,-0.43226319551467896,1.780982255935669,-1.365441918373108,-1.2464300394058228,-2.9182636737823486,-1.7827733755111694,4.1342949867248535,0.5855893492698669,-0.9566391706466675,-1.848954200744629,1.6889235973358154,-4.476962566375732,1.5691730976104736,0.8639421463012695,8.413269996643066,2.6089041233062744,2.085780143737793,1.4400197267532349,2.524519205093384,-1.3565008640289307,0.12058164179325104,1.0062460899353027,3.3648762702941895,-0.9184914231300354,-1.3469542264938354,0.11057649552822113,-5.412775993347168,6.020942211151123,2.5590763092041016,0.02662399783730507,-2.405693769454956,-5.03508186340332,0.3034760057926178,0.0750623568892479,1.29774010181427,-0.937921941280365,2.5862767696380615,-0.7786912322044373,2.9929468631744385,1.3168514966964722,-1.9604264497756958,-0.08879842609167099,-2.4226412773132324,4.618700981140137,1.3671892881393433,0.03386899083852768,0.638088047504425,0.06443282216787338,1.104674220085144,-3.7995684146881104,-4.124148368835449,0.24828584492206573,-2.7511348724365234,-0.797001302242279,-2.430584192276001,0.17623016238212585],"yaxis":"y","type":"scattergl"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmapgl":[{"type":"heatmapgl","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"}}},"xaxis":{"anchor":"y","domain":[0.0,1.0],"title":{"text":"x0"}},"yaxis":{"anchor":"x","domain":[0.0,1.0],"title":{"text":"x1"}},"legend":{"tracegroupgap":0,"itemsizing":"constant"},"margin":{"t":60}},                        {"responsive": true}                    ).then(function(){
                            
var gd = document.getElementById('b01ca6a7-a493-4c88-8a08-2664d6d3a2b7');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                });            </script>        </div>
</div>
</div>
<p>The lyrics model had training accuracies of around 67%, which suggests that the word embeddings may not be learning as well as we expect.</p>
<p>However, we can still see a dinstinction among the three genres:</p>
<ul>
<li>Words at the top corner are typical for hip hop lyrics.</li>
<li>Words at the bottom corner, around 0 at the x0 axis, seem to be more related to rock, and have a negative valence, as typical rock music does (e.g., band, pain, desperate, nightmare).</li>
<li>Words at the left corner appear more in country music (e.g., cowboy, road, Texas).</li>
</ul>
<p>Finally, I will try to visualize any clusterings that the embedding model learned by doing a k-mean clustering analysis.</p>
<div class="cell" data-outputid="25e32cec-8e20-44ce-e98b-043c9baa3f94">
<div class="sourceCode cell-code" id="cb75"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.cluster <span class="im">import</span> KMeans</span>
<span id="cb75-2"><a href="#cb75-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb75-3"><a href="#cb75-3" aria-hidden="true" tabindex="-1"></a><span class="co"># perform k-means clustering on the weights matrix</span></span>
<span id="cb75-4"><a href="#cb75-4" aria-hidden="true" tabindex="-1"></a>kmeans <span class="op">=</span> KMeans(n_clusters<span class="op">=</span><span class="dv">3</span>, random_state<span class="op">=</span><span class="dv">42</span>).fit(weights)</span>
<span id="cb75-5"><a href="#cb75-5" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> kmeans.labels_</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning:

The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
</code></pre>
</div>
</div>
<div class="cell" data-outputid="d500bfc4-1c83-4829-a7b7-7cf376863480" data-execution_count="54">
<div class="sourceCode cell-code" id="cb77"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a><span class="co"># scatter plot with cluster information</span></span>
<span id="cb77-2"><a href="#cb77-2" aria-hidden="true" tabindex="-1"></a>embedding_df <span class="op">=</span> pd.DataFrame({</span>
<span id="cb77-3"><a href="#cb77-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">"word"</span>: tokens, </span>
<span id="cb77-4"><a href="#cb77-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">"x0"</span>: weights[:,<span class="dv">0</span>], </span>
<span id="cb77-5"><a href="#cb77-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">"x1"</span>: weights[:,<span class="dv">1</span>], </span>
<span id="cb77-6"><a href="#cb77-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">"cluster"</span>: labels</span>
<span id="cb77-7"><a href="#cb77-7" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb77-8"><a href="#cb77-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-9"><a href="#cb77-9" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> px.scatter(embedding_df, </span>
<span id="cb77-10"><a href="#cb77-10" aria-hidden="true" tabindex="-1"></a>                 x <span class="op">=</span> <span class="st">"x0"</span>, </span>
<span id="cb77-11"><a href="#cb77-11" aria-hidden="true" tabindex="-1"></a>                 y <span class="op">=</span> <span class="st">"x1"</span>, </span>
<span id="cb77-12"><a href="#cb77-12" aria-hidden="true" tabindex="-1"></a>                 color <span class="op">=</span> <span class="st">"cluster"</span>,</span>
<span id="cb77-13"><a href="#cb77-13" aria-hidden="true" tabindex="-1"></a>                 size <span class="op">=</span> <span class="bu">list</span>(np.ones(<span class="bu">len</span>(embedding_df))),</span>
<span id="cb77-14"><a href="#cb77-14" aria-hidden="true" tabindex="-1"></a>                 size_max <span class="op">=</span> <span class="dv">10</span>,</span>
<span id="cb77-15"><a href="#cb77-15" aria-hidden="true" tabindex="-1"></a>                 hover_name <span class="op">=</span> <span class="st">"word"</span>)</span>
<span id="cb77-16"><a href="#cb77-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-17"><a href="#cb77-17" aria-hidden="true" tabindex="-1"></a>fig.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

<div>                            <div id="cc9ce53b-393c-4057-9837-1cfe9a57a5ac" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                require(["plotly"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("cc9ce53b-393c-4057-9837-1cfe9a57a5ac")) {                    Plotly.newPlot(                        "cc9ce53b-393c-4057-9837-1cfe9a57a5ac",                        [{"hovertemplate":"<b>%{hovertext}</b><br><br>x0=%{x}<br>x1=%{y}<br>size=%{marker.size}<br>cluster=%{marker.color}<extra></extra>","hovertext":["<unk>","know","like","time","come","go","heart","away","life","cause","feel","leave","yeah","live","right","want","night","break","think","long","tell","baby","look","world","need","gonna","good","fall","hold","hear","mind","home","dream","hand","eye","say","little","lose","walk","stand","believe","turn","play","better","take","sing","head","wanna","face","tear","blue","fuck","stay","change","tonight","love","start","girl","inside","things","light","song","place","hard","word","lonely","sweet","cold","give","fight","bring","money","remember","wrong","try","watch","thing","days","stop","true","gotta","black","fool","wait","wish","kill","make","shit","soul","get","miss","people","forget","line","kiss","hell","pain","dead","real","lyric","mean","care","burn","close","commercial","hurt","talk","today","arm","hide","high","bitch","sleep","years","shoot","cry","roll","call","somebody","forever","woman","lie","lord","sound","save","free","touch","help","till","door","grow","work","rain","wall","smile","open","see","goodbye","learn","blood","nigga","game","keep","drink","bout","party","wind","ready","deep","best","listen","everybody","morning","death","road","beat","apart","die","blow","nothin","music","matter","fear","reason","catch","throw","strong","white","run","body","tomorrow","write","alive","grind","sure","maybe","dark","guess","wonder","young","step","devil","memory","follow","friends","dance","speak","begin","understand","couldn","late","hate","promise","spend","pass","damn","friend","fade","pull","smoke","hang","comin","kind","ring","easy","hearts","heaven","niggas","rest","round","tire","crazy","house","ride","thank","truth","dear","street","goin","reach","gettin","scar","straight","drop","memories","afraid","bone","bear","voice","past","slip","darling","drive","soon","sick","moment","livin","stick","cross","nights","kick","blame","room","lookin","star","land","blind","shame","build","pray","steal","radio","feet","tryin","moon","city","wear","piece","stone","power","hole","breath","plan","waste","fast","laugh","trust","floor","scream","control","shake","songs","meet","belong","breathe","fine","water","someday","bleed","share","pick","send","pride","wouldn","talkin","tight","shine","lead","mama","picture","alright","check","summer","happen","lips","knees","yesterday","river","anymore","band","doubt","felt","feelin","chain","trouble","doin","million","dirty","girls","sign","brain","pretty","lonesome","mouth","end","streets","ways","second","choose","count","gold","raise","rule","great","country","peace","clear","push","rise","ball","loud","walkin","train","search","read","slow","lock","child","different","earth","swear","boys","thousand","runnin","sight","flow","wild","number","darkness","near","teach","evil","hair","warm","lady","forgive","story","outside","fell","sell","window","answer","finally","lovin","deal","mistake","flame","move","rhyme","worry","year","knock","wasn","daddy","shoe","finger","mother","sorrow","hop","track","jump","kid","ghost","pay","ask","ones","skin","wide","dust","sorry","future","shin","bury","half","longer","guitar","safe","lay","shut","thinkin","lover","point","letter","somethin","tree","beautiful","block","tongue","minute","bless","bind","whoa","mirror","phone","single","whisper","crack","fill","sayin","okay","school","crowd","brother","bust","crawl","taste","style","ahead","slowly","wake","queen","bottle","fuckin","shoulder","grave","simple","book","heat","race","perfect","show","waitin","remain","realize","wife","welcome","poor","children","cover","paper","darlin","twist","babe","cool","loose","nice","paint","small","repeat","state","cloud","makin","question","everyday","shout","record","thoughts","sense","pack","survive","quit","dress","pretend","stranger","bright","edge","father","mess","silence","outta","hours","playin","regret","wing","hour","glass","middle","brand","desire","corner","escape","human","rock","texas","club","return","sittin","family","green","strange","weak","woah","fly","luck","wine","insane","soldier","surprise","tune","drinkin","fate","treat","haunt","rockin","sink","fake","grab","ladies","clean","clock","warn","bite","tie","draw","south","heal","sit","space","prove","smell","tryna","bend","business","cowboy","motherfucker","freeze","spot","freedom","funny","steel","swing","brothers","color","closer","shadow","awake","dope","drug","couple","ocean","bang","drown","lovers","mountain","pour","sand","spin","wheel","cash","dare","neck","singin","america","disappear","have","honky","mornin","strike","anybody","climb","happiness","battle","master","saturday","serve","soft","spit","fail","gun","instead","magic","wave","clothe","gimme","heartache","roses","truck","bell","travel","crime","field","holy","let","fallin","fee","figure","imagine","kinda","movin","sky","stronger","suck","midnight","snow","spirit","tough","dirt","drift","misery","beer","lifetime","news","week","lean","precious","quick","buy","cryin","heavy","surround","even","echo","pocket","satisfy","short","tonk","crash","fame","momma","steady","thrill","whiskey","beg","booty","bridge","golden","tender","hello","highway","sail","clap","folks","quiet","ridin","shall","fact","guide","load","metal","sweat","card","decide","harder","hood","lift","prayer","ship","carry","hangin","hoe","takin","wander","choice","drag","string","whip","christmas","oooh","proud","tall","winter","fair","instrumental","judge","machine","mighty","table","california","flesh","hit","silver","name","niggaz","slide","women","wreck","remind","ease","freak","seat","suppose","bank","cheat","crown","dancin","hungry","ohoh","brave","cost","early","special","spell","boat","wrap","funky","diamonds","find","mend","respect","demons","force","hook","seek","spread","strength","angels","paradise","slave","suffer","wonderful","beneath","curse","higher","holdin","twice","wire","chill","older","trap","ache","callin","everytime","givin","hollow","leavin","murder","pussy","american","claim","favorite","knife","recall","secret","amaze","difference","double","later","stare","boss","lesson","loneliness","problem","dollar","hallelujah","jealous","weed","flower","lovely","pound","workin","fresh","hurry","note","stage","suddenly","bullet","erase","hill","weather","weight","bird","cast","foot","path","probably","final","gather","noise","rid","scene","store","tennessee","anger","doctor","rag","sin","southern","stuff","underneath","weep","church","fever","friday","garden","horse","release","shatter","surrender","animal","bitter","distance","flash","praise","seven","swallow","watchin","prison","spring","surely","thunder","trick","danger","heartaches","refuse","rope","shape","shouldn","stain","sweetheart","company","doors","enemy","hiphop","jail","mister","teardrops","bar","choke","coffee","hat","mountains","poison","pump","silent","vain","beast","chair","dime","fault","flip","bigger","certain","drum","fiddle","heartbreak","history","list","message","season","tellin","york","aren","cigarette","daylight","dont","goodnight","laughter","pillow","shelter","clown","offer","undo","bass","beauty","bore","cars","explain","nail","plain","shed","speed","switch","test","trade","trigger","admit","crew","savior","souls","stack","teeth","wound","appear","bomb","calm","chest","circle","scratch","ash","bother","chance","press","roar","trail","valley","weary","buzz","cell","chase","coast","fist","sister","stumble","view","boot","gift","hardly","sunday","tick","waltz","wed","asleep","class","foolish","join","put","rap","tide","float","grip","lyin","marry","nation","passion","stupid","will","cure","faster","madness","rough","wash","date","lick","rush","struggle","ticket","worst","boom","destroy","endless","groove","havin","problems","rebel","rome","strip","yellow","attack","daughter","east","excuse","fantasy","grand","numb","roam","toss","victim","worse","barely","desert","ears","hustle","iron","joke","nose","romance","softly","swim","wipe","bling","bloody","charm","crush","leather","santa","screen","standin","telephone","tremble","act","advice","bill","busy","dog","hammer","last","moonlight","secrets","smooth","breeze","burnin","cop","cut","glow","piss","quarter","smokin","suicide","whistle","action","cage","deserve","nature","reality","rhythm","self","settle","snake","spark","deeper","ghetto","island","lately","melt","moan","moments","notice","shotgun","solo","suit","unite","weekend","disguise","expect","gods","naked","pity","seed","vein","windows","alabama","breakin","comfort","motion","part","plenty","rip","set","uncle","downtown","flat","grey","guy","hotel","neon","proof","worlds","aside","cheap","handle","hollywood","order","punk","roof","stories","vision","zone","darkest","keepin","monster","orleans","ought","redneck","restless","sippin","condition","creep","flag","gently","guilty","inch","large","lightning","march","mood","roads","sugar","trippin","worship","blowin","gang","heartbeat","neighbor","north","plane","rainbow","smart","wise","destiny","fire","form","gentle","hearted","jungle","mystery","pills","revolution","treasure","wicked","win","youth","attention","awhile","county","deceive","dumb","embrace","exactly","fold","monday","park","salvation","sinner","angry","arrive","beach","deny","grass","guard","page","plus","ruin","shore","super","tangle","age","blast","borrow","cling","cock","direction","gain","homies","jeans","liar","liquor","match","needle","resist","reveal","silly","slap","strangers","team","thats","finish","heel","homie","memphis","mile","squeeze","tail","thee","trash","bull","create","dice","distant","harm","papa","porch","sake","seal","strap","stretch","sunset","type","damage","hardest","kitchen","main","mexico","nerve","pure","relax","sadness","score","screw","tempt","toe","unknown","crank","innocent","kingdom","movie","shots","simply","smash","solid","whore","witch","bullets","cadillac","cheer","command","feed","fish","houston","image","knockin","pleasure","pop","root","saddle","satan","shade","stereo","torture","weren","chicken","confuse","cotton","dough","explode","gate","holla","ignore","pool","protect","riot","shirt","shop","station","agree","apple","brick","brush","cocaine","cook","electric","forward","horn","kings","maker","separate","seventh","situation","spill","sweep","throat","turnin","weakness","ashamed","boogie","bunch","capture","crumble","demon","divine","farm","gamble","grin","guarantee","holiday","pardon","photograph","pimp","pistol","replace","sacrifice","saint","shinin","side","square","tiny","ugly","bein","cruel","goodbyes","heavenly","linger","mississippi","ohhhh","pin","rage","sheet","slam","suitcase","victory","actin","bible","blaze","bread","careful","complain","cowboys","dangerous","habit","haters","heroes","hopin","layin","oohooh","police","rent","tumble","winner","dreamin","flood","frame","hunger","hurricane","key","minutes","monkey","plant","prepare","shift","stress","teacher","tequila","tower","witness","attitude","bruise","candle","cheek","confess","dame","eternity","farther","fence","forgiveness","funk","innocence","invisible","jailhouse","joint","legs","motherfuckers","preacher","rappers","shelf","sunlight","sweeter","truly","wonderin","yell","yiggy","blink","bloom","brighter","chevrolet","desperate","dollars","knowin","motherfucking","natural","nightmare","prayers","pressure","reflection","sleepless","thirty","underground","upside","woods","back","birthday","blade","blunt","bullshit","complete","hero","hurtin","junior","lack","mass","range","sacred","snap","tread"," "],"legendgroup":"","marker":{"color":[0,0,0,0,0,2,1,2,0,0,2,2,2,2,0,2,2,0,0,2,0,0,0,2,2,1,2,0,0,0,0,0,1,0,2,1,1,1,0,0,0,0,0,2,0,2,2,0,2,1,1,2,0,2,2,1,0,0,2,0,0,1,0,0,0,0,1,2,0,0,2,0,2,1,0,0,0,0,2,0,0,2,0,2,2,2,1,0,0,0,0,0,0,0,2,2,2,2,0,2,2,2,2,0,2,1,0,1,1,2,0,0,2,0,2,1,0,2,0,2,1,2,1,2,2,0,2,1,1,0,0,0,0,2,1,2,1,0,1,2,0,0,1,1,0,0,0,0,0,0,0,2,2,2,1,0,2,1,0,0,0,0,2,0,2,0,0,2,1,2,0,0,0,2,1,2,2,1,1,0,0,0,1,2,1,2,2,2,1,0,0,2,2,0,1,0,1,2,2,0,1,0,0,0,1,0,1,0,0,0,2,1,0,0,1,1,0,0,1,0,0,2,2,0,1,2,2,2,2,0,0,1,1,0,2,0,1,0,1,2,2,0,1,0,0,2,0,2,0,2,2,1,0,0,1,0,1,2,1,2,2,0,0,2,0,1,2,0,2,2,0,1,0,2,0,2,1,0,2,1,1,0,1,1,0,0,0,1,0,0,0,0,2,0,1,2,2,1,2,2,1,1,0,0,1,0,0,0,0,0,2,1,1,2,0,2,0,0,2,0,0,0,2,2,1,0,0,2,2,2,1,1,1,1,0,0,2,2,2,2,1,0,0,0,2,0,0,0,1,1,0,2,1,1,0,0,2,2,1,0,0,2,0,1,0,0,0,0,0,0,2,0,0,0,0,2,2,0,1,0,2,1,0,0,0,0,2,2,0,0,2,2,2,1,0,1,2,1,2,0,0,0,0,0,0,2,0,2,0,0,1,2,2,0,2,1,2,1,0,2,0,0,0,0,2,2,0,0,1,0,0,0,2,1,2,0,0,2,2,0,0,0,2,2,1,2,0,2,2,1,1,2,2,0,2,2,2,1,1,2,0,0,0,2,2,0,2,2,1,2,1,1,1,2,1,2,2,2,2,0,0,0,1,0,2,1,1,0,2,0,2,2,2,1,0,2,1,0,2,2,2,2,1,1,1,2,0,2,0,1,2,2,2,0,1,0,2,0,2,1,0,2,2,2,0,1,1,2,1,0,0,1,0,1,0,2,0,0,0,1,2,0,2,2,1,2,0,2,0,2,0,2,1,1,0,1,2,0,0,0,2,1,2,2,2,1,1,1,2,1,1,2,2,0,2,1,2,2,0,0,1,2,0,2,1,1,1,2,1,2,0,2,0,1,2,2,0,1,1,1,0,2,1,1,2,0,2,0,1,1,2,0,0,0,1,2,1,0,0,0,1,0,1,1,0,1,2,2,0,2,1,1,1,0,1,0,1,0,0,1,2,1,2,0,1,0,2,1,2,0,1,2,1,0,2,1,0,1,1,0,0,1,2,2,1,2,2,2,1,2,0,0,1,0,2,2,0,2,0,2,1,1,0,0,0,2,2,0,2,1,0,1,0,1,1,1,2,2,0,2,1,0,1,1,2,2,1,1,0,0,2,0,2,2,2,1,2,2,2,1,2,2,0,0,1,0,0,2,2,2,0,0,1,2,1,0,2,2,0,2,2,1,2,1,0,0,2,0,0,0,1,0,0,0,0,0,1,2,0,2,0,1,0,0,2,1,1,1,2,2,1,2,1,2,0,2,0,0,0,2,0,1,0,2,0,2,1,0,2,2,1,2,1,2,1,0,2,2,2,1,1,0,2,2,2,0,0,1,2,0,2,2,1,2,2,2,0,2,1,2,1,2,0,0,0,1,0,2,1,1,2,2,2,1,2,2,1,1,2,0,0,1,0,1,1,2,2,2,2,1,0,1,1,2,0,0,2,1,1,1,0,2,0,2,2,0,2,0,1,1,0,0,2,0,0,0,0,2,2,0,2,0,0,2,2,2,0,2,0,1,0,2,2,1,1,1,2,2,0,0,2,0,0,2,0,0,2,1,2,0,1,2,0,1,2,0,2,2,2,2,1,1,2,2,2,1,2,2,0,0,1,1,2,0,2,0,0,0,2,2,0,1,2,1,2,2,1,2,2,0,0,2,0,2,2,0,2,0,1,0,2,1,1,2,2,1,1,0,1,0,2,2,0,2,1,0,0,2,1,0,2,0,0,2,0,0,0,2,0,1,1,0,2,0,2,0,0,2,1,2,1,2,2,0,0,2,1,1,0,2,0,1,2,2,0,0,0,1,1,0,2,0,2,1,2,2,2,0,0,1,1,0,2,2,1,0,2,0,1,1,1,0,0,1,1,0,0,2,0,1,2,0,2,0,1,2,0,0,0,2,0,0,1,0,0,1,2,2,1,1,2,0,2,2,1,1,1,0,2,0,0,2,0,1,0,1,0,1,2,0,0,1,0,2,0,1,2,0,0,2,1,1,1,1,2,2,2,1,1,1,0,0,1,0,2,1,0,1,2,1,0,2,1,0,0,1,2,0,1,2,0,2,0,0,2,1,0,1,1,2,2,0,2,0,0,1,2,0,1,1,2,1,1,1,1,2,0,2,2,2,1,1,2,0,0,2,2,0,2,0,2,1,1,2,0,2,2,2,1,0,2,1,2,2,1,0,2,2,0,2,2,1,0,0,0,2,2,1,2,0,2,1,1,1,2,0,2,0,1,1,2,0,0,2,0,2,2,1,2,2,2,1,0,0,2,2,1,0,1,2,2,2,2,2,2,2,2,0,2,2,1,1,1,0,1,0,2,2,0,1,1,1,2,2,0,1,0,0,2,2,0,1,0,2,1,1,1,2,1,0,1,1,0,0,2,0,0,1,0,1,1,0,0,1,0,1,0,1,1,2,1,1,2,2,1,2,1,1,0,0,0,2,0,0,2,2,2,0,0,0,1,1,0,2,2,1,0,0,0,1,1,1,1,2,2,2,1,2,2,0,1,0,0,0,0,2,1,2,0,0,1,2,1,1,0,1,0,2,2,2,2,0,0,0,2,0,1,1,0,2,0,0,2,0,1,0,2,2,1,1,2,2,0],"coloraxis":"coloraxis","size":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0],"sizemode":"area","sizeref":0.01,"symbol":"circle"},"mode":"markers","name":"","showlegend":false,"x":[1.6995059251785278,1.9406745433807373,-0.12173966318368912,0.035872578620910645,-0.11843281239271164,0.07545451074838638,-3.4063520431518555,1.9739305973052979,1.4660042524337769,1.3782153129577637,1.6171702146530151,1.1612000465393066,1.500238060951233,2.0111823081970215,0.06929023563861847,0.867279052734375,0.41998007893562317,0.5313323736190796,-0.9315704703330994,0.3335350751876831,1.3772425651550293,-0.5170290470123291,0.35182279348373413,1.3020060062408447,2.105351448059082,-2.7871716022491455,-0.013027130626142025,0.4771774411201477,0.11784818768501282,-0.4310609698295593,-0.22309766709804535,-0.7340279817581177,-2.3974156379699707,-0.06574535369873047,2.668562889099121,-1.3242902755737305,-2.0156490802764893,-1.5032930374145508,-0.20221281051635742,-0.02734849974513054,0.8039650321006775,1.3887234926223755,-0.3134401738643646,1.2921552658081055,-0.44684019684791565,0.16643348336219788,3.3321800231933594,1.6518287658691406,2.850158929824829,-1.5229374170303345,-3.9679503440856934,3.5636234283447266,0.5423137545585632,0.38365787267684937,1.5315862894058228,-1.6855887174606323,-0.6202724575996399,-0.33202579617500305,4.188085556030273,-0.2730904519557953,-1.5046749114990234,-2.586074113845825,-0.8927977681159973,0.8368072509765625,0.5009186863899231,-0.1719793677330017,-0.8601940870285034,0.5071600079536438,-0.1695549488067627,1.3929270505905151,2.0591232776641846,1.8454748392105103,2.768495559692383,-1.6364859342575073,0.8571687340736389,0.7787779569625854,-1.1309833526611328,0.7632659673690796,2.712526559829712,0.4033794105052948,2.195047378540039,0.5357837677001953,0.08143798261880875,2.561142683029175,0.19735082983970642,0.2326047569513321,-0.734858512878418,2.654890537261963,-0.6319705247879028,0.548969566822052,-0.035350922495126724,0.08518940955400467,0.20451772212982178,1.0818310976028442,-0.2679976224899292,1.1106557846069336,2.2418856620788574,4.799577713012695,-0.3519485890865326,36.77808380126953,4.804645538330078,0.3015158474445343,0.8927000164985657,-1.470966100692749,39.140193939208984,-4.588335990905762,0.4682883620262146,-1.5855119228363037,-1.8654323816299438,0.43552881479263306,1.4090384244918823,-0.27007874846458435,1.3177063465118408,-0.7363564968109131,3.1522154808044434,-1.3206743001937866,0.288497656583786,1.0422760248184204,0.4196038842201233,0.3066492974758148,-1.9118047952651978,1.5621364116668701,-8.368370056152344,1.2372924089431763,1.900536060333252,-0.5236776471138,2.255605936050415,-1.2941067218780518,-2.5880441665649414,-0.5063183307647705,-2.074275255203247,0.8938148617744446,-1.6822142601013184,1.7352025508880615,-2.5872230529785156,2.358368396759033,-0.6202980875968933,-0.22087416052818298,-3.226506471633911,0.1360837072134018,2.8274195194244385,0.4061172902584076,-3.117807388305664,-2.5383450984954834,-2.695401906967163,2.660336971282959,-0.7844515442848206,-0.6987929344177246,-0.46184271574020386,-0.8587643504142761,1.585351586341858,1.2801376581192017,2.276262044906616,2.492042303085327,-6.420569896697998,1.1503889560699463,4.595770359039307,-4.152090549468994,1.9207215309143066,-1.7367854118347168,-0.03275741636753082,-0.857168972492218,5.028644561767578,-0.3773770332336426,0.237786203622818,2.383795738220215,-0.6724620461463928,1.0930479764938354,-1.72630774974823,3.0752899646759033,-0.12807197868824005,1.241741418838501,2.4300544261932373,1.000551700592041,-1.7272531986236572,1.0163930654525757,-0.033757489174604416,-6.573124885559082,-4.010196685791016,3.157745838165283,0.8450120091438293,0.6749720573425293,-6.56807804107666,2.952453374862671,-0.813368022441864,2.1997814178466797,1.5479532480239868,3.553422212600708,-1.0948399305343628,-1.3868298530578613,-0.4533539116382599,2.174217939376831,2.561765432357788,0.04436023533344269,-2.842045307159424,0.3365553319454193,-2.976529836654663,3.5421974658966064,1.0195327997207642,-1.6126904487609863,-1.791494607925415,-0.41334110498428345,0.184705451130867,-1.6057442426681519,-3.3158040046691895,-0.6115790009498596,-2.8427822589874268,0.6820458769798279,1.8858835697174072,-0.4515085518360138,0.06675953418016434,-3.9260473251342773,-1.8438119888305664,-0.7591304183006287,-4.143612861633301,-0.903163492679596,-1.8379336595535278,0.8541869521141052,-1.824211597442627,-0.34857553243637085,-3.655796527862549,1.7475954294204712,4.278853416442871,0.5312531590461731,-7.816947937011719,2.006187677383423,0.7035778760910034,1.3318010568618774,2.464722156524658,-0.6920268535614014,0.44401106238365173,-3.8916232585906982,-1.1869250535964966,1.0338066816329956,4.026699542999268,-0.07306934148073196,-3.1707100868225098,-0.09635458141565323,-6.393095016479492,1.534996747970581,2.6862456798553467,-0.5859373807907104,-3.689532995223999,-2.0066471099853516,0.4278252422809601,0.3823868930339813,1.4702634811401367,2.5225212574005127,-0.17747168242931366,1.0184112787246704,-0.3024462163448334,-2.1158103942871094,-0.3586027920246124,-3.147642135620117,-2.6326823234558105,0.3244806230068207,-2.1961464881896973,0.31568825244903564,-2.1226272583007812,1.7119814157485962,-0.5777193903923035,-0.08914031088352203,-0.9385486841201782,4.704444408416748,-0.12993189692497253,-2.9284465312957764,3.5992088317871094,0.0049493457190692425,5.035700798034668,3.0046615600585938,0.8816647529602051,-2.4777402877807617,0.5467808842658997,0.8769200444221497,1.4100602865219116,-0.33130431175231934,-2.9633562564849854,-0.093485027551651,5.825380325317383,-2.7511653900146484,-4.791096210479736,2.632962942123413,-3.2865116596221924,-2.3611042499542236,1.848691463470459,-2.208487033843994,-0.9056057929992676,-3.5119574069976807,-1.0627245903015137,-0.07441435009241104,2.769138813018799,0.7033182978630066,0.6965830326080322,1.2633693218231201,-4.795852184295654,1.7854418754577637,1.6976436376571655,-2.7465548515319824,-0.496946781873703,0.0757678896188736,-5.588857173919678,-1.2741221189498901,-2.0075736045837402,0.37662801146507263,-5.162201404571533,-0.3098166882991791,-0.739450216293335,-0.8355267643928528,0.31070613861083984,-0.015425012446939945,3.738757371902466,-5.325382709503174,-8.065062522888184,3.089003801345825,-2.1365530490875244,6.004992485046387,1.32464599609375,-0.44686219096183777,3.1863670349121094,-0.8909366726875305,1.046112060546875,-0.24353839457035065,2.6705703735351562,-0.4922071099281311,-8.475784301757812,-0.6616876721382141,1.9412999153137207,6.068378448486328,4.348984718322754,2.7614059448242188,-1.392552137374878,-5.19753360748291,-4.619576454162598,-2.5293123722076416,-1.0252017974853516,-5.349100589752197,0.3475153148174286,3.1650164127349854,3.2870726585388184,3.1913299560546875,-4.118500709533691,0.35151299834251404,1.3972790241241455,-0.8728285431861877,2.7636077404022217,-0.060398880392313004,0.5721859335899353,-1.6537728309631348,-0.8874960541725159,-1.7025657892227173,-1.5226496458053589,2.1470916271209717,-2.411776542663574,-4.993805885314941,-0.7767878174781799,0.29230913519859314,2.26544189453125,2.9617652893066406,-2.0846974849700928,1.5077704191207886,-3.152696132659912,1.5698740482330322,0.6361508369445801,-5.641232013702393,0.23349761962890625,2.205982208251953,-1.692823886871338,-1.3847856521606445,2.491567611694336,-0.919653594493866,2.8502280712127686,1.133082389831543,-0.4168859124183655,-1.7240301370620728,0.7467068433761597,2.754992961883545,3.0271244049072266,-0.7058278918266296,-3.8943593502044678,-2.031270980834961,1.2901049852371216,-2.849646806716919,0.060271427035331726,-1.5420818328857422,1.6261307001113892,-0.1570456176996231,2.9912707805633545,-0.41289272904396057,1.4848302602767944,0.8693858981132507,2.7500743865966797,0.9150644540786743,2.2640888690948486,-3.175119400024414,-2.155123233795166,-8.39661979675293,1.7680811882019043,-4.425253868103027,2.167419195175171,-2.8900747299194336,0.6912316083908081,0.5071381330490112,2.727445125579834,0.8921741247177124,-3.1575798988342285,1.3469316959381104,-1.5422015190124512,1.981420636177063,-0.20090481638908386,-0.426531046628952,-5.768795490264893,3.6660077571868896,-0.06498932838439941,-1.3363456726074219,1.7949848175048828,-6.4595947265625,4.867516040802002,-3.3391666412353516,-4.1696600914001465,1.8222638368606567,0.5418640971183777,0.6144477128982544,-2.1811375617980957,-0.11647741496562958,2.7292373180389404,2.5281150341033936,1.7001811265945435,0.5887854695320129,-2.77983021736145,0.9191842079162598,2.171107769012451,-3.040644407272339,3.8984975814819336,-4.210373878479004,2.6815762519836426,-0.848577618598938,1.6257710456848145,4.114508628845215,3.009347677230835,-0.9387663006782532,-1.3824036121368408,-2.847609043121338,3.859579086303711,2.945044994354248,-9.164511680603027,1.300627589225769,-0.5293194651603699,4.974598407745361,2.4239726066589355,-2.4765069484710693,-9.211256980895996,3.4450321197509766,2.9498469829559326,1.1525923013687134,2.080005645751953,6.007797718048096,-0.21514570713043213,-2.597137689590454,-4.4456787109375,5.486353397369385,-0.7912349104881287,1.24520742893219,-1.6844189167022705,4.113361358642578,4.345670223236084,1.0931649208068848,5.110175609588623,6.480611801147461,-3.516763687133789,3.202293634414673,-3.068220376968384,-2.848881244659424,-4.967443466186523,1.124337077140808,-3.2939391136169434,4.357579708099365,5.0164690017700195,5.201258182525635,5.205567359924316,0.8027799129486084,-1.108290672302246,-1.6953990459442139,-2.994168519973755,0.10504266619682312,3.247816801071167,-2.9668242931365967,-6.230284214019775,-1.3829467296600342,3.298330783843994,0.32426968216896057,7.88744592666626,2.780705451965332,2.6726653575897217,-9.750960350036621,1.3441342115402222,2.9784836769104004,-6.454412460327148,-1.011377215385437,0.5141196250915527,0.06423743814229965,4.911319255828857,3.086454153060913,-2.4965732097625732,-1.906519889831543,-1.3679028749465942,1.392791509628296,-1.8550912141799927,1.404288649559021,0.12790635228157043,-4.644371509552002,0.9954899549484253,4.274538040161133,4.0543718338012695,-1.0285086631774902,-1.3404136896133423,1.1633714437484741,5.182363033294678,-0.3477315902709961,3.678056240081787,-4.0758843421936035,0.7991302609443665,5.287332057952881,2.9002373218536377,1.9355766773223877,-3.570335626602173,-1.2774442434310913,-1.4765783548355103,3.387446403503418,-2.9462015628814697,0.8870668411254883,-3.179392099380493,-3.85860538482666,1.1339268684387207,-12.473369598388672,-0.8218761682510376,7.3448638916015625,2.8328967094421387,-1.8524724245071411,2.8694510459899902,-4.024280548095703,1.9409339427947998,-0.02143755927681923,2.851604461669922,1.5318686962127686,-1.0873517990112305,4.707732200622559,3.7012155055999756,3.4496009349823,-0.10284242779016495,0.5193830728530884,-0.4880211353302002,1.3659842014312744,-3.6136066913604736,-5.393479824066162,-3.736149311065674,-2.894697666168213,2.8198421001434326,-0.4436703622341156,-0.41837215423583984,0.08797668665647507,2.706101179122925,-0.9028229117393494,2.0974924564361572,0.8568336963653564,2.705299139022827,-8.009699821472168,-2.825281858444214,-3.764047145843506,-0.4615664482116699,-3.0258495807647705,-8.41796875,0.25518494844436646,3.821559429168701,-1.198124885559082,2.0630931854248047,-2.0712759494781494,3.5881848335266113,2.5413763523101807,-1.2494313716888428,-1.5611714124679565,-2.0972466468811035,4.559410572052002,-0.3372495174407959,1.8374238014221191,-4.735352993011475,-1.7785130739212036,-8.615769386291504,1.3252756595611572,-3.4786314964294434,4.373807430267334,0.8802254796028137,1.804547667503357,-1.452553629875183,-3.9773547649383545,4.924982070922852,2.2651846408843994,-0.01835945062339306,-4.410627365112305,-1.768602728843689,-2.7210655212402344,-2.5443923473358154,6.69824743270874,-2.645437002182007,-4.107673168182373,3.339603900909424,0.12448587268590927,0.45876482129096985,-1.8044887781143188,-1.6829558610916138,-15.086213111877441,4.266053199768066,-1.8836705684661865,0.7337573766708374,-2.707731008529663,-2.9006543159484863,3.979456663131714,-3.6118903160095215,-2.64762806892395,2.190504789352417,0.4376874268054962,-2.079092502593994,0.2413463145494461,-6.205280303955078,-4.899164199829102,-1.7247586250305176,-8.805187225341797,2.788132905960083,1.8578596115112305,-0.8086163997650146,2.983276844024658,-1.5881822109222412,-8.67251205444336,-1.8649412393569946,-0.5514096021652222,-1.7540079355239868,-0.5115605592727661,-1.598174810409546,1.0744030475616455,0.498583048582077,-1.0457864999771118,3.541865348815918,-6.253194808959961,3.4289937019348145,0.02772931568324566,-2.9087345600128174,0.11234971880912781,0.747025191783905,-1.276953935623169,3.775360345840454,3.4522664546966553,-4.05693244934082,7.0216569900512695,-1.7567641735076904,-0.8596468567848206,2.7523844242095947,-3.970747947692871,0.4480971693992615,-2.3760504722595215,-4.230010032653809,2.802311897277832,-3.5110585689544678,-2.0010907649993896,3.481215476989746,1.4568262100219727,-1.365818738937378,4.099803447723389,0.0989290326833725,3.5297272205352783,-3.568366527557373,0.7245394587516785,0.20768237113952637,-0.9782758355140686,-16.671131134033203,1.2022359371185303,2.5189449787139893,1.30103600025177,-0.26166248321533203,-0.5171424746513367,2.256800413131714,1.9883668422698975,-4.933732032775879,-2.818802833557129,5.082000255584717,1.2759621143341064,1.4432698488235474,2.5766501426696777,2.2489547729492188,-2.0406038761138916,3.603762626647949,-3.0953032970428467,2.038203239440918,-3.97316837310791,-1.0772182941436768,-2.4835474491119385,-2.417823314666748,-0.8263788819313049,4.63776159286499,0.5100556015968323,2.6560311317443848,1.7053385972976685,-3.034966230392456,0.18243764340877533,-1.3456045389175415,-0.7201623320579529,3.8071951866149902,2.296431064605713,-4.47169828414917,-2.7256901264190674,4.261474132537842,-1.699303150177002,7.453990459442139,2.4580748081207275,5.408223628997803,5.166528224945068,0.7186464071273804,-6.483591556549072,3.8140461444854736,3.2689270973205566,2.339170217514038,-3.9687998294830322,5.885075092315674,0.18131199479103088,-1.898805856704712,-2.265512704849243,-3.768742799758911,-0.28547635674476624,-4.595541000366211,2.948078155517578,4.089010715484619,2.6296002864837646,-2.8104248046875,0.037699297070503235,-2.7409274578094482,3.2102556228637695,-8.508746147155762,3.0455760955810547,6.572522163391113,-0.5119708180427551,0.4660626947879791,1.828127384185791,1.1432305574417114,-6.747279644012451,8.194195747375488,-1.4216333627700806,-1.9284874200820923,-0.9277052879333496,1.849745512008667,3.177178144454956,0.658642053604126,-0.8656023144721985,-2.5535714626312256,-0.21191468834877014,-0.12776903808116913,0.1955832540988922,-0.8576517701148987,-2.8963570594787598,-3.063178777694702,3.808685302734375,-2.4563281536102295,1.3075913190841675,-1.295256495475769,-3.3245508670806885,-1.6936590671539307,2.5406527519226074,2.920380115509033,-1.0161659717559814,-3.0508525371551514,-3.886314868927002,1.9381732940673828,6.5693678855896,-2.2481367588043213,0.016288997605443,-3.4732272624969482,0.9846822023391724,-0.024672577157616615,2.5236220359802246,-1.0887387990951538,-0.41568389534950256,0.4529587924480438,2.551992416381836,-0.8119215965270996,-6.342216968536377,1.224023461341858,2.4053704738616943,0.590916097164154,1.2074627876281738,-2.897150754928589,-2.3919787406921387,5.360910415649414,3.374244451522827,-2.0640289783477783,1.4166961908340454,-4.671737194061279,-0.22397631406784058,-4.695302486419678,-0.7816755175590515,-0.21366670727729797,4.041713714599609,8.57574462890625,-2.4645631313323975,-3.725598096847534,-0.9287117123603821,2.2512481212615967,0.4616882801055908,3.1673743724823,1.1682777404785156,-0.28271177411079407,-3.2676610946655273,3.247633695602417,-1.0983366966247559,0.8699164986610413,2.448610544204712,-9.246732711791992,1.8513290882110596,2.721064567565918,2.765667200088501,1.6644260883331299,-0.36083847284317017,-6.876705646514893,1.7002041339874268,-2.273397922515869,4.46538782119751,3.3581573963165283,-2.7166290283203125,0.004415108822286129,-10.772294998168945,-2.0978522300720215,2.9655263423919678,-4.12235689163208,-3.886996030807495,0.09002570062875748,3.0690407752990723,2.6588757038116455,-1.4110078811645508,1.2913053035736084,5.43192195892334,-3.021822929382324,-5.407227516174316,-0.01316614169627428,0.38449475169181824,2.4502170085906982,-6.957235813140869,-1.3392151594161987,-5.5236735343933105,-3.808528423309326,3.99908447265625,3.2185404300689697,1.9091720581054688,1.8122421503067017,-6.42324161529541,2.888608455657959,-2.339097023010254,-2.2247374057769775,0.022939080372452736,-1.2008719444274902,1.462268590927124,6.9454545974731445,-5.74730920791626,-1.5226095914840698,-3.1787312030792236,-0.04451589286327362,0.6287513375282288,-0.17754073441028595,0.4245130717754364,1.1525691747665405,-0.6851840615272522,2.8243794441223145,-0.9190956950187683,-6.617860317230225,-3.142723560333252,0.08116678148508072,2.3167724609375,3.8631436824798584,-1.4287580251693726,-1.9870710372924805,-1.2096036672592163,-1.5902907848358154,2.2906246185302734,2.24743390083313,-4.715816020965576,5.899677753448486,3.0892322063446045,-1.4770218133926392,4.861459255218506,4.224470615386963,5.8648576736450195,1.6607509851455688,2.7869887351989746,1.3897881507873535,-3.2133724689483643,0.4400586485862732,4.512856960296631,0.12181000411510468,-3.47007155418396,-2.638246536254883,-9.881497383117676,1.7925547361373901,4.5901312828063965,-0.5193463563919067,2.9502780437469482,5.115223407745361,-0.1169518232345581,0.0667777955532074,3.017918348312378,-2.290902614593506,-1.8571250438690186,0.1500205397605896,-3.114410400390625,1.1626670360565186,-0.2892932593822479,-8.402587890625,1.2040668725967407,-2.4970710277557373,-7.7495198249816895,1.5668443441390991,-0.06433313339948654,3.0947394371032715,4.296990394592285,4.367506980895996,4.695237159729004,-2.1200766563415527,-7.853061199188232,1.7816219329833984,2.117906332015991,4.031605243682861,-3.8608930110931396,3.1727943420410156,1.8049410581588745,2.713435649871826,-1.0043448209762573,-6.987863540649414,-4.1461405754089355,1.2395458221435547,2.601715564727783,0.9478702545166016,0.22486759722232819,-0.10816200822591782,1.501002311706543,4.684719085693359,0.44607678055763245,-0.5196811556816101,-2.5189332962036133,5.314509868621826,-2.3397624492645264,2.194441556930542,0.3276628255844116,-2.0174496173858643,2.932821750640869,0.7612712979316711,1.115049123764038,-1.0659273862838745,4.554991245269775,-1.78782320022583,5.460937023162842,2.402953863143921,-1.6482514142990112,3.680861711502075,-0.29047760367393494,-3.464385986328125,-0.7445377111434937,4.429631233215332,-3.088209629058838,-0.7778899073600769,1.7531925439834595,7.373058319091797,-0.8604730367660522,-3.471385955810547,-1.3156323432922363,-4.3807291984558105,2.585681676864624,0.7861326932907104,3.1088287830352783,1.1680538654327393,5.656093120574951,-2.086625814437866,0.7185921669006348,-2.649348735809326,2.334327220916748,-6.307701587677002,0.1708165556192398,2.0969078540802,-0.8652932047843933,0.8879024386405945,2.3343892097473145,-1.2474734783172607,-0.8677831888198853,-0.38268178701400757,4.043359279632568,-1.7371629476547241,-4.7838826179504395,-4.167636871337891,1.3033071756362915,6.6473612785339355,-2.1839146614074707,2.8069419860839844,1.750462532043457,-2.618161916732788,2.247741937637329,-2.8572282791137695,8.997452735900879,-2.1198418140411377,4.555812358856201,0.7217230200767517,0.5667783617973328,-0.18862652778625488,1.5261895656585693,-5.055846691131592,-2.1891844272613525,2.2650933265686035,6.883592128753662,0.38323694467544556,-2.7493820190429688,-0.07383427768945694,-0.44772517681121826,-1.0256054401397705,0.23860742151737213,-1.339273452758789,-2.836268186569214,-2.364452362060547,-0.47943204641342163,2.666266679763794,-0.6260800957679749,-0.09426075220108032,-2.2745606899261475,3.6609649658203125,4.27014684677124,1.1405243873596191,-2.3021490573883057,1.340903639793396,-2.8668053150177,-1.6745610237121582,-0.3252062499523163,1.4025802612304688,3.3400139808654785,-3.3456814289093018,0.8802928924560547,3.8347160816192627,-1.3482383489608765,-6.294110298156738,-3.6130239963531494,-5.145914554595947,-0.21459056437015533,0.3526163101196289,-3.9942502975463867,-5.402000904083252,-0.5157928466796875,1.0300440788269043,-0.22198715806007385,-1.3055044412612915,-2.3128325939178467,3.613431930541992,0.7212842106819153,7.635899543762207,0.18950481712818146,-1.8654085397720337,3.3838071823120117,-0.32461339235305786,-0.28265616297721863,-1.8816940784454346,7.496301651000977,-1.6311336755752563,-0.2337043136358261,-8.906037330627441,1.65891695022583,-2.1926913261413574,-3.892504930496216,3.4647419452667236,1.0443239212036133,-1.318305492401123,-3.302673101425171,1.2756439447402954,2.4627838134765625,6.753573894500732,1.1596530675888062,-4.354905128479004,-2.8025624752044678,-1.9193817377090454,1.012284517288208,2.929067611694336,-3.033433675765991,-2.987562656402588,2.8227739334106445,-0.7338098883628845,-4.115737438201904,1.1891568899154663,-7.091860294342041,-1.524877667427063,-3.815249443054199,2.5801892280578613,-1.249523401260376,1.8726578950881958,-2.346129894256592,0.40986377000808716,2.2648260593414307,-0.0070009371265769005,-1.604937195777893,7.217586517333984,-1.3271516561508179,-0.585703432559967,2.7658274173736572,-2.8924245834350586,-1.8236340284347534,-4.139923572540283,-3.5787601470947266,3.5206470489501953,6.932949542999268,3.143165111541748,-3.6213912963867188,-5.5444207191467285,-3.384427547454834,-0.706948459148407,0.7318337559700012,-5.718910217285156,1.0625590085983276,8.258378028869629,-2.5068159103393555,1.5195633172988892,-4.486362934112549,1.782439112663269,-1.9256840944290161,1.396248698234558,1.5169998407363892,-4.003109931945801,1.645303726196289,1.053481101989746,-2.7927913665771484,1.2880898714065552,-1.2910795211791992,-6.6060261726379395,5.1051483154296875,-0.8177725672721863,4.308004856109619,1.6388636827468872,-1.5370237827301025,5.9691081047058105,-5.683413505554199,-0.7604551315307617,-1.9509674310684204,-0.678397536277771,-0.3605725169181824,6.408292293548584,1.3007891178131104,3.229736089706421,-0.9570712447166443,1.1275265216827393,-2.716759204864502,0.15387114882469177,2.1760458946228027,-3.0838232040405273,-3.3855714797973633,4.536764144897461,-4.251428127288818,-7.9719014167785645,-3.551424026489258,-5.996895790100098,3.0596272945404053,-0.19970963895320892,0.4997033178806305,2.0367841720581055,0.8607282042503357,-9.913108825683594,-2.807054281234741,1.2500841617584229,0.6727224588394165,-0.6728910803794861,1.419755458831787,2.930925130844116,-1.581751823425293,3.8288912773132324,1.1097956895828247,2.913640260696411,-4.69462776184082,-3.5492634773254395,1.9166339635849,-0.015635421499609947,4.230494022369385,0.5483354926109314,5.095211505889893,-4.527791500091553,-0.4480118453502655,4.039353847503662,-1.8291231393814087,2.6732468605041504,4.890900135040283,-1.3427057266235352,0.5752979516983032,4.642094612121582,4.1250457763671875,-1.0806390047073364,4.500986576080322,4.155993461608887,-0.6355438232421875,-1.5608930587768555,0.6178154945373535,0.12741559743881226,11.70093822479248,1.3333852291107178,-5.8946452140808105,4.825130939483643,0.22063493728637695,4.5068535804748535,-3.599403142929077,-3.7300939559936523,-5.142202377319336,3.6744167804718018,-1.3519822359085083,0.4909569025039673,0.7586514949798584,-4.682286262512207,-3.3563072681427,4.669466972351074,-0.8612692952156067,-1.8583552837371826,5.890474796295166,-1.2969101667404175,4.671620845794678,3.3609697818756104,-3.0943243503570557,0.0850691944360733,5.176321983337402,1.6364648342132568,-4.760904788970947,-1.3927812576293945,0.06727278232574463,5.345211505889893,3.299010753631592,-4.412222385406494,0.3972754180431366,-5.44537878036499,5.753441333770752,1.1788733005523682,2.7933688163757324,1.7618402242660522,1.8501427173614502,2.7886834144592285,1.9961310625076294,1.4692939519882202,1.9166531562805176,6.766496181488037,4.73324728012085,-6.350697994232178,-2.856508731842041,-4.09020471572876,-0.09816814213991165,-4.783427715301514,0.2726486623287201,6.703761100769043,5.264937877655029,-0.8373537659645081,-6.363037109375,-8.615543365478516,-3.11746883392334,1.5858529806137085,4.917567729949951,-2.0579371452331543,-2.788478374481201,3.5335347652435303,0.49653899669647217,5.38320255279541,8.404579162597656,3.035414934158325,-8.541099548339844,0.3884785771369934,3.026559829711914,-1.5354043245315552,-2.3996427059173584,-5.08775520324707,5.034589767456055,-4.728330135345459,-0.5420781373977661,-2.9049994945526123,-4.106952667236328,-1.7770757675170898,0.035961780697107315,3.347050189971924,1.6944258213043213,1.7297232151031494,-1.4721808433532715,0.5725987553596497,-6.030242919921875,-4.593624591827393,2.011528491973877,0.48140713572502136,-5.571067810058594,-1.7762047052383423,-8.039122581481934,0.31280314922332764,-2.7216596603393555,-3.0390586853027344,0.8099053502082825,-7.564577579498291,-3.202829599380493,3.547805070877075,3.741595983505249,-4.011846542358398,0.7904600501060486,-4.681108474731445,-6.544406414031982,-1.3074042797088623,1.4292176961898804,-0.04492180421948433,2.0361669063568115,-1.53819739818573,0.5029553771018982,5.947168827056885,1.845304012298584,6.7937774658203125,-0.46513280272483826,-0.020479490980505943,-2.322838306427002,-5.7350239753723145,-1.788389801979065,-2.271500825881958,2.011357307434082,2.4835355281829834,-3.3818199634552,0.11539223045110703,-1.1664460897445679,1.7906434535980225,-3.3410778045654297,-3.643784761428833,-7.4690423011779785,-6.56914758682251,6.123844146728516,2.694255828857422,6.655964374542236,-1.8575884103775024,4.262815952301025,3.115356922149658,-1.8189769983291626,-3.4482924938201904,4.436944961547852,-2.0304441452026367,-0.28612029552459717,-2.3934855461120605,4.574805736541748,-3.332017183303833,2.3879947662353516,1.258185625076294,-0.26865413784980774,-6.655214786529541,2.5377423763275146,-5.189742088317871,-1.8855398893356323,-3.4074954986572266,-5.063748359680176,-1.4218288660049438,3.2837672233581543,1.0945936441421509,4.700606346130371,4.544920921325684,0.6564035415649414,-0.7848002314567566,-0.8242366313934326,4.203673362731934,-0.9846176505088806,-4.625539302825928,-0.8144856691360474,-0.04304980859160423,4.317598819732666,3.9520771503448486,2.2636241912841797,1.5220335721969604,-0.2777344286441803,-2.925720453262329,-1.3315706253051758,2.216937780380249,6.143080234527588,-2.5993552207946777,-1.2309356927871704,2.7143688201904297,-0.4495411217212677,0.1668587327003479],"xaxis":"x","y":[0.8694708347320557,2.996119260787964,2.815622329711914,-1.1954847574234009,1.5622974634170532,-1.845407485961914,-3.460927963256836,-1.3695660829544067,0.3746860921382904,1.2877014875411987,-1.1250147819519043,-0.2601541578769684,-0.6378770470619202,-0.5605581998825073,0.3482101857662201,-2.3418684005737305,-2.1951048374176025,0.22343704104423523,0.06662745028734207,-1.618050217628479,2.0272765159606934,0.2600097358226776,-0.7223504185676575,-0.009808179922401905,0.8008738160133362,-2.9954941272735596,-1.405261754989624,-0.02134554088115692,-0.8132045269012451,-1.5354334115982056,-0.7535904049873352,-1.2751202583312988,-2.8926267623901367,-0.8991247415542603,0.04121384024620056,-2.177772045135498,-2.1343276500701904,-1.4774116277694702,1.1914491653442383,-0.38005197048187256,1.0018349885940552,0.8460996150970459,0.5045467019081116,-0.8674473762512207,-1.6534688472747803,-1.2795734405517578,-2.076385974884033,1.3738858699798584,-0.9763845205307007,-0.491506427526474,-3.310237169265747,-0.19184669852256775,-0.6978062987327576,-3.6217873096466064,-1.7309409379959106,-1.6490182876586914,2.5052437782287598,1.3919312953948975,-0.10760810971260071,0.4442489743232727,0.5847231149673462,0.16820049285888672,-0.8080828785896301,1.2523857355117798,-0.7212403416633606,-1.1294671297073364,-4.266604900360107,-0.9085303544998169,-0.7719676494598389,0.48043563961982727,-0.5355837345123291,0.9365022778511047,-0.48173460364341736,-1.9479950666427612,2.223482370376587,0.7903945446014404,1.2181065082550049,1.8973480463027954,0.13481637835502625,-0.11286339908838272,1.2009174823760986,-1.0098801851272583,0.6590168476104736,-2.567831039428711,-1.5002325773239136,-1.361339807510376,-2.6289446353912354,3.857398271560669,-0.770554780960083,-0.33206456899642944,-0.691431999206543,-0.9696308374404907,1.7835915088653564,-0.058492712676525116,-3.060736656188965,-1.6545355319976807,-4.5171332359313965,-2.90091609954834,2.955726146697998,7.126415252685547,-0.2304159700870514,-4.410159587860107,-2.7498910427093506,0.2214759737253189,8.14641284942627,1.5776335000991821,-0.4164043366909027,-4.183259010314941,-2.4679250717163086,-3.124063014984131,0.9095832705497742,4.199721336364746,-1.1116158962249756,0.05666979402303696,-3.1618151664733887,-1.82895028591156,-0.6473932862281799,-0.33291876316070557,-0.490764319896698,-1.6971269845962524,-0.4793533682823181,-2.0106942653656006,0.8941130638122559,-1.5644510984420776,-0.6728526949882507,-1.0273220539093018,0.7914055585861206,-2.7771458625793457,-0.3011709153652191,-0.21443727612495422,1.2358654737472534,2.580787181854248,1.5307317972183228,-1.0579099655151367,-3.853879928588867,-0.4790601134300232,-2.3867361545562744,-0.6140120029449463,-3.7887213230133057,-3.6383321285247803,8.435462951660156,4.8930487632751465,-1.1817010641098022,0.7574959397315979,2.1291589736938477,2.966512441635132,-0.6178267002105713,-0.39338016510009766,-1.4908366203308105,0.4758494198322296,1.3039946556091309,-0.8321311473846436,0.8279938101768494,-2.693668842315674,-3.8239693641662598,4.316201686859131,2.629209280014038,0.26681438088417053,1.271407961845398,1.5863391160964966,1.4941567182540894,-1.1823490858078003,-1.5990465879440308,-1.2295129299163818,-1.4523766040802002,2.2730555534362793,0.7306851148605347,-0.6224253177642822,-3.7913620471954346,-0.1346360743045807,-0.506352424621582,0.8348580002784729,1.3999375104904175,-0.8797750473022461,-0.15546461939811707,-0.42871662974357605,-2.5870678424835205,-0.37786611914634705,-2.8268823623657227,2.561802387237549,1.9246468544006348,0.23516429960727692,-2.062471628189087,-1.5306432247161865,-1.577721118927002,0.3452131152153015,-0.13997530937194824,1.265523076057434,-1.4352748394012451,1.8360539674758911,0.029755352064967155,0.4076039791107178,-0.671973466873169,-0.4816071093082428,1.5988272428512573,1.1312096118927002,0.2430802285671234,2.1073224544525146,-2.747068166732788,2.193481683731079,-0.30173367261886597,0.11518801748752594,1.9439983367919922,0.5471686720848083,-0.35967686772346497,-0.9394145011901855,-1.5260270833969116,8.729360580444336,1.0198376178741455,-0.6822750568389893,-1.8133008480072021,-1.335668921470642,0.6262459754943848,3.6733837127685547,-0.694953978061676,-1.2566341161727905,2.4749245643615723,0.7895696759223938,-0.854773759841919,-0.6105740070343018,6.285892486572266,-0.3209371566772461,0.7560034990310669,2.1649792194366455,0.15781213343143463,-1.0035433769226074,-2.1913204193115234,-0.05490298196673393,-0.14679217338562012,-1.1572062969207764,-0.761897087097168,-0.7738943696022034,-1.2287311553955078,-0.12771622836589813,-1.4549115896224976,-0.9458336234092712,0.8413608074188232,-0.7882797718048096,-2.206298589706421,-1.1111291646957397,1.1953502893447876,-1.1308631896972656,-2.8289506435394287,4.457761287689209,2.3059937953948975,-2.4895637035369873,2.258392095565796,-0.32328781485557556,2.2468101978302,-0.8194273114204407,-3.8115394115448,0.36795729398727417,0.27432331442832947,4.159948825836182,-0.5097637176513672,-0.12617658078670502,-0.529747486114502,-2.4021642208099365,-5.260644912719727,-1.087950587272644,-4.847179889678955,-1.407091736793518,0.2416086494922638,-2.1087231636047363,0.7603581547737122,-0.39156273007392883,2.160339593887329,0.9344603419303894,-3.429111957550049,-2.219313383102417,0.06919344514608383,-3.0900096893310547,0.2352093756198883,-1.5673680305480957,2.7706453800201416,-3.1069209575653076,-4.489226818084717,-0.15670666098594666,-0.6594967842102051,-3.239375352859497,0.8559631705284119,1.822306513786316,-1.5670174360275269,-0.05015510693192482,2.538581132888794,2.09191632270813,1.1736406087875366,-1.000722885131836,0.6780276894569397,1.0269854068756104,1.8436230421066284,3.2709410190582275,-1.1041008234024048,2.8550469875335693,-5.392828464508057,-2.5652170181274414,-3.0121405124664307,-2.938840627670288,-5.883024215698242,-4.279268741607666,3.1927437782287598,-2.0498106479644775,0.9408297538757324,2.781146764755249,-3.7098898887634277,2.2415263652801514,-1.1283936500549316,-0.2865675091743469,2.2228317260742188,0.1494932770729065,-2.1471807956695557,1.3208354711532593,-0.6428682208061218,0.30156782269477844,1.650413155555725,2.1573073863983154,2.1991488933563232,-0.6373098492622375,-2.6660430431365967,0.06134524196386337,0.2594722509384155,0.3777429759502411,0.5688411593437195,-3.3093888759613037,0.8681575655937195,-0.04902981221675873,1.7672230005264282,-0.9109614491462708,-1.4888110160827637,-3.8773679733276367,-1.774387001991272,3.5374484062194824,1.1476526260375977,-3.018620491027832,1.3698540925979614,5.662445545196533,-1.9898871183395386,-0.19819380342960358,-1.712641954421997,-1.158553957939148,1.2166039943695068,-0.5496259331703186,1.406913161277771,1.0968098640441895,-0.6741600632667542,7.236550807952881,1.7534723281860352,0.34146401286125183,-1.570701003074646,-3.6775097846984863,1.820672869682312,-2.1785268783569336,-2.0172817707061768,-1.595584750175476,-1.1522769927978516,0.7660521864891052,-0.8220889568328857,-0.9813430309295654,-1.7694767713546753,2.4272186756134033,2.853292226791382,-0.2723845839500427,0.689851701259613,-0.7446726560592651,0.6236824989318848,3.3747146129608154,0.9007624983787537,1.321948766708374,5.600836277008057,1.712926983833313,-0.5095086097717285,0.8122243285179138,1.4101886749267578,1.7913867235183716,0.9792684316635132,0.7530383467674255,1.0453448295593262,3.812777519226074,-1.3182883262634277,4.012703895568848,-0.9658846259117126,0.9811657071113586,1.7758609056472778,0.035638440400362015,2.4261844158172607,-1.1060121059417725,-3.8341281414031982,-4.83658504486084,0.808627724647522,0.3059088885784149,-0.908168613910675,-1.6110060214996338,-0.6610037684440613,1.8972944021224976,2.1506900787353516,-3.6788816452026367,-0.5605013370513916,-1.9152311086654663,-0.4658626914024353,3.9070005416870117,-0.16573838889598846,0.4364311993122101,2.1516411304473877,4.797669410705566,2.8221912384033203,-1.2262544631958008,5.89559268951416,-2.215810537338257,-0.9504077434539795,-0.8525049686431885,4.03191614151001,-0.26355111598968506,-1.467753291130066,5.40432071685791,-2.9465932846069336,-1.7770966291427612,-3.9107911586761475,-2.7362756729125977,7.657130241394043,0.5641249418258667,1.247023582458496,3.1375935077667236,1.8719370365142822,3.148235559463501,-3.8335015773773193,-4.940777778625488,6.082406044006348,0.3180052638053894,-2.6722488403320312,1.4207768440246582,2.7197556495666504,2.6397509574890137,-2.018606185913086,-1.0744150876998901,-1.1487599611282349,0.7756173610687256,2.176572322845459,0.6078751683235168,-3.5665862560272217,-0.6659103035926819,1.92134428024292,3.163961410522461,0.5115735530853271,0.5192679166793823,-0.7340417504310608,-3.1756279468536377,-1.0235921144485474,-3.642071008682251,-0.04721621796488762,0.324858158826828,-1.1154124736785889,1.0335984230041504,-3.238539457321167,1.599147915840149,-1.0510749816894531,2.2892355918884277,-1.6936681270599365,0.3187503218650818,2.8078439235687256,-3.914705991744995,-0.8217913508415222,1.668858289718628,1.005751609802246,1.6401653289794922,-2.4018542766571045,2.061182975769043,0.21031267940998077,-1.6527976989746094,1.0228394269943237,-1.950645923614502,1.197016954421997,-0.3227798044681549,-0.4084412753582001,-3.246195077896118,0.03547853231430054,-4.09659481048584,-2.1871869564056396,2.4079318046569824,2.0480833053588867,2.5201382637023926,0.36535942554473877,2.564008951187134,-0.5308527946472168,0.5418288111686707,-1.9670802354812622,1.21895170211792,0.6874061822891235,0.8692610859870911,-2.6760995388031006,1.194308876991272,0.09214948862791061,-2.083911895751953,-0.22251902520656586,0.17657171189785004,5.319089889526367,1.4361445903778076,6.156388759613037,0.2928007245063782,-2.0989584922790527,-3.1527607440948486,-0.3967934846878052,-2.447906732559204,-2.480476140975952,-0.16698111593723297,-3.2178614139556885,-1.369267463684082,2.442538022994995,-1.8154720067977905,-0.1696503907442093,1.2723653316497803,-0.7839028239250183,2.6237807273864746,0.36620280146598816,0.4773859679698944,-2.3204097747802734,4.03815221786499,-0.41516315937042236,1.2819781303405762,-1.1519132852554321,2.31540846824646,1.2290085554122925,0.9486817717552185,-2.18288516998291,-2.6215476989746094,3.8922994136810303,-3.7265565395355225,-3.4776625633239746,-3.7086849212646484,-1.6432236433029175,0.36618706583976746,5.357827663421631,-2.017585039138794,0.07427804917097092,2.1310389041900635,-1.2376582622528076,-0.18714271485805511,2.1691184043884277,2.098464250564575,2.1627697944641113,-2.782449960708618,-2.6648480892181396,2.593906879425049,-3.5723602771759033,-0.16523241996765137,-3.8979570865631104,-5.097107410430908,6.495848178863525,0.7164801359176636,0.4673803448677063,-2.433201551437378,4.421757221221924,-0.47083255648612976,-1.2309514284133911,-1.3735495805740356,3.55976939201355,-1.975500226020813,-0.6468795537948608,1.405555009841919,3.9332985877990723,0.03837283328175545,-0.4393451511859894,-1.360207200050354,0.5388351082801819,-1.2274316549301147,0.5045048594474792,0.7134578227996826,-4.949018955230713,-4.950201034545898,-2.9008307456970215,-0.021723901852965355,-1.0577698945999146,-3.371073007583618,1.2969729900360107,-0.48016348481178284,0.7671273946762085,0.2484133243560791,0.5510039925575256,-0.7792974710464478,-0.04595155268907547,0.48485231399536133,-5.054408550262451,-0.8148693442344666,1.2116843461990356,-2.0050017833709717,1.3611149787902832,-0.33767950534820557,3.663893699645996,0.1047326996922493,-1.1535128355026245,-0.3573281466960907,2.2919390201568604,-0.42145010828971863,3.7163712978363037,0.3847408890724182,1.9646469354629517,0.40298226475715637,1.4451960325241089,2.822427272796631,-0.4152815043926239,-1.374292254447937,1.724871277809143,0.6642205715179443,-0.905770480632782,-2.2713072299957275,-2.0329177379608154,-0.39253148436546326,-1.2622121572494507,1.008161187171936,-1.2375601530075073,-0.2714633047580719,0.01818006858229637,2.374206066131592,1.7728325128555298,2.1564571857452393,-3.149298906326294,2.9575393199920654,2.3457977771759033,2.700953960418701,1.483498454093933,-0.22971655428409576,-2.6127052307128906,-0.48582202196121216,1.1243928670883179,-0.9055498838424683,3.6095547676086426,3.520113706588745,-1.698996901512146,-0.4183657169342041,3.201005458831787,1.3394542932510376,-1.3195217847824097,-1.8289467096328735,-2.276944160461426,1.8439505100250244,-0.5261495113372803,0.121628537774086,-2.838210105895996,0.9136218428611755,0.6451017260551453,-1.8326997756958008,1.9654154777526855,2.496654748916626,1.627010703086853,1.7852814197540283,0.24492648243904114,4.952192783355713,-2.465991497039795,-0.8217288255691528,-3.0926175117492676,3.3181471824645996,-2.261265993118286,-3.340059280395508,-2.3040049076080322,4.907275199890137,-3.7034573554992676,2.517284393310547,-0.5777310132980347,-1.146620273590088,1.2494781017303467,4.430436134338379,4.6443891525268555,-0.4669678807258606,-3.4992895126342773,-6.643031120300293,-4.177077770233154,1.1398050785064697,-2.8136394023895264,1.0755201578140259,1.5742955207824707,-2.719827651977539,-0.04547703638672829,3.591562032699585,3.250333786010742,2.153728723526001,-3.657949924468994,-3.243208169937134,2.3914151191711426,-3.3443808555603027,1.2026623487472534,0.8519225120544434,-1.7981804609298706,1.306283712387085,10.81544303894043,2.37799334526062,5.6448211669921875,-0.03230420500040054,-0.08565189689397812,2.2563960552215576,-1.583066701889038,0.05332815274596214,1.7129361629486084,-1.6253225803375244,1.3836578130722046,-1.4247974157333374,0.839956521987915,-2.5002377033233643,-2.2972819805145264,-1.824134349822998,2.0847086906433105,-0.036023419350385666,-0.06313811242580414,-0.03709246590733528,-3.4094457626342773,-2.0986759662628174,-0.3017728924751282,-2.4107186794281006,1.1489458084106445,0.7805357575416565,4.357479095458984,1.9526002407073975,-0.8868429064750671,2.62382435798645,-1.0415453910827637,0.3344433903694153,-2.0446016788482666,-1.5170021057128906,-2.121019124984741,-2.3164892196655273,-3.5278162956237793,0.5455809235572815,-0.9425843358039856,-1.4537633657455444,3.1327552795410156,1.0755592584609985,0.36173397302627563,0.37002062797546387,8.04270076751709,-1.123498558998108,0.8390033841133118,-1.4836792945861816,5.92299222946167,2.292310953140259,-1.2270619869232178,-1.1040277481079102,1.8662149906158447,2.4129438400268555,2.312509775161743,-2.9240312576293945,0.9207028150558472,0.021029211580753326,-1.522147297859192,-2.194134473800659,-0.6531227231025696,-1.0828684568405151,0.9254307746887207,0.006915336474776268,0.07744143158197403,3.0560383796691895,1.470015287399292,1.4518041610717773,-2.4860453605651855,1.8607827425003052,2.1707229614257812,0.6379968523979187,2.604449510574341,4.218001842498779,-0.9396244883537292,0.3629032373428345,4.9298224449157715,-0.5457403659820557,3.0513105392456055,-2.7773818969726562,3.0465734004974365,2.561295747756958,-2.70894455909729,-2.72621488571167,-0.40538257360458374,-0.483574241399765,-0.40438705682754517,2.652750253677368,-0.6611615419387817,-3.409391164779663,0.9063512086868286,-2.7419917583465576,3.0469934940338135,-1.0819669961929321,3.5234594345092773,-1.339093804359436,4.545552730560303,1.22939133644104,2.15297532081604,-3.3556151390075684,0.34934327006340027,-0.13339322805404663,1.6114673614501953,-1.5669387578964233,1.0714211463928223,4.195094108581543,-3.0755412578582764,-0.37957316637039185,-0.5136317014694214,-0.02132556401193142,0.2083469033241272,-1.6926071643829346,2.2245476245880127,1.9165807962417603,-2.6315338611602783,0.16455288231372833,-3.565002679824829,0.9809302091598511,-2.065999984741211,2.2095253467559814,-0.6795826554298401,-2.8175487518310547,-1.8957685232162476,2.027508020401001,-0.21696963906288147,0.002665229607373476,-3.9957964420318604,1.5699301958084106,-0.8283367156982422,-0.5361363291740417,-0.3105134963989258,-0.42128029465675354,-0.18846897780895233,-1.972064733505249,0.7137831449508667,-2.777719020843506,-1.0693919658660889,0.10627710074186325,-0.7595224380493164,-2.197047472000122,5.352449893951416,2.4630978107452393,-0.20070011913776398,2.2583839893341064,0.9011811017990112,-0.9693353176116943,-3.1396965980529785,0.21021249890327454,-3.308082342147827,-1.3222705125808716,-0.5408174395561218,-4.227806568145752,-2.446925401687622,-1.9043586254119873,-1.0129519701004028,1.5475269556045532,-2.834747076034546,6.27354097366333,5.6397881507873535,-0.07095362991094589,-0.1038321852684021,1.8699451684951782,-1.8237335681915283,-0.5184738636016846,-1.4718446731567383,-1.3928767442703247,-0.9601389765739441,0.18983043730258942,3.7808661460876465,-2.8149337768554688,-1.061095118522644,-2.34610915184021,3.7756659984588623,1.3796517848968506,-1.0598540306091309,-2.629054546356201,-0.7339480519294739,-4.9692606925964355,-0.37107735872268677,-2.027651071548462,2.9463632106781006,-3.1712632179260254,-1.2179526090621948,3.973966598510742,1.116291880607605,0.8070346713066101,-0.6706216335296631,0.651174008846283,1.3359322547912598,2.143768310546875,2.09460186958313,2.8578197956085205,2.5245447158813477,3.0200605392456055,3.863293409347534,-0.16735903918743134,-1.0790237188339233,4.827416896820068,-0.2227851152420044,2.245046615600586,0.10696754604578018,-1.8831945657730103,-0.0033194436691701412,-1.4021326303482056,2.679131269454956,0.036231521517038345,3.23441481590271,0.13620291650295258,2.094756841659546,-0.8992837071418762,-3.0349719524383545,-3.6431374549865723,-0.8099656105041504,-2.6153717041015625,-0.29565995931625366,0.41858533024787903,1.582600474357605,2.246112585067749,-2.3196873664855957,-1.449666976928711,-0.9045727252960205,-2.849287509918213,3.0552375316619873,1.4163419008255005,-3.377077579498291,-2.9081003665924072,-0.8692387342453003,0.4608389735221863,0.06549084931612015,-0.5704502463340759,2.428644895553589,-0.8279678225517273,-0.5812512040138245,0.6977270245552063,1.9143012762069702,-0.8085545897483826,-4.900876045227051,2.4051499366760254,0.3512952923774719,-0.38580983877182007,-1.2675155401229858,0.6264578104019165,0.8411076068878174,-1.356038212776184,-1.6531813144683838,-4.027482986450195,2.356688976287842,3.495650053024292,1.362724781036377,1.302780032157898,-0.47246938943862915,4.79101037979126,-1.6556788682937622,-0.953707218170166,2.5449609756469727,2.874830722808838,-3.7846803665161133,-4.830830097198486,-0.06590887904167175,0.9145386815071106,3.2040557861328125,-3.050123929977417,-0.2374897301197052,-1.7106353044509888,-0.48698896169662476,-0.7467425465583801,-0.8805660009384155,4.001055717468262,2.019695281982422,-3.8324427604675293,0.22356732189655304,-2.863629102706909,1.3032987117767334,2.291715621948242,-0.7372957468032837,-0.6035779118537903,0.1411333829164505,-0.7073177695274353,0.5288699865341187,0.3338455557823181,-2.4001758098602295,0.4183981418609619,2.463954448699951,-4.575450420379639,-1.6322381496429443,1.606119155883789,-2.156505823135376,2.6302199363708496,-5.655728816986084,1.6520934104919434,3.967597007751465,2.869858503341675,0.5410991907119751,0.96283358335495,3.6737453937530518,1.2003124952316284,-1.5187636613845825,-0.1959417760372162,-1.425016164779663,-0.8002532124519348,0.118509940803051,-0.14931422472000122,0.6226131916046143,-0.3901490271091461,-1.6376773118972778,-2.0112082958221436,2.1446986198425293,-1.2325565814971924,0.6355416178703308,2.7665469646453857,-0.9902048707008362,2.2416882514953613,0.0744960755109787,2.183070182800293,4.856414318084717,-3.2504069805145264,-0.6454969644546509,-2.1039137840270996,-0.5132392048835754,-1.177716851234436,-2.2303690910339355,0.5890149474143982,3.0083024501800537,-1.0412733554840088,-0.7276728749275208,-0.1306009441614151,2.1579067707061768,-0.7484848499298096,4.12955904006958,0.19678834080696106,-1.8554677963256836,-2.7362353801727295,0.155549094080925,0.09597976505756378,1.4078606367111206,-0.621146559715271,-3.5198686122894287,1.8104804754257202,0.2575634717941284,0.5730908513069153,-3.669326066970825,0.8081802725791931,-0.34164032340049744,-1.27984619140625,-2.9332616329193115,1.305494785308838,0.8166778087615967,0.5677902698516846,-1.2942863702774048,3.1393258571624756,-2.2926485538482666,-1.8996329307556152,1.8831162452697754,1.410807728767395,-0.1380738615989685,3.183180093765259,-0.734872043132782,-0.9600180387496948,-0.4844793379306793,-0.1038428544998169,2.3098671436309814,0.5001188516616821,-4.259586811065674,-0.01164485327899456,1.7587270736694336,-2.009204864501953,0.3589305281639099,-0.792252242565155,-6.422672748565674,1.712473750114441,-2.845693588256836,-0.8960685133934021,-0.6786363124847412,-0.8779922723770142,2.7613067626953125,2.1101839542388916,1.5542418956756592,-1.7914438247680664,0.09622030705213547,-1.438734531402588,-2.500824451446533,1.5313544273376465,1.157492995262146,-1.0870071649551392,1.137083888053894,-1.4336938858032227,-1.1875132322311401,-2.5788025856018066,-0.26490330696105957,2.569779634475708,-1.3234714269638062,-3.4917004108428955,-0.32829952239990234,-0.2513826787471771,-1.355506181716919,0.5495710968971252,-1.8892459869384766,2.692176342010498,2.471764326095581,0.7255761623382568,2.236447811126709,0.786490797996521,0.9188079833984375,-1.4831372499465942,1.5996103286743164,1.3768094778060913,-0.4823136329650879,-0.21030963957309723,1.2522228956222534,0.8091447949409485,-0.006362860091030598,-1.1168708801269531,-0.09749945253133774,-1.1711955070495605,-1.0831358432769775,0.11787191033363342,2.9350059032440186,-0.7843350768089294,-3.6963932514190674,-0.13558602333068848,0.44358885288238525,1.040592908859253,-2.569768190383911,1.2976408004760742,-3.632274866104126,-1.8685822486877441,1.4387431144714355,-0.8136664032936096,1.3484492301940918,1.1881564855575562,-1.7502703666687012,1.892024278640747,-1.019708275794983,0.26222339272499084,1.2286932468414307,-2.5824294090270996,-0.26509204506874084,-1.4453439712524414,6.637756824493408,-3.090714693069458,0.2374068945646286,3.3133280277252197,0.1417398452758789,0.7374356389045715,0.010822809301316738,-0.5753375887870789,-0.07205817848443985,-1.9374840259552002,-0.07789357006549835,2.4914424419403076,0.9710296392440796,-0.01990341767668724,-1.6205388307571411,0.5977770090103149,1.349372386932373,-2.521728992462158,-4.347507476806641,-2.554969310760498,0.9076154232025146,0.7550259828567505,-1.8839164972305298,3.3444080352783203,5.050083637237549,1.0732332468032837,-4.771300792694092,6.4261555671691895,-2.381176233291626,-1.4493458271026611,0.09957166016101837,-3.736647367477417,2.8079419136047363,-1.2484928369522095,0.1294020563364029,-0.33818957209587097,-0.9257047772407532,-2.012500524520874,-0.34669893980026245,-1.0811738967895508,1.7855409383773804,0.00985149759799242,-1.1412538290023804,2.264160394668579,0.7181172370910645,0.020859574899077415,-0.44704487919807434,0.12502209842205048,0.07573827356100082,1.9426242113113403,-1.0711936950683594,-0.753379225730896,0.22615505754947662,-0.47344598174095154,-0.0598166361451149,-2.50193190574646,-1.8198262453079224,0.5616495609283447,-1.0836142301559448,2.6003763675689697,-0.1775333136320114,-2.0448670387268066,-3.780880928039551,-0.05183057859539986,-0.8886125087738037,2.9275062084198,3.004265069961548,1.0035638809204102,1.2448810338974,1.363071084022522,-2.2009785175323486,-2.317371129989624,0.6871606111526489,-0.05959778651595116,-0.6526194214820862,-2.8804123401641846,-0.450784832239151,2.045975685119629,0.7061522006988525,1.3131026029586792,-2.331728935241699,1.4584548473358154,1.0349880456924438,-2.5370876789093018,-0.12821060419082642,1.7189940214157104,-1.6051170825958252,1.270205020904541,-1.4745644330978394,-0.8988402485847473,2.616957426071167,-0.24851490557193756,5.030877113342285,-4.557457447052002,2.430690050125122,2.111121654510498,-1.619437575340271,-0.24825316667556763,-3.0273988246917725,0.7439892888069153,0.16659310460090637,1.1194998025894165,2.748769998550415,0.43646517395973206,-3.720662832260132,0.5012239813804626,-2.148696184158325,0.34768378734588623,1.2085206508636475,-3.7667653560638428,-0.3529111444950104,0.17604288458824158,-3.755547285079956,-2.2200870513916016,-2.73280930519104,-2.6864874362945557,0.20741593837738037,1.9487411975860596,0.3143402636051178,-1.671457052230835,1.770768404006958,0.8840028047561646,0.6147453784942627,2.4787776470184326,2.237668037414551,1.218251347541809,-0.18562230467796326,-0.723547637462616,0.13505670428276062,-2.5839672088623047,3.2070186138153076,2.0924079418182373,-0.35129299759864807,-1.3443816900253296,3.144615888595581,-0.6722713708877563,3.534775495529175,4.918873310089111,-1.1887528896331787,-1.263795018196106,4.7574615478515625,2.3240811824798584,1.1837449073791504,-4.4831695556640625,-1.3020495176315308,0.2544240653514862,1.2294131517410278,-1.1710251569747925,-0.01947459951043129,2.7090578079223633,0.4695219099521637,-0.641140341758728,2.0910847187042236,0.03844102472066879,0.10225827246904373,0.7590773105621338,1.9986364841461182,-3.1333017349243164,0.9991056323051453,1.7270113229751587,2.0697295665740967,0.9106760025024414,4.253509521484375,-2.013463258743286,1.9838626384735107,0.8198035955429077,-0.9452731013298035,-0.5759653449058533,0.8339107632637024,-1.9817287921905518,-0.04223639890551567,0.8726548552513123,0.5341706275939941,1.2183769941329956,-1.1668463945388794,-0.9726966619491577,-3.1408023834228516,3.809648036956787,-0.47476881742477417,1.587283730506897,2.278883695602417,0.6091713309288025,0.06680948287248611,1.7817573547363281,-2.4747426509857178,-0.6456933617591858,1.9480737447738647,0.5793362855911255,1.9670528173446655,3.1358609199523926,-3.580613374710083,-3.8909332752227783,2.272921562194824,-2.8282902240753174,-2.2235867977142334,0.46681007742881775,2.6663591861724854,-0.43226319551467896,1.780982255935669,-1.365441918373108,-1.2464300394058228,-2.9182636737823486,-1.7827733755111694,4.1342949867248535,0.5855893492698669,-0.9566391706466675,-1.848954200744629,1.6889235973358154,-4.476962566375732,1.5691730976104736,0.8639421463012695,8.413269996643066,2.6089041233062744,2.085780143737793,1.4400197267532349,2.524519205093384,-1.3565008640289307,0.12058164179325104,1.0062460899353027,3.3648762702941895,-0.9184914231300354,-1.3469542264938354,0.11057649552822113,-5.412775993347168,6.020942211151123,2.5590763092041016,0.02662399783730507,-2.405693769454956,-5.03508186340332,0.3034760057926178,0.0750623568892479,1.29774010181427,-0.937921941280365,2.5862767696380615,-0.7786912322044373,2.9929468631744385,1.3168514966964722,-1.9604264497756958,-0.08879842609167099,-2.4226412773132324,4.618700981140137,1.3671892881393433,0.03386899083852768,0.638088047504425,0.06443282216787338,1.104674220085144,-3.7995684146881104,-4.124148368835449,0.24828584492206573,-2.7511348724365234,-0.797001302242279,-2.430584192276001,0.17623016238212585],"yaxis":"y","type":"scattergl"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmapgl":[{"type":"heatmapgl","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"}}},"xaxis":{"anchor":"y","domain":[0.0,1.0],"title":{"text":"x0"}},"yaxis":{"anchor":"x","domain":[0.0,1.0],"title":{"text":"x1"}},"coloraxis":{"colorbar":{"title":{"text":"cluster"}},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]},"legend":{"tracegroupgap":0,"itemsizing":"constant"},"margin":{"t":60}},                        {"responsive": true}                    ).then(function(){
                            
var gd = document.getElementById('cc9ce53b-393c-4057-9837-1cfe9a57a5ac');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                });            </script>        </div>
</div>
</div>
<p>Looking into the words in the three clusters, it makes sense that the red cluster includes more “country” lyrics, while the blue cluster is hip hop and the yellow cluster is rock.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>To conclude, I have experimented with using different features and models to classify music by genre. The best performing model used only engineered features about the audio characteristics of the music and achieved a 76% test accuracy.</p>
<p>Although the models that used lyrics data did not perform better, we see that they have learned valid word embeddings that reflected the styles of the lyrics across the three genres.</p>
<p>For this attempt, I have chosen three genres that are quite distinctive from each other in terms of lyrics and music style. The models may face more challenges with data that include more genres that differ in a more subtle way.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
</div> <!-- /content -->



</body></html>