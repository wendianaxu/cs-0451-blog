[
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CSCI 0451 Blog - Diana Xu",
    "section": "",
    "text": "Implementing logistic degression with gradient descent.\n\n\n\n\n\n\nMar 1, 2023\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/perceptron/perceptron-blogpost.html",
    "href": "posts/perceptron/perceptron-blogpost.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Source code\n\n\nI implemented the perceptron update with the following steps:\n\nModifying the feature matrix \\(X\\) and vector of labels \\(y\\). This includes appending a column of 1s to \\(X\\) and turning \\(y\\) into a vector of -1s and 1s.\nInitializing a random weight vector \\(w\\).\nIn a for-loop that breaks either when accuracy reaches 1 or when the specified max_steps is reached,\n\nPick a random data point with index \\(i\\) and access its features \\(x_{i}\\) and label \\(y_{i}\\);\nCompute the predicted label \\(\\hat{y}\\) using the current \\(w\\);\nPerform update if \\(y\\) * \\(\\hat{y}\\) < 0:\n\n\n\nself.w = self.w + (y_i*y_hat < 0) * (y_i * x_)\n\nWhere only when y_i * y_hat < 0 is (y_i * x_) added to the current w.\nFinally, the accuracy of the current \\(w\\) is computed by using \\(w\\) to predict labels for all data points, and then calculating the number of correct predictions compared to \\(y\\). This accuracy is then appended to the history array.\n\n\n\n\n\n\nfrom perceptron import Perceptron\nimport numpy as np\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\n\nn = 100\np_features = 3\n\n# generate random data\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\n# fit perceptron to data\np1 = Perceptron()\np1.fit(X, y, max_steps = 1000)\n\n# visualizations\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize = (8, 3)) \n\n# visualize accuracy history\nax1.plot(p1.history)\nax1.set(xlabel = \"Iteration\",\nylabel = \"Accuracy\")\n\n# visualize data points and perceptron line\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nax2.set(xlabel = \"Feature 1\",\nylabel = \"Feature 2\")\n\nax2 = plt.scatter(X[:,0], X[:,1], c = y)\nax2 = draw_line(p1.w, -2, 2)\n\n\n\n\nHere, the data is linearly separable. The accuracy is able to reach 1.0 after 125+ iterations. The perceptron line generated is able to separate data with labels -1 and 1.\n\n\n\n\n# generate random data\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\n# fit perceptron to data\np2 = Perceptron()\np2.fit(X, y, max_steps = 1000)\n\n# visualizations\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize = (8, 3)) \n\n# visualize accuracy history\nax1.plot(p2.history)\nax1.set(xlabel = \"Iteration\",\nylabel = \"Accuracy\")\n\n# visualize data points and perceptron line\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nax2.set(xlabel = \"Feature 1\",\nylabel = \"Feature 2\")\n\nax2 = plt.scatter(X[:,0], X[:,1], c = y)\nax2 = draw_line(p2.w, -2, 2)\n\n\n\n\nHere, the data is not linearly separable as there are overlapping points. The accuracy fails to reach 1.0 after 1000 steps. The perceptron line does not perfectly separate the two groups of data.\n\n\n\nHere I generate random data with 6 features and feed it to the perceptron algorithm.\n\n# change the number of features\np_features = 7\n\n# generate random data\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\n# fit perceptron to data\np3 = Perceptron()\np3.fit(X, y, max_steps = 1000)\n\n# visualize accuracy history\nfig = plt.plot(p3.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nSince the accuracy has reached 1.0, the data should be linearly separable.\n\n\n\n\nThe runtime complexity of a single iteration of the perceptron update should be \\(O(p)\\). It would depend on the number of features but not the number of data points."
  },
  {
    "objectID": "posts/reflective-goal-setting/goal-setting.html",
    "href": "posts/reflective-goal-setting/goal-setting.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Wen (Diana) Xu\n\n\nThe knowledge we’ll develop in CSCI 0451 can be broadly divided into four main areas:\n\nTheory: mathematical descriptions of frameworks and algorithms.\nImplementation: effective coding and use of tools in order to implement efficient machine learning algorithms.\nExperimentation: performing experiments to assess the performance of algorithms and clearly communicating about the results.\nSocial responsibility: critical analysis of sources of bias and harm in machine learning algorithms; theoretical formulations of fairness and bias\n\nEvery student should grow toward each of these areas, but you can choose to specialize if you’d like! If there are one or two of these categories on which you’d especially like to focus, list them below. Feel free to include any details that you’d like – this can help me tailor the course content to your interests.\nI would like to focus more on experimentation and social responsibility. I am interested in the applications and implications of machine learning algorithms – how to make use of machine learning to solve problems in the real world, how to assess the performance of the algorithms in relation to the specific problem, and how we can effectively communicate machine learning to a broader audience. I also hope to look more into bias in machine learning algorithms – what causes them and how to identify and reduce sources of bias.\n\n\n\n\n\nMost blog posts will require around 5-8 hours on average to complete, plus time for revisions in response to feedback. Blog posts will most frequently involve a mix of mathematical problem-solving, coding, experimentation, and written discussion. Some blog posts will ask you to critically discuss recent readings in an essay-like format.\n\nSubmit a blog post in most weeks during the semester.\nSubmit the first draft of no more than two blog post after the “best-by” date.\nRevise at least five blog posts to the “No Revisions Suggested” level.\nGo above and beyond in at least two blog posts by performing experiments/exploring visualization choices to communicate results/discussing implications significantly beyond what is required.\n\n\n\n\nYou make a choice each day about how to show up for class: whether you’ll be prepared, whether you’ll engage with me and your peers in a constructive manner; and whether you’ll be active during lecture and discussions. We will also have a special opportunity this semester to engage with a renowned expert in machine learning, algorithmic bias, and the ethics of artificial intelligence.\nAn especially important form of course presence is the daily warmup. We’ll spend the first 10-15 minutes of most class periods on warmup activities. You’re expected to have prepared the warmup activity ahead of time (this means you’ll need to have completed the readings as well). Each time, we’ll sort into groups of 5-6 students, and one of you (randomly selected) will be responsible for presenting the activity on the whiteboard. If you’re not feeling prepared to present the activity, you can “pass” to the next person, or ask for help along the way.\n\nComplete all core readings prior to each class periods.\nComplete the optional readings that correspond to my areas of specialization.\nBe prepared for most warmup activities even on days when I am not the leader.\n“Pass” at most once when asked to lead the warmup activity for my group.\nPropose questions ahead of time for our guest speaker.\nOften work with classmates together on blog posts or other course work outside of class time.\nOften attend Peer Help or Student Hours (after preparing questions and working examples).\n\n\n\n\nTo finish off the course, you’ll complete a long-term project that showcases your interests and skills. You’ll be free to propose and pursue a topic. My expectation is that most projects will move significantly beyond the content covered in class in some way: you might implement a new algorithm, study a complex data set in depth, or conduct a series of experiments related to assessing algorithmic bias in a certain class of algorithm. You’ll be expected to complete this project in small groups (of your choosing), and update us at a few milestones along the way.\nPlease share a bit about what kind of topic might excite you, and set a few goals about how you plan to show up as a constructive team-member and co-inquirer (see the ideas for some inspiration).\n\nSubmit all project milestones (proposal, progress report, etc) on time.\nSet regular time each week to work with project partners.\nCommunicate with my group in a clear and timely manner.\nComplete all my designated work on time.\nDraft designated sections of the project report.\nRevise sections of the project report in response to feedback.\nTake the lead in creating and delivering part of the final project presentation.\nTake the lead in checking project figures for accuracy and clear labeling."
  },
  {
    "objectID": "posts/reflective-goal-setting/goal-setting.html#what-youll-learn",
    "href": "posts/reflective-goal-setting/goal-setting.html#what-youll-learn",
    "title": "Reflective Goal-Setting",
    "section": "What You’ll Learn",
    "text": "What You’ll Learn\nThe knowledge we’ll develop in CSCI 0451 can be broadly divided into four main areas:\n\nTheory: mathematical descriptions of frameworks and algorithms.\nImplementation: effective coding and use of tools in order to implement efficient machine learning algorithms.\nExperimentation: performing experiments to assess the performance of algorithms and clearly communicating about the results.\nSocial responsibility: critical analysis of sources of bias and harm in machine learning algorithms; theoretical formulations of fairness and bias\n\nEvery student should grow toward each of these areas, but you can choose to specialize if you’d like! If there are one or two of these categories on which you’d especially like to focus, list them below. Feel free to include any details that you’d like – this can help me tailor the course content to your interests.\nI would like to focus more on experimentation and social responsibility. I am interested in the applications and implications of machine learning algorithms – how to make use of machine learning to solve problems in the real world, how to assess the performance of the algorithms in relation to the specific problem, and how we can effectively communicate machine learning to a broader audience. I also hope to look more into bias in machine learning algorithms – what causes them and how to identify and reduce sources of bias."
  },
  {
    "objectID": "posts/reflective-goal-setting/goal-setting.html#what-youll-achieve",
    "href": "posts/reflective-goal-setting/goal-setting.html#what-youll-achieve",
    "title": "Reflective Goal-Setting",
    "section": "What You’ll Achieve",
    "text": "What You’ll Achieve\n\nBlog Posts\nMost blog posts will require around 5-8 hours on average to complete, plus time for revisions in response to feedback. Blog posts will most frequently involve a mix of mathematical problem-solving, coding, experimentation, and written discussion. Some blog posts will ask you to critically discuss recent readings in an essay-like format.\n\nSubmit a blog post in most weeks during the semester.\nSubmit the first draft of no more than two blog post after the “best-by” date.\nRevise at least five blog posts to the “No Revisions Suggested” level.\nGo above and beyond in at least two blog posts by performing experiments/exploring visualization choices to communicate results/discussing implications significantly beyond what is required.\n\n\n\nCourse Presence (Participation)\nYou make a choice each day about how to show up for class: whether you’ll be prepared, whether you’ll engage with me and your peers in a constructive manner; and whether you’ll be active during lecture and discussions. We will also have a special opportunity this semester to engage with a renowned expert in machine learning, algorithmic bias, and the ethics of artificial intelligence.\nAn especially important form of course presence is the daily warmup. We’ll spend the first 10-15 minutes of most class periods on warmup activities. You’re expected to have prepared the warmup activity ahead of time (this means you’ll need to have completed the readings as well). Each time, we’ll sort into groups of 5-6 students, and one of you (randomly selected) will be responsible for presenting the activity on the whiteboard. If you’re not feeling prepared to present the activity, you can “pass” to the next person, or ask for help along the way.\n\nComplete all core readings prior to each class periods.\nComplete the optional readings that correspond to my areas of specialization.\nBe prepared for most warmup activities even on days when I am not the leader.\n“Pass” at most once when asked to lead the warmup activity for my group.\nPropose questions ahead of time for our guest speaker.\nOften work with classmates together on blog posts or other course work outside of class time.\nOften attend Peer Help or Student Hours (after preparing questions and working examples).\n\n\n\nProject\nTo finish off the course, you’ll complete a long-term project that showcases your interests and skills. You’ll be free to propose and pursue a topic. My expectation is that most projects will move significantly beyond the content covered in class in some way: you might implement a new algorithm, study a complex data set in depth, or conduct a series of experiments related to assessing algorithmic bias in a certain class of algorithm. You’ll be expected to complete this project in small groups (of your choosing), and update us at a few milestones along the way.\nPlease share a bit about what kind of topic might excite you, and set a few goals about how you plan to show up as a constructive team-member and co-inquirer (see the ideas for some inspiration).\n\nSubmit all project milestones (proposal, progress report, etc) on time.\nSet regular time each week to work with project partners.\nCommunicate with my group in a clear and timely manner.\nComplete all my designated work on time.\nDraft designated sections of the project report.\nRevise sections of the project report in response to feedback.\nTake the lead in creating and delivering part of the final project presentation.\nTake the lead in checking project figures for accuracy and clear labeling."
  },
  {
    "objectID": "posts/logistic-regression/logistic-regression-blogpost.html",
    "href": "posts/logistic-regression/logistic-regression-blogpost.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "Source code\n\n\n\n\nI implemented gradient descent in a for-loop where the maximum number of iterations is the max_epoch specified in the fit() method.\nIn each iteration (epoch), \\(w\\) is first updated:\n\nself.w -= alpha*self.gradient(self.w, X_, y)\n\nwhere the gradient() function computes the gradient of logistic loss associated with the previous \\(w\\).\nThen, we compute the loss and accuracy of the current \\(w\\) and pass them to corresponding variables.\nFinally, a if-statement within the for-loop checks if the current loss is close to the previous loss and breaks the loop if they are close (i.e. the minimum loss is found).\n\n\n\nFor fit_stochastic(), an update is performed in each iteration within the for-loop where a different set of random points is picked each time:\n\nfor batch in np.array_split(order, n // batch_size + 1):\n    ...\n    # gradient step\n    self.w -= alpha*self.gradient(self.w, x_batch, y_batch)\n\nThis for-loop is nested within another for-loop that represents epochs. After updates are performed in all batches, loss and accuracy for the current epoch is calculated and the current loss is checked against the previous loss to see if the minimum is reached.\n\n\n\n\n\n\nIn this example, gradient descent with \\(\\alpha\\) = 0.1 converged before the 1000 steps. However, using the same data set with \\(\\alpha\\) increased to 10, the loss kept oscillating and never converged. This demonstrates how gradient descent would fail to converge when the learning rate is too large.\n\nfrom logistic_regression import LogisticRegression\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\n# generate data\nX, y = make_blobs(n_samples = 200, n_features = 2, centers = [(0, 0), (1, 1)])\n\n# alpha = 0.1\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 0.1, max_epochs = 1000)\n\n# plot\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient (alpha = 0.1)\")\n\n# alpha = 10\nLR.fit(X, y, alpha = 10, max_epochs = 1000)\n\n# plot\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient (alpha = 10)\")\n\nplt.loglog()\n\nxlab = plt.xlabel(\"Number of steps\")\nylab = plt.ylabel(\"Loss\")\nlegend = plt.legend() \n\n\n\n\n\n\n\nHere, we perform stochastic gradient descent with 4 different batch sizes on a data set with 10 features. The smaller the batch size, the quicker the algorithm converges.\n\n# generate data\nX, y = make_blobs(n_samples = 200, n_features = 10, centers = [(0, 0), (1, 1)])\n\n# batch_size = 5\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 1000, batch_size = 5)\n\n# plot\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"batch_size = 5\")\n\n# batch_size = 10\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 1000, batch_size = 10)\n\n# plot\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"batch_size = 10\")\n\n# batch_size = 20\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 1000, batch_size = 10)\n\n# plot\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"batch_size = 20\")\n\n# batch_size = 50\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 1000, batch_size = 50)\n\n# plot\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"batch_size = 50\")\n\nplt.loglog()\n\nxlab = plt.xlabel(\"Number of steps\")\nylab = plt.ylabel(\"Loss\")\nlegend = plt.legend()"
  }
]