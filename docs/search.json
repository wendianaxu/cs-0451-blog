[
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "An example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/perceptron/perceptron-blogpost.html",
    "href": "posts/perceptron/perceptron-blogpost.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Source code\n\n\nI implemented the perceptron update with the following steps:\n\nModifying the feature matrix X and vector of labels y. This includes appending a column of 1s to X and turning y into a vector of -1s and 1s.\nInitializing a random weight vector w.\nIn a for-loop that breaks either when accuracy reaches 1 or when the specified max_steps is reached,\n\nPick a random data point with index i and access its features x_ and label y_i;\nCompute the predicted label y_hat using the current w;\nPerform update if y * y_hat < 0:\n\n\n\nself.w = self.w + (y_i*y_hat < 0) * (y_i * x_)\n\nWhere only when y_i * y_hat < 0 is (y_i * x_) added to the current w.\nFinally, the accuracy of the current w is computed by using w to predict labels for all data points, and then calculating the number of correct predictions compared to y. This accuracy is then appended to the history array.\n\n\n\n\n\n\nfrom perceptron import Perceptron\nimport numpy as np\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\n\nn = 100\np_features = 3\n\n# generate random data\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\n# fit perceptron to data\np1 = Perceptron()\np1.fit(X, y, max_steps = 1000)\n\n# visualizations\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize = (8, 3)) \n\n# visualize accuracy history\nax1.plot(p1.history)\nax1.set(xlabel = \"Iteration\",\nylabel = \"Accuracy\")\n\n# visualize data points and perceptron line\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nax2.set(xlabel = \"Feature 1\",\nylabel = \"Feature 2\")\n\nax2 = plt.scatter(X[:,0], X[:,1], c = y)\nax2 = draw_line(p1.w, -2, 2)\n\n\n\n\nHere, the data is linearly separable. The accuracy is able to reach 1.0 after 125+ iterations. The perceptron line generated is able to separate data with labels -1 and 1.\n\n\n\n\n# generate random data\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\n# fit perceptron to data\np2 = Perceptron()\np2.fit(X, y, max_steps = 1000)\n\n# visualizations\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize = (8, 3)) \n\n# visualize accuracy history\nax1.plot(p2.history)\nax1.set(xlabel = \"Iteration\",\nylabel = \"Accuracy\")\n\n# visualize data points and perceptron line\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nax2.set(xlabel = \"Feature 1\",\nylabel = \"Feature 2\")\n\nax2 = plt.scatter(X[:,0], X[:,1], c = y)\nax2 = draw_line(p2.w, -2, 2)\n\n\n\n\nHere, the data is not linearly separable as there are overlapping points. The accuracy fails to reach 1.0 after 1000 steps. The perceptron line does not perfectly separate the two groups of data.\n\n\n\nHere I generate random data with 6 features and feed it to the perceptron algorithm.\n\n# change the number of features\np_features = 7\n\n# generate random data\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\n# fit perceptron to data\np3 = Perceptron()\np3.fit(X, y, max_steps = 1000)\n\n# visualize accuracy history\nfig = plt.plot(p3.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nSince the accuracy has reached 1.0, the data should be linearly separable.\n\n\n\n\nThe runtime complexity of a single iteration of the perceptron update should be O(p). It would depend on the number of data points and features."
  }
]