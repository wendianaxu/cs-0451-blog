[
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CSCI 0451 Blog - Diana Xu",
    "section": "",
    "text": "Classify music by genre using neural networks in PyTorch.\n\n\n\n\n\n\nMay 14, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWhat I have learned from Dr. Timnit Gebru’s talk and visit.\n\n\n\n\n\n\nApr 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWe reflect on our learning, engagement, and achievement in the first part of the semester.\n\n\n\n\n\n\nApr 5, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nCreating a machine learning model to predict an individual characteristic based on demographic variables and performing a bias audit on the model.\n\n\n\n\n\n\nMar 29, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing linear regression.\n\n\n\n\n\n\nMar 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nClassifying Palmer penguins.\n\n\n\n\n\n\nMar 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing logistic degression with gradient descent.\n\n\n\n\n\n\nMar 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWe plan our goals for learning, engagement, and achievement over the course of the semester.\n\n\n\n\n\n\nFeb 27, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing the perceptron algorithm using numerical programming and demonstrating its use on synthetic data sets.\n\n\n\n\n\n\nFeb 22, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/perceptron/perceptron-blogpost.html",
    "href": "posts/perceptron/perceptron-blogpost.html",
    "title": "Perceptron",
    "section": "",
    "text": "Source code\n\n\nI implemented the perceptron update with the following steps:\n\nModifying the feature matrix \\(X\\) and vector of labels \\(y\\). This includes appending a column of 1s to \\(X\\) and turning \\(y\\) into a vector of -1s and 1s.\nInitializing a random weight vector \\(w\\).\nIn a for-loop that breaks either when accuracy reaches 1 or when the specified max_steps is reached,\n\nPick a random data point with index \\(i\\) and access its features \\(x_{i}\\) and label \\(y_{i}\\);\nCompute the predicted label \\(\\hat{y}\\) using the current \\(w\\);\nPerform update if \\(y\\) * \\(\\hat{y}\\) < 0:\n\n\n\nself.w = self.w + (y_i*y_hat < 0) * (y_i * x_)\n\nWhere only when y_i * y_hat < 0 is (y_i * x_) added to the current w.\nFinally, the accuracy of the current \\(w\\) is computed by using \\(w\\) to predict labels for all data points, and then calculating the number of correct predictions compared to \\(y\\). This accuracy is then appended to the history array.\n\n\n\n\n\n\nfrom perceptron import Perceptron\nimport numpy as np\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\n\nn = 100\np_features = 3\n\n# generate random data\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\n# fit perceptron to data\np1 = Perceptron()\np1.fit(X, y, max_steps = 1000)\n\n# visualizations\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize = (8, 3)) \n\n# visualize accuracy history\nax1.plot(p1.history)\nax1.set(xlabel = \"Iteration\",\nylabel = \"Accuracy\")\n\n# visualize data points and perceptron line\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nax2.set(xlabel = \"Feature 1\",\nylabel = \"Feature 2\")\n\nax2 = plt.scatter(X[:,0], X[:,1], c = y)\nax2 = draw_line(p1.w, -2, 2)\n\n\n\n\nHere, the data is linearly separable. The accuracy is able to reach 1.0 after 125+ iterations. The perceptron line generated is able to separate data with labels -1 and 1.\n\n\n\n\n# generate random data\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\n# fit perceptron to data\np2 = Perceptron()\np2.fit(X, y, max_steps = 1000)\n\n# visualizations\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize = (8, 3)) \n\n# visualize accuracy history\nax1.plot(p2.history)\nax1.set(xlabel = \"Iteration\",\nylabel = \"Accuracy\")\n\n# visualize data points and perceptron line\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nax2.set(xlabel = \"Feature 1\",\nylabel = \"Feature 2\")\n\nax2 = plt.scatter(X[:,0], X[:,1], c = y)\nax2 = draw_line(p2.w, -2, 2)\n\n\n\n\nHere, the data is not linearly separable as there are overlapping points. The accuracy fails to reach 1.0 after 1000 steps. The perceptron line does not perfectly separate the two groups of data.\n\n\n\nHere I generate random data with 6 features and feed it to the perceptron algorithm.\n\n# change the number of features\np_features = 7\n\n# generate random data\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\n# fit perceptron to data\np3 = Perceptron()\np3.fit(X, y, max_steps = 1000)\n\n# visualize accuracy history\nfig = plt.plot(p3.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nSince the accuracy has reached 1.0, the data should be linearly separable.\n\n\n\n\nThe runtime complexity of a single iteration of the perceptron update should be \\(O(p)\\). It would depend on the number of features but not the number of data points."
  },
  {
    "objectID": "posts/reflective-goal-setting/goal-setting.html",
    "href": "posts/reflective-goal-setting/goal-setting.html",
    "title": "Reflective Goal-Setting",
    "section": "",
    "text": "Wen (Diana) Xu\n\n\nThe knowledge we’ll develop in CSCI 0451 can be broadly divided into four main areas:\n\nTheory: mathematical descriptions of frameworks and algorithms.\nImplementation: effective coding and use of tools in order to implement efficient machine learning algorithms.\nExperimentation: performing experiments to assess the performance of algorithms and clearly communicating about the results.\nSocial responsibility: critical analysis of sources of bias and harm in machine learning algorithms; theoretical formulations of fairness and bias\n\nEvery student should grow toward each of these areas, but you can choose to specialize if you’d like! If there are one or two of these categories on which you’d especially like to focus, list them below. Feel free to include any details that you’d like – this can help me tailor the course content to your interests.\nI would like to focus more on experimentation and social responsibility. I am interested in the applications and implications of machine learning algorithms – how to make use of machine learning to solve problems in the real world, how to assess the performance of the algorithms in relation to the specific problem, and how we can effectively communicate machine learning to a broader audience. I also hope to look more into bias in machine learning algorithms – what causes them and how to identify and reduce sources of bias.\n\n\n\n\n\nMost blog posts will require around 5-8 hours on average to complete, plus time for revisions in response to feedback. Blog posts will most frequently involve a mix of mathematical problem-solving, coding, experimentation, and written discussion. Some blog posts will ask you to critically discuss recent readings in an essay-like format.\n\nSubmit a blog post in most weeks during the semester.\nSubmit the first draft of no more than two blog post after the “best-by” date.\nRevise at least five blog posts to the “No Revisions Suggested” level.\nGo above and beyond in at least two blog posts by performing experiments/exploring visualization choices to communicate results/discussing implications significantly beyond what is required.\n\n\n\n\nYou make a choice each day about how to show up for class: whether you’ll be prepared, whether you’ll engage with me and your peers in a constructive manner; and whether you’ll be active during lecture and discussions. We will also have a special opportunity this semester to engage with a renowned expert in machine learning, algorithmic bias, and the ethics of artificial intelligence.\nAn especially important form of course presence is the daily warmup. We’ll spend the first 10-15 minutes of most class periods on warmup activities. You’re expected to have prepared the warmup activity ahead of time (this means you’ll need to have completed the readings as well). Each time, we’ll sort into groups of 5-6 students, and one of you (randomly selected) will be responsible for presenting the activity on the whiteboard. If you’re not feeling prepared to present the activity, you can “pass” to the next person, or ask for help along the way.\n\nComplete all core readings prior to each class periods.\nComplete the optional readings that correspond to my areas of specialization.\nBe prepared for most warmup activities even on days when I am not the leader.\n“Pass” at most once when asked to lead the warmup activity for my group.\nPropose questions ahead of time for our guest speaker.\nOften work with classmates together on blog posts or other course work outside of class time.\nOften attend Peer Help or Student Hours (after preparing questions and working examples).\n\n\n\n\nTo finish off the course, you’ll complete a long-term project that showcases your interests and skills. You’ll be free to propose and pursue a topic. My expectation is that most projects will move significantly beyond the content covered in class in some way: you might implement a new algorithm, study a complex data set in depth, or conduct a series of experiments related to assessing algorithmic bias in a certain class of algorithm. You’ll be expected to complete this project in small groups (of your choosing), and update us at a few milestones along the way.\nPlease share a bit about what kind of topic might excite you, and set a few goals about how you plan to show up as a constructive team-member and co-inquirer (see the ideas for some inspiration).\n\nSubmit all project milestones (proposal, progress report, etc) on time.\nSet regular time each week to work with project partners.\nCommunicate with my group in a clear and timely manner.\nComplete all my designated work on time.\nDraft designated sections of the project report.\nRevise sections of the project report in response to feedback.\nTake the lead in creating and delivering part of the final project presentation.\nTake the lead in checking project figures for accuracy and clear labeling."
  },
  {
    "objectID": "posts/reflective-goal-setting/goal-setting.html#what-youll-learn",
    "href": "posts/reflective-goal-setting/goal-setting.html#what-youll-learn",
    "title": "Reflective Goal-Setting",
    "section": "What You’ll Learn",
    "text": "What You’ll Learn\nThe knowledge we’ll develop in CSCI 0451 can be broadly divided into four main areas:\n\nTheory: mathematical descriptions of frameworks and algorithms.\nImplementation: effective coding and use of tools in order to implement efficient machine learning algorithms.\nExperimentation: performing experiments to assess the performance of algorithms and clearly communicating about the results.\nSocial responsibility: critical analysis of sources of bias and harm in machine learning algorithms; theoretical formulations of fairness and bias\n\nEvery student should grow toward each of these areas, but you can choose to specialize if you’d like! If there are one or two of these categories on which you’d especially like to focus, list them below. Feel free to include any details that you’d like – this can help me tailor the course content to your interests.\nI would like to focus more on experimentation and social responsibility. I am interested in the applications and implications of machine learning algorithms – how to make use of machine learning to solve problems in the real world, how to assess the performance of the algorithms in relation to the specific problem, and how we can effectively communicate machine learning to a broader audience. I also hope to look more into bias in machine learning algorithms – what causes them and how to identify and reduce sources of bias."
  },
  {
    "objectID": "posts/reflective-goal-setting/goal-setting.html#what-youll-achieve",
    "href": "posts/reflective-goal-setting/goal-setting.html#what-youll-achieve",
    "title": "Reflective Goal-Setting",
    "section": "What You’ll Achieve",
    "text": "What You’ll Achieve\n\nBlog Posts\nMost blog posts will require around 5-8 hours on average to complete, plus time for revisions in response to feedback. Blog posts will most frequently involve a mix of mathematical problem-solving, coding, experimentation, and written discussion. Some blog posts will ask you to critically discuss recent readings in an essay-like format.\n\nSubmit a blog post in most weeks during the semester.\nSubmit the first draft of no more than two blog post after the “best-by” date.\nRevise at least five blog posts to the “No Revisions Suggested” level.\nGo above and beyond in at least two blog posts by performing experiments/exploring visualization choices to communicate results/discussing implications significantly beyond what is required.\n\n\n\nCourse Presence (Participation)\nYou make a choice each day about how to show up for class: whether you’ll be prepared, whether you’ll engage with me and your peers in a constructive manner; and whether you’ll be active during lecture and discussions. We will also have a special opportunity this semester to engage with a renowned expert in machine learning, algorithmic bias, and the ethics of artificial intelligence.\nAn especially important form of course presence is the daily warmup. We’ll spend the first 10-15 minutes of most class periods on warmup activities. You’re expected to have prepared the warmup activity ahead of time (this means you’ll need to have completed the readings as well). Each time, we’ll sort into groups of 5-6 students, and one of you (randomly selected) will be responsible for presenting the activity on the whiteboard. If you’re not feeling prepared to present the activity, you can “pass” to the next person, or ask for help along the way.\n\nComplete all core readings prior to each class periods.\nComplete the optional readings that correspond to my areas of specialization.\nBe prepared for most warmup activities even on days when I am not the leader.\n“Pass” at most once when asked to lead the warmup activity for my group.\nPropose questions ahead of time for our guest speaker.\nOften work with classmates together on blog posts or other course work outside of class time.\nOften attend Peer Help or Student Hours (after preparing questions and working examples).\n\n\n\nProject\nTo finish off the course, you’ll complete a long-term project that showcases your interests and skills. You’ll be free to propose and pursue a topic. My expectation is that most projects will move significantly beyond the content covered in class in some way: you might implement a new algorithm, study a complex data set in depth, or conduct a series of experiments related to assessing algorithmic bias in a certain class of algorithm. You’ll be expected to complete this project in small groups (of your choosing), and update us at a few milestones along the way.\nPlease share a bit about what kind of topic might excite you, and set a few goals about how you plan to show up as a constructive team-member and co-inquirer (see the ideas for some inspiration).\n\nSubmit all project milestones (proposal, progress report, etc) on time.\nSet regular time each week to work with project partners.\nCommunicate with my group in a clear and timely manner.\nComplete all my designated work on time.\nDraft designated sections of the project report.\nRevise sections of the project report in response to feedback.\nTake the lead in creating and delivering part of the final project presentation.\nTake the lead in checking project figures for accuracy and clear labeling."
  },
  {
    "objectID": "posts/logistic-regression/logistic-regression-blogpost.html",
    "href": "posts/logistic-regression/logistic-regression-blogpost.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "Source code\n\n\n\n\nI implemented gradient descent in a for-loop where the maximum number of iterations is the max_epoch specified in the fit() method.\nIn each iteration (epoch), \\(w\\) is first updated:\n\nself.w -= alpha*self.gradient(self.w, X_, y)\n\nwhere the gradient() function computes the gradient of logistic loss associated with the previous \\(w\\).\nThen, we compute the loss and accuracy of the current \\(w\\) and pass them to corresponding variables.\nFinally, a if-statement within the for-loop checks if the current loss is close to the previous loss and breaks the loop if they are close (i.e. the minimum loss is found).\n\n\n\nFor fit_stochastic(), an update is performed in each iteration within the for-loop where a different set of random points is picked each time:\n\nfor batch in np.array_split(order, n // batch_size + 1):\n    ...\n    # gradient step\n    self.w -= alpha*self.gradient(self.w, x_batch, y_batch)\n\nThis for-loop is nested within another for-loop that represents epochs. After updates are performed in all batches, loss and accuracy for the current epoch is calculated and the current loss is checked against the previous loss to see if the minimum is reached.\n\n\n\n\n\n\nFirst, I will test my implementation on a simple dataset, using the code from the demo on the assignment page, to make sure it works as we expect.\n\nfrom mlxtend.plotting import plot_decision_regions\n\nnp.seterr(all='ignore') \n\n# make the data\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n# fit the model\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 0.1, max_epochs = 1000)\n\n# show score\nprint(LR.score(X, y))\n\n# plot decision regions\nplot_decision_regions(X, y, clf = LR)\n\n0.93\n\n\n<AxesSubplot:xlabel='Feature 1', ylabel='Feature 2'>\n\n\n\n\n\nThe model has a score of 0.93 and classifies the data quite accurately according to the graph.\n\n\n\n\nIn this example, gradient descent with \\(\\alpha\\) = 0.1 converged before the 1000 steps. However, using the same data set with \\(\\alpha\\) increased to 10, the loss kept oscillating and never converged. This demonstrates how gradient descent would fail to converge when the learning rate is too large.\n\nfrom logistic_regression import LogisticRegression\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\n# generate data\nX, y = make_blobs(n_samples = 200, n_features = 2, centers = [(0, 0), (1, 1)])\n\n# alpha = 0.1\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 0.1, max_epochs = 1000)\n\n# plot\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient (alpha = 0.1)\")\n\n# alpha = 10\nLR.fit(X, y, alpha = 10, max_epochs = 1000)\n\n# plot\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient (alpha = 10)\")\n\nplt.loglog()\n\nxlab = plt.xlabel(\"Number of steps\")\nylab = plt.ylabel(\"Loss\")\nlegend = plt.legend() \n\n\n\n\n\n\n\nHere, we perform stochastic gradient descent with 4 different batch sizes on a data set with 10 features. The smaller the batch size, the quicker the algorithm converges.\n\n# generate data\nX, y = make_blobs(n_samples = 200, n_features = 10, centers = [(0, 0), (1, 1)])\n\n# batch_size = 5\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 1000, batch_size = 5)\n\n# plot\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"batch_size = 5\")\n\n# batch_size = 10\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 1000, batch_size = 10)\n\n# plot\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"batch_size = 10\")\n\n# batch_size = 20\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 1000, batch_size = 10)\n\n# plot\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"batch_size = 20\")\n\n# batch_size = 50\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 1000, batch_size = 50)\n\n# plot\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"batch_size = 50\")\n\nplt.loglog()\n\nxlab = plt.xlabel(\"Number of steps\")\nylab = plt.ylabel(\"Loss\")\nlegend = plt.legend()"
  },
  {
    "objectID": "posts/penguins/penguins.html",
    "href": "posts/penguins/penguins.html",
    "title": "Penguins",
    "section": "",
    "text": "In this blog post, I will be training some machine learning models on the Palmer Penguins data set. The goal is to find a model trained with three of the features in the data set that achieves 100% testing accuracy.\n\n\nHere is an overview of the Palmer Penguins data set:\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\ntrain.head()\n\n\n\n\n\n  \n    \n      \n      studyName\n      Sample Number\n      Species\n      Region\n      Island\n      Stage\n      Individual ID\n      Clutch Completion\n      Date Egg\n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Flipper Length (mm)\n      Body Mass (g)\n      Sex\n      Delta 15 N (o/oo)\n      Delta 13 C (o/oo)\n      Comments\n    \n  \n  \n    \n      0\n      PAL0708\n      27\n      Gentoo penguin (Pygoscelis papua)\n      Anvers\n      Biscoe\n      Adult, 1 Egg Stage\n      N46A1\n      Yes\n      11/29/07\n      44.5\n      14.3\n      216.0\n      4100.0\n      NaN\n      7.96621\n      -25.69327\n      NaN\n    \n    \n      1\n      PAL0708\n      22\n      Gentoo penguin (Pygoscelis papua)\n      Anvers\n      Biscoe\n      Adult, 1 Egg Stage\n      N41A2\n      Yes\n      11/27/07\n      45.1\n      14.5\n      215.0\n      5000.0\n      FEMALE\n      7.63220\n      -25.46569\n      NaN\n    \n    \n      2\n      PAL0910\n      124\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Torgersen\n      Adult, 1 Egg Stage\n      N67A2\n      Yes\n      11/16/09\n      41.4\n      18.5\n      202.0\n      3875.0\n      MALE\n      9.59462\n      -25.42621\n      NaN\n    \n    \n      3\n      PAL0910\n      146\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Dream\n      Adult, 1 Egg Stage\n      N82A2\n      Yes\n      11/16/09\n      39.0\n      18.7\n      185.0\n      3650.0\n      MALE\n      9.22033\n      -26.03442\n      NaN\n    \n    \n      4\n      PAL0708\n      24\n      Chinstrap penguin (Pygoscelis antarctica)\n      Anvers\n      Dream\n      Adult, 1 Egg Stage\n      N85A2\n      No\n      11/28/07\n      50.6\n      19.4\n      193.0\n      3800.0\n      MALE\n      9.28153\n      -24.97134\n      NaN\n    \n  \n\n\n\n\nUpon intuition, I am guessing that the species of a penguin might be related to which island they live on and their physiological measures. To test my guess, I will make some tables and figures using these variables.\n\n\n\ntrain.groupby([\"Species\", \"Island\"])[[\"Island\"]].count()\n\n\n\n\n\n  \n    \n      \n      \n      Island\n    \n    \n      Species\n      Island\n      \n    \n  \n  \n    \n      Adelie Penguin (Pygoscelis adeliae)\n      Biscoe\n      35\n    \n    \n      Dream\n      41\n    \n    \n      Torgersen\n      42\n    \n    \n      Chinstrap penguin (Pygoscelis antarctica)\n      Dream\n      56\n    \n    \n      Gentoo penguin (Pygoscelis papua)\n      Biscoe\n      101\n    \n  \n\n\n\n\nThere is a clear pattern in the island that different species of penguins inhabit. Adelie is found on all three islands, whereas Chinstrap is only found on Dream and Gentoo is only found on Biscoe.\n\n\n\n\nimport seaborn as sns\n\nsns.set_theme()\n\nsns.relplot(data = train, \n            x = \"Culmen Length (mm)\", \n            y = \"Culmen Depth (mm)\", \n            hue = \"Flipper Length (mm)\",\n            style = \"Species\")\n\n<seaborn.axisgrid.FacetGrid at 0x7fd323a6f310>\n\n\n\n\n\nThis figure shows the relationship between three of the physiological measures of a penguin (culmen depth, culmen length, flipper length) and their species.\nThe three species form nice clusters on the graph. This tells us that we can do a good job predicting them using culmen depth and length, as we can separate species using the x and y coordinates easily.\nThere is also a pattern in the color of the clusters: Gentoo has the deepest colors, followed by Chinstrap. Therefore, flipper length is also likely correlated with species.\n\n\n\n\n\n\nI start by preparing the training data using the code provided by Professor Phil:\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\nX_train\n\n\n\n\n\n  \n    \n      \n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Flipper Length (mm)\n      Body Mass (g)\n      Delta 15 N (o/oo)\n      Delta 13 C (o/oo)\n      Island_Biscoe\n      Island_Dream\n      Island_Torgersen\n      Stage_Adult, 1 Egg Stage\n      Clutch Completion_No\n      Clutch Completion_Yes\n      Sex_FEMALE\n      Sex_MALE\n    \n  \n  \n    \n      1\n      45.1\n      14.5\n      215.0\n      5000.0\n      7.63220\n      -25.46569\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n    \n    \n      2\n      41.4\n      18.5\n      202.0\n      3875.0\n      9.59462\n      -25.42621\n      0\n      0\n      1\n      1\n      0\n      1\n      0\n      1\n    \n    \n      3\n      39.0\n      18.7\n      185.0\n      3650.0\n      9.22033\n      -26.03442\n      0\n      1\n      0\n      1\n      0\n      1\n      0\n      1\n    \n    \n      4\n      50.6\n      19.4\n      193.0\n      3800.0\n      9.28153\n      -24.97134\n      0\n      1\n      0\n      1\n      1\n      0\n      0\n      1\n    \n    \n      5\n      33.1\n      16.1\n      178.0\n      2900.0\n      9.04218\n      -26.15775\n      0\n      1\n      0\n      1\n      0\n      1\n      1\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      269\n      41.1\n      17.5\n      190.0\n      3900.0\n      8.94365\n      -26.06943\n      0\n      1\n      0\n      1\n      0\n      1\n      0\n      1\n    \n    \n      270\n      45.4\n      14.6\n      211.0\n      4800.0\n      8.24515\n      -25.46782\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n    \n    \n      271\n      36.2\n      17.2\n      187.0\n      3150.0\n      9.04296\n      -26.19444\n      0\n      0\n      1\n      1\n      1\n      0\n      1\n      0\n    \n    \n      272\n      50.0\n      15.9\n      224.0\n      5350.0\n      8.20042\n      -26.39677\n      1\n      0\n      0\n      1\n      0\n      1\n      0\n      1\n    \n    \n      273\n      48.2\n      14.3\n      210.0\n      4600.0\n      7.68870\n      -25.50811\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n    \n  \n\n256 rows × 14 columns\n\n\n\n\n\n\n\n\nIn the following for-loops, I train and score all possible combinations of 1 qualitative + 2 quantitative features using cross-validation. I choose to start with the logistic regression model.\n\nfrom warnings import simplefilter\nfrom sklearn.exceptions import ConvergenceWarning\nsimplefilter(\"ignore\", category=ConvergenceWarning)\n\nfrom itertools import combinations\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nimport numpy as np\n\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island\", \"Stage\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)', \n                 'Delta 15 N (o/oo)', 'Delta 13 C (o/oo)']\n\ncv_max = 0;\nbest_features = [];\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = list(pair) + qual_cols\n    \n    # fit model\n    LR = LogisticRegression()\n    LR.fit(X_train[cols], y_train)\n    \n    # cross-validation\n    cv_scores = cross_val_score(LR, X_train[cols], y_train, cv = 5)\n    cv_mean = cv_scores.mean()\n    \n    # update best cv scores and best features list\n    if cv_mean > cv_max:\n        cv_max = cv_mean\n        best_features = cols\n    \n    \nprint(\"Best CV score = \", cv_max)\nprint(\"Best features = \", best_features)\n\nBest CV score =  0.996078431372549\nBest features =  ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n\n\nIt seems that the best cross-validation score we can get using logistic regression is 0.996.\nNext, I will try a few other machine learning models and see if we can get even higher scores. #### Decision tree\n\nfrom sklearn.tree import DecisionTreeClassifier\n\ncv_max = 0\nbest_features = []\nbest_depth = 0\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = list(pair) + qual_cols\n    \n    for d in range(2, 30):\n        # fit model\n        DTC = DecisionTreeClassifier(max_depth = d)\n        DTC.fit(X_train[cols], y_train)\n\n        # cross-validation\n        cv_scores = cross_val_score(DTC, X_train[cols], y_train, cv = 5)\n        cv_mean = cv_scores.mean()\n\n        # update top 3 cv scores and best features list\n        if cv_mean > cv_max:\n            cv_max = cv_mean\n            best_features = cols\n            best_depth = d\n    \n    \nprint(\"Best CV score = \", cv_max)\nprint(\"Best features = \", best_features)\nprint(\"Best max depth = \", best_depth)\n\nBest CV score =  0.9803921568627452\nBest features =  ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\nBest max depth =  4\n\n\nUsing the decision tree method, the best CV score we can achieve is 0.98, a little lower than with logistic regression. The best features are the same as what we got from searching through logistic regression models. The max depth that would allow the best CV score turns out to be 4.\n\n\n\n\nfrom sklearn.ensemble import RandomForestClassifier\n\ncv_max = 0;\nbest_features = [];\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = list(pair) + qual_cols\n    \n    # fit model\n    RF = RandomForestClassifier()\n    RF.fit(X_train[cols], y_train)\n    \n    # cross-validation\n    cv_scores = cross_val_score(RF, X_train[cols], y_train, cv = 5)\n    cv_mean = cv_scores.mean()\n    \n    # update top 3 cv scores and best features list\n    if cv_mean > cv_max:\n        cv_max = cv_mean\n        best_features = cols;\n    \n    \nprint(\"Best CV score = \", cv_max)\nprint(\"Best features = \", best_features)\n\nBest CV score =  0.9843137254901961\nBest features =  ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n\n\nWe again get the same set of best features from searching all possible random forest models. The best CV score is very close to what we got from the decision tree classifiers.\n\n\n\n\nfrom sklearn.svm import SVC\n\ncv_max = 0\nbest_features = []\nbest_gamma = 0\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = list(pair) + qual_cols\n    \n    for g in np.float_power(10, np.arange(-5, 5)):\n        # fit model\n        SVC_ = SVC(gamma = g)\n        SVC_.fit(X_train[cols], y_train)\n\n        # cross-validation\n        cv_scores = cross_val_score(SVC_, X_train[cols], y_train, cv = 5)\n        cv_mean = cv_scores.mean()\n\n        # update top 3 cv scores and best features list\n        if cv_mean > cv_max:\n            cv_max = cv_mean\n            best_features = cols\n            best_gamma = g\n    \n    \nprint(\"Best CV score = \", cv_max)\nprint(\"Best features = \", best_features)\nprint(\"Best gamma = \", best_gamma)\n\nBest CV score =  0.984389140271493\nBest features =  ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\nBest gamma =  0.1\n\n\nFinally, I search through all possible feature combinations and gamma values from \\(10^{-5}\\) to \\(10^5\\). The best model turns out to be trained with the same set of features as the rest of the models, with gamma = 0.1.\n\n\n\nIt seems that with any of the four models, the most predictive features are always culmen length, culmen depth, and island. I wonder if this applies to other machine learning models in general – is there always a set of “best” features to use regardless of the model choice?\nComparing the cross-validation scores of all four models, it looks like they have similar performance. Logistic regression is slightly outperforming the rest. Next, I will try applying the best candidates from each of the four model types to testing data and see if they have similar testing accuracy as well.\n\n\n\n\n\ntest_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\n\nLR = LogisticRegression()\nLR.fit(X_train[best_features], y_train)\nprint(\"Logistic regression: \", LR.score(X_test[best_features], y_test))\n      \nDTC = DecisionTreeClassifier(max_depth = best_depth)\nDTC.fit(X_train[best_features], y_train)\nprint(\"Decision tree: \", DTC.score(X_test[best_features], y_test))\n\nRF = RandomForestClassifier()\nRF.fit(X_train[best_features], y_train)\nprint(\"Random forest: \", RF.score(X_test[best_features], y_test))\n      \nSVC_ = SVC(gamma = best_gamma)\nSVC_.fit(X_train[best_features], y_train)\nprint(\"SVC: \", SVC_.score(X_test[best_features], y_test))\n\nLogistic regression:  1.0\nDecision tree:  0.9852941176470589\nRandom forest:  0.9852941176470589\nSVC:  0.9558823529411765\n\n\nWith the testing data, we reached 1.0 accuracy using logistic regression.\nThe decision tree and random forest models also had pretty good accuracy, close to what the cross-validation scores suggest.\nThe accuracy from SVC is a little lower than anticipated since the CV score from training data is higher by ~0.03.\n\n\n\n\nFinally, I will plot the decision regions of all four selected models. I will compare them and try to understand what makes logistic regression the best of them with this data set.\n\nfrom matplotlib import pyplot as plt\n\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\n\n\n\nplot_regions(LR, X_train[best_features], y_train)\nplt.show()\n\n\n\n\n\n\n\n\nplot_regions(DTC, X_train[best_features], y_train)\nplt.show()\n\n\n\n\n\n\n\n\nplot_regions(RF, X_train[best_features], y_train)\nplt.show()\n\n\n\n\n\n\n\n\nplot_regions(SVC_, X_train[best_features], y_train)\nplt.show()\n\n\n\n\n\n\n\nComparing the decision regions of different models, it seems that the errors of the last three models almost exclusively come from the separation between the two species on the second island. There are a few green points right next to the blue ones, which were only correctly classified by logistic regression.\nI can see how this creates a problem for decision tree and random forest – since they only create separation lines at two directions (along the x axis or y axis), it would be hard for them to be precise where the two clusters are extremely close to each other.\nFor the support vector classifier, however, I am curious about what makes it the least accurate, at least for this data set. Is this by nature of some characteristics of this model or just by chance?"
  },
  {
    "objectID": "posts/linear-regression/linear-regression-blogpost.html",
    "href": "posts/linear-regression/linear-regression-blogpost.html",
    "title": "Linear Regression",
    "section": "",
    "text": "Source code\n\n\n\n\nThe first way to implement linear regression is by using the analytical formula that gives us the \\(\\hat{w}\\) to minimize \\(L(w)\\):\n\nself.w = np.linalg.inv(X_.T@X_)@X_.T@y\n\n\n\n\nWe could also use the regular gradient descent to compute \\(\\hat{w}\\). To reduce time complexity, we precompute P = X_.T@X and q = X.T@y, and then pass them to gradient() in the for-loop where we update the gradient:\n\nself.w -= 2*alpha*self.gradient(P, q, self.w)\n\n\n\n\n\n\n\nIn this experiment, I explore how the training score and validation score change as the number of features increases.\n\nfrom linear_regression import LinearRegression\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\n# functions for data generation\ndef pad(X):\n    return np.append(X, np.ones((X.shape[0], 1)), 1)\n\ndef LR_data(n_train = 100, n_val = 100, p_features = 1, noise = .1, w = None):\n    if w is None: \n        w = np.random.rand(p_features + 1) + .2\n    \n    X_train = np.random.rand(n_train, p_features)\n    y_train = pad(X_train)@w + noise*np.random.randn(n_train)\n\n    X_val = np.random.rand(n_val, p_features)\n    y_val = pad(X_val)@w + noise*np.random.randn(n_val)\n    \n    return X_train, y_train, X_val, y_val\n\nI increase the number of features used in the model from 1 to n_train-1 and plot the change in training and validation scores.\n\n# generate data\nn_train = 100\nn_val = 100\nnoise = 0.2\ntrain_scores = []\nval_scores = []\n\n# increase p_features from 1 to n_train-1 and calculate training and validation scores for each\nfor p_features in np.arange(1, n_train): \n    # create data\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n    LR = LinearRegression()\n    LR.fit(X_train, y_train)\n    train_scores.append(LR.score(X_train, y_train))\n    val_scores.append(LR.score(X_val, y_val))\n\n\n# plot\nplt.plot(np.arange(1, n_train), train_scores, label = \"Training scores\")\n\n# plot\nplt.plot(np.arange(1, n_train), val_scores, label = \"Validation scores\")\n\nxlab = plt.xlabel(\"Number of features\")\nylab = plt.ylabel(\"Score\")\nlegend = plt.legend() \n\n\n\n\nWe can observe from the chart that the training score increased all the way to 1.0 as the number of features increases. The validation score, however, has been fluctuating and forms a slightly downward trend, and dramatically decreased to almost 0 when the number of features reached ~99.\nThis is a demonstration of overfitting. With too many features, the model becomes increasingly accurate in describing the trend in the training data, but at the same time takes into account more noise from the training data that doesn’t generate to the rest of the data. As a result, validation scores decrease.\n\n\n\nTo fix overfitting, I experiment with LASSO regularization:\n\nfrom sklearn.linear_model import Lasso\n\n# generate data\nn_train = 100\nn_val = 100\nnoise = 0.2\ntrain_scores = []\nval_scores = []\n\n# increase p_features from 1 to n_train-1 and calculate training and validation scores for each\nfor p_features in np.arange(1, n_train + 10): \n    # create data\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n    L = Lasso(alpha = 0.001)\n    L.fit(X_train, y_train)\n    train_scores.append(L.score(X_train, y_train))\n    val_scores.append(L.score(X_val, y_val))\n\n\n# plot\nplt.plot(np.arange(1, n_train + 10), train_scores, label = \"Training scores\")\nplt.plot(np.arange(1, n_train + 10), val_scores, label = \"Validation scores\")\n\nxlab = plt.xlabel(\"Number of features\")\nylab = plt.ylabel(\"Score\")\nlegend = plt.legend() \n\n\n\n\nUsing LASSO regularization, the validation scores still drops as the number of features increases, but there is no dramatical decrease as the number of features approaches, or even exceeds the number of data points. This is because LASSO is able to force entries of the weight vector to zero, which can help eliminate the effect of features that act as noise to the model.\n\n\n\n\nIn this section, I train my linear regression model to a bikeshare data set.\n\n# import data\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nbikeshare = pd.read_csv(\"https://philchodrow.github.io/PIC16A/datasets/Bike-Sharing-Dataset/day.csv\")\n\nbikeshare.head()\n\n\n\n\n\n  \n    \n      \n      instant\n      dteday\n      season\n      yr\n      mnth\n      holiday\n      weekday\n      workingday\n      weathersit\n      temp\n      atemp\n      hum\n      windspeed\n      casual\n      registered\n      cnt\n    \n  \n  \n    \n      0\n      1\n      2011-01-01\n      1\n      0\n      1\n      0\n      6\n      0\n      2\n      0.344167\n      0.363625\n      0.805833\n      0.160446\n      331\n      654\n      985\n    \n    \n      1\n      2\n      2011-01-02\n      1\n      0\n      1\n      0\n      0\n      0\n      2\n      0.363478\n      0.353739\n      0.696087\n      0.248539\n      131\n      670\n      801\n    \n    \n      2\n      3\n      2011-01-03\n      1\n      0\n      1\n      0\n      1\n      1\n      1\n      0.196364\n      0.189405\n      0.437273\n      0.248309\n      120\n      1229\n      1349\n    \n    \n      3\n      4\n      2011-01-04\n      1\n      0\n      1\n      0\n      2\n      1\n      1\n      0.200000\n      0.212122\n      0.590435\n      0.160296\n      108\n      1454\n      1562\n    \n    \n      4\n      5\n      2011-01-05\n      1\n      0\n      1\n      0\n      3\n      1\n      1\n      0.226957\n      0.229270\n      0.436957\n      0.186900\n      82\n      1518\n      1600\n    \n  \n\n\n\n\n\n# plot the number of casual users over time\nfig, ax = plt.subplots(1, figsize = (7, 3))\nax.plot(pd.to_datetime(bikeshare['dteday']), bikeshare['casual'])\nax.set(xlabel = \"Day\", ylabel = \"# of casual users\")\nl = plt.tight_layout()\n\n\n\n\n\n# transforming data\ncols = [\"casual\", \n        \"mnth\", \n        \"weathersit\", \n        \"workingday\",\n        \"yr\",\n        \"temp\", \n        \"hum\", \n        \"windspeed\",\n        \"holiday\"]\n\nbikeshare = bikeshare[cols]\n\nbikeshare = pd.get_dummies(bikeshare, columns = ['mnth'], drop_first = \"if_binary\")\nbikeshare\n\n\n\n\n\n  \n    \n      \n      casual\n      weathersit\n      workingday\n      yr\n      temp\n      hum\n      windspeed\n      holiday\n      mnth_2\n      mnth_3\n      mnth_4\n      mnth_5\n      mnth_6\n      mnth_7\n      mnth_8\n      mnth_9\n      mnth_10\n      mnth_11\n      mnth_12\n    \n  \n  \n    \n      0\n      331\n      2\n      0\n      0\n      0.344167\n      0.805833\n      0.160446\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      1\n      131\n      2\n      0\n      0\n      0.363478\n      0.696087\n      0.248539\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      2\n      120\n      1\n      1\n      0\n      0.196364\n      0.437273\n      0.248309\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      3\n      108\n      1\n      1\n      0\n      0.200000\n      0.590435\n      0.160296\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      4\n      82\n      1\n      1\n      0\n      0.226957\n      0.436957\n      0.186900\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      726\n      247\n      2\n      1\n      1\n      0.254167\n      0.652917\n      0.350133\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n    \n    \n      727\n      644\n      2\n      1\n      1\n      0.253333\n      0.590000\n      0.155471\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n    \n    \n      728\n      159\n      2\n      0\n      1\n      0.253333\n      0.752917\n      0.124383\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n    \n    \n      729\n      364\n      1\n      0\n      1\n      0.255833\n      0.483333\n      0.350754\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n    \n    \n      730\n      439\n      2\n      1\n      1\n      0.215833\n      0.577500\n      0.154846\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n    \n  \n\n731 rows × 19 columns\n\n\n\n\n# train-test split\ntrain, test = train_test_split(bikeshare, test_size = .2, shuffle = False)\n\nX_train = train.drop([\"casual\"], axis = 1)\ny_train = train[\"casual\"]\n\nX_test = test.drop([\"casual\"], axis = 1)\ny_test = test[\"casual\"]\n\n\n# fit and score the model\nLR = LinearRegression()\nLR.fit(X_train, y_train)\nLR.score(X_train, y_train)\n\n0.7318355359284503\n\n\nThe model has a score of 0.73.\n\n# compute predictions and visualize in comparison to actual test data\ny_hat = LR.predict(X_test)\n\nplt.plot(np.arange(len(y_hat)), y_hat, label = \"Predictions\")\nplt.plot(np.arange(len(y_hat)), y_test, label = \"Test data\")\n         \nxlab = plt.xlabel(\"Day\")\nylab = plt.ylabel(\"Number of casual users\")\nlegend = plt.legend() \n\n\n\n\nIt seems that the model does a good job in predicting the general trend in the data – the overall decreasing number of users and the timing of peaks. However, it tends to underestimate the minimum and maximum values.\nFinally, I will look into the weight vector and see what it reveals about people’s preference of when to use bikeshare.\n\n# compare weight vector to list of features\nfeature_weights = pd.DataFrame(LR.w[:-1], X_train.columns)\nfeature_weights\n\n\n\n\n\n  \n    \n      \n      0\n    \n  \n  \n    \n      weathersit\n      -108.371136\n    \n    \n      workingday\n      -791.690549\n    \n    \n      yr\n      280.586927\n    \n    \n      temp\n      1498.715113\n    \n    \n      hum\n      -490.100340\n    \n    \n      windspeed\n      -1242.800381\n    \n    \n      holiday\n      -235.879349\n    \n    \n      mnth_2\n      -3.354397\n    \n    \n      mnth_3\n      369.271956\n    \n    \n      mnth_4\n      518.408753\n    \n    \n      mnth_5\n      537.301886\n    \n    \n      mnth_6\n      360.807998\n    \n    \n      mnth_7\n      228.881481\n    \n    \n      mnth_8\n      241.316412\n    \n    \n      mnth_9\n      371.503854\n    \n    \n      mnth_10\n      437.600848\n    \n    \n      mnth_11\n      252.433004\n    \n    \n      mnth_12\n      90.821460\n    \n  \n\n\n\n\nWe can observe a couple of interesting trends from the magnitude and direction of the weights: 1. People use bikeshare more when the weather is nicer – especially when it is warmer and there is less wind. It is also interesting, though, that the weathersit variable is slightly negatively correlated with the number of users. I wonder what exactly this variable is measuring. 2. People use bikeshare the most in April and May. This makes sense because weather is the best during this time of the year, while summer is too hot and winter is too cold. 3. People use bikeshare more on weekends but less on holidays."
  },
  {
    "objectID": "posts/auditing-allocative-bias/auditing-allocative-bias-blogpost.html",
    "href": "posts/auditing-allocative-bias/auditing-allocative-bias-blogpost.html",
    "title": "Auditing Allocative Bias",
    "section": "",
    "text": "In this blogpost, I will train a model to predict income based on demographic variables excluding sex, and perform audit for gender bias on the model.\n\n\nThe data I choose to look into is the PUMS data in for Illinois in 2019.\n\nfrom folktables import ACSDataSource, ACSEmployment, BasicProblem, adult_filter\nimport numpy as np\n\nSTATE = \"IL\"\n\ndata_source = ACSDataSource(survey_year='2019', \n                            horizon='1-Year', \n                            survey='person')\n\nacs_data = data_source.get_data(states=[STATE], download=True)\npossible_features=['AGEP', 'SCHL', 'MAR', 'DIS', 'ESP', 'CIT', 'MIG', 'MIL', 'ANC', 'NATIVITY', 'DEAR', 'DEYE', 'DREM', 'SEX', 'RAC1P', 'ESR', \"PINCP\"]\nacs_data[possible_features].head()\n\n\n\n\n\n  \n    \n      \n      AGEP\n      SCHL\n      MAR\n      DIS\n      ESP\n      CIT\n      MIG\n      MIL\n      ANC\n      NATIVITY\n      DEAR\n      DEYE\n      DREM\n      SEX\n      RAC1P\n      ESR\n      PINCP\n    \n  \n  \n    \n      0\n      19\n      19.0\n      5\n      2\n      NaN\n      1\n      1.0\n      4.0\n      1\n      1\n      2\n      2\n      2.0\n      2\n      1\n      6.0\n      0.0\n    \n    \n      1\n      18\n      18.0\n      5\n      2\n      NaN\n      1\n      3.0\n      4.0\n      4\n      1\n      2\n      2\n      2.0\n      2\n      1\n      6.0\n      800.0\n    \n    \n      2\n      22\n      16.0\n      5\n      2\n      NaN\n      1\n      3.0\n      1.0\n      2\n      1\n      2\n      2\n      2.0\n      1\n      1\n      4.0\n      35000.0\n    \n    \n      3\n      64\n      16.0\n      5\n      2\n      NaN\n      1\n      1.0\n      4.0\n      1\n      1\n      2\n      2\n      2.0\n      2\n      1\n      6.0\n      0.0\n    \n    \n      4\n      53\n      16.0\n      5\n      2\n      NaN\n      5\n      1.0\n      4.0\n      1\n      2\n      2\n      2\n      2.0\n      1\n      1\n      1.0\n      31200.0\n    \n  \n\n\n\n\nI would like to predict whether an individual’s income is over $50K based on all the variables in possible_features except for sex. I will create a feature matrix, a label vector, and a group label vector using the relevant variables, and split them into training data and test data.\n\nfeatures_to_use = [f for f in possible_features if f not in [\"PINCP\", \"SEX\"]]\n\nIncomeProblem = BasicProblem(\n    features=features_to_use,\n    target='PINCP',\n    target_transform=lambda x: (x > 50000) * 1, # transform income variable into binary category\n    group='SEX',\n    preprocess=lambda x: x,\n    postprocess=lambda x: np.nan_to_num(x, -1),\n)\n\nfeatures, label, group = IncomeProblem.df_to_numpy(acs_data)\n\n# train-test split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n    features, label, group, test_size=0.2, random_state=0)\n\n\n\nBefore training any models, I will first explore the basic descriptives of the dataset.\n\nimport pandas as pd\ndf = pd.DataFrame(X_train, columns = features_to_use)\ndf[\"group\"] = group_train\ndf[\"label\"] = y_train\n\n\nsize = len(df)\nsize\n\n100005\n\n\nThere are 100005 individuals in the dataset.\n\n(df[\"label\"] == 1).sum()/size\n\n0.25900704964751764\n\n\nAround 25.9% of these individuals earn over $50K.\n\ndf.groupby(\"group\").size()\n\ngroup\n1    49135\n2    50870\ndtype: int64\n\n\n49,135 of the individuals in the dataset are male and 50,870 are female.\n\ndf.groupby(\"group\")[\"label\"].mean().reset_index()\n\n\n\n\n\n  \n    \n      \n      group\n      label\n    \n  \n  \n    \n      0\n      1\n      0.326712\n    \n    \n      1\n      2\n      0.193611\n    \n  \n\n\n\n\nAround 32.7% of male individuals in the dataset has income over 50k dollars, but only 19.4% of women has income over $50k.\n\nimport seaborn as sns\nproportions = df.groupby([\"group\", \"RAC1P\"])[\"label\"].mean().reset_index()\nax = sns.barplot(data = proportions, x = \"RAC1P\", y = \"label\", hue = \"group\")\n\n\n\n\nIt seems that for almost all race groups, the proportion of men with income over $50k is greater than that of women. The only exception is African Americans, for whom this proportion doesn’t differ very much across sex.\n\n\n\n\nI choose to use a decision tree classifier for my prediction. I will use cross-validation to find out the best max depth for the model.\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_val_score\n\ncv_max = 0\nbest_depth = 0\ncvs = []\n\nfor d in range(2, 30):\n    # fit model\n    DTC = DecisionTreeClassifier(max_depth = d)\n    DTC.fit(X_train, y_train)\n\n    # cross-validation\n    cv_scores = cross_val_score(DTC, X_train, y_train, cv = 5)\n    cv_mean = cv_scores.mean()\n    cvs.append(cv_mean)\n\n    # update best cv score and best degree\n    if cv_mean > cv_max:\n        cv_max = cv_mean\n        best_depth = d\n\nprint(\"Best cv score = \", cv_max)\nprint(\"Best max depth = \", best_depth)\n\n# draw a line chart for cv scores at varying degrees\nimport matplotlib.pyplot as plt\nfig = plt.plot(range(2, 30), cvs)\nxlab = plt.xlabel(\"Max depth\")\nylab = plt.ylabel(\"CV score\")\n\nBest cv score =  0.8124193790310483\nBest max depth =  8\n\n\n\n\n\nAs shown by the graph, the best performance is reached at a max depth of 8.\n\n\n\nNext, I will use the model on the test data and perform an audit for bias.\n\n# train model and predict using test data\nDTC = DecisionTreeClassifier(max_depth = best_depth)\nDTC.fit(X_train, y_train)\ny_pred = DTC.predict(X_test)\n\n\n\nFirst, we will take a look at the measures for the whole dataset.\n\nfrom sklearn.metrics import confusion_matrix\n\n# get confusion matrix values\ncm = confusion_matrix(y_test, y_pred)\nTP, FP, FN, TN = cm[0][0], cm[0][1], cm[1][0], cm[1][1]\n\ntest_score = DTC.score(X_test, y_test)\nPPV = TP / (TP + FP)\nFNR = FN / (TP + FN)\nFPR = FP / (TN + FP)\n\nprint(\"Overall accuracy = \", test_score)\nprint(\"PPV = \", PPV)\nprint(\"FNR = \", FNR)\nprint(\"FPR = \", FPR)\n\nOverall accuracy =  0.8154547636189104\nPPV =  0.9172221027735971\nFNR =  0.15264673751117291\nFPR =  0.31661184210526316\n\n\nThe overall accuracy and PPV show that the model is quite accurate. The FPR is one time higher than the FPR, suggesting that this model is more likely to wrongly predict that someone earns over $50K when they does not in reality, compared to wrongly predict that someone earns less than 50K.\n\n\n\nThen we will compute the same measures by each group (i.e., male and female) and see if there are any differences.\n\nmale = group_test == 1\nfemale = group_test == 2\n\ncorrect = (y_test == y_pred)\n\naccuracy_male = correct[male].mean()\naccuracy_female = correct[female].mean()\n\naccuracy_male, accuracy_female\n\n(0.7848350566223535, 0.8445692883895131)\n\n\nThe accuracy for male and female is similar, and a little higher for female.\n\n#X_test_df = pd.DataFrame(X_test, columns = [features_to_use])\ncm_male = confusion_matrix(y_test[male],y_pred[male])\ncm_female = confusion_matrix(y_test[female],y_pred[female])\n\n# male\nTP, FP, FN, TN = cm_male[0][0], cm_male[0][1], cm_male[1][0], cm_male[1][1]\nPPV_male = TP / (TP + FP)\nFNR_male = FN / (TP + FN)\nFPR_male = FP / (TN + FP)\n\n# female\nTP, FP, FN, TN = cm_female[0][0], cm_female[0][1], cm_female[1][0], cm_female[1][1]\nPPV_female = TP / (TP + FP)\nFNR_female = FN / (TP + FN)\nFPR_female = FP / (TN + FP)\n\nPPV_male, PPV_female\n\n(0.9251758428328887, 0.9108901332303534)\n\n\nThe PPV for male and female is similar too.\n\nFNR_male, FNR_female\n\n(0.20811708532281503, 0.10177075399847677)\n\n\nThe false negative rate is higher for male than for female. This shows that if a man earns more than $50K, the model is more likely to make a mistake and say he earns less, compared to the same situation for a woman.\n\nFPR_male, FPR_female\n\n(0.24177115987460815, 0.3992214532871972)\n\n\nThe false positive rate is higher for female than for male. This means that the model is more likely to predict that a woman earns more than $50K when she actually earns less.\n\n\n\nFinally, we will calculate the three different bias measures discussed by Chouldechova (2017).\nThe first measure is calibration. Good calibration means that a man and a woman who received the same score (in this case, either 0 or 1) have the same probability of earning over $50K.\n\n# Calibration\nprint(\"% positive for male predicted positive = \", y_test[male][y_pred[male] == 1].mean())\nprint(\"% positive for female predicted positive = \", y_test[female][y_pred[female] == 1].mean())\nprint(\"% positive for male predicted negative = \", y_test[male][y_pred[male] == 0].mean())\nprint(\"% positive for female predicted negative = \", y_test[female][y_pred[female] == 0].mean())\n\n% positive for male predicted positive =  0.7582288401253918\n% positive for female predicted positive =  0.6007785467128027\n% positive for male predicted negative =  0.20811708532281503\n% positive for female predicted negative =  0.10177075399847677\n\n\nThe probabilities are around 0.1-0.15 higher for men than for women. It looks like the model is roughly calibrated, but still, there is a slight difference in what the same score means for the two groups.\nNext we will look at error rate balance. According to Chouldechova (2017), a score satisfies error rate balance at a threshold \\(S_{HR}\\) if the FPR and FNR are both equal across groups. Because our model only have binary scores, this would just mean that the FPR and FNR are equal across gender.\n\nFPR_male, FPR_female\n\n(0.24177115987460815, 0.3992214532871972)\n\n\n\nFNR_male, FNR_female\n\n(0.20811708532281503, 0.10177075399847677)\n\n\nFrom the FPR and FNR that we calculated in the last section, we can see that the model doesn’t have error rate balance. The FPR is higher for women and the FNR is higher for men.\nOur last measure is statistical parity. This is satisfied if the proportion of individuals classified as positive is the same for each group.\n\nTP, FP, FN, TN = cm_male[0][0], cm_male[0][1], cm_male[1][0], cm_male[1][1]\nPR_male = (TP+FP) / (TP+FP+TN+FN)\nTP, FP, FN, TN = cm_female[0][0], cm_female[0][1], cm_female[1][0], cm_female[1][1]\nPR_female = (TP+FP) / (TP+FP+TN+FN)\n\nPR_male, PR_female\n\n(0.6766781552601345, 0.8082084893882646)\n\n\nThis model doesn’t have statistical parity – the proportion of women classified as earning over $50K is higher than that of men.\n\n\n\n\nWith a system like what I have built in this blogpost, companies will be able to identify high-income individuals based on a set of demographics data. This would especially benefit companies that sell high-end products by enabling them to locate their target group of consumers.\nThe main takeaway from my bias audit is that this model tends to overestimate the income of a woman and underestimate the income of a man. If the model is deployed in a large-scale setting, it would mean that more women will be incorrectly identified as marketing targets for certain products. Moreover, in the case that companies make use of predicted income for discriminatory pricing across income groups, more women will be harmed by this act.\nI think that this model exhibits bias in all forms that I have measured. In terms of calibration, men who received positive labels were more likely to be positive than women who received positive labels. Looking at the error rates, we also see that false positive rates are higher for women and false negative rates are higher for men. For statistical parity, we see a similar pattern where higher proportions of women than men were classified as positive. All of these measures reflect more or less the same theme.\nI find it interesting and unsettling that just like the recidivism prediction model that we discussed in class, the bias of this model is also potentially an artifact of inequalities present in the real world. Given a man and a woman who are similar in features other than sex, the model would predict that they have similar levels of income. In reality, however, women with similar qualifications as men are often paid less as a result of structural discrimination that is not captured by the dataset. This explains why the model tends to classify women as earning more than they do in reality.\nAnother problem beyond bias is that whether it is ethical at all to use any model that predicts income. Even if the model is accurate and relatively unbiased, it is hard to see how such a model would benefit the individuals upon which it is used. The government may need income information for social welfare programs, but it doesn’t need to use a model to acquire such information. As for companies, I would imagine that the main motivation for predicting income is to maximize profit by discriminately charging income groups."
  },
  {
    "objectID": "posts/mid-course/mid-course.html",
    "href": "posts/mid-course/mid-course.html",
    "title": "Mid-Course Reflection",
    "section": "",
    "text": "Wen (Diana) Xu\n\n\nIn this section I’ll ask you to fill in some data. You don’t have to give precise numbers – approximate, conversational responses are fine. For example, when I ask “how often have you attended class,” good answers include “almost always,” “I’ve missed three times,” “about 75% of the time,” “not as often as I want,” etc.\n\n\n\nHow often have you attended class? (e.g. “almost always,” “I missed three times,” etc.) I attended all classes.\nHow often have you taken notes on the core readings ahead of the class period? About 2/3 of the times.\nHow often have you been prepared to present the daily warm-up exercise to your team, even if you weren’t actually called? I have been prepared for all warmup activities.\nHow many times have you actually presented the daily warm-up to your team? I don’t remember exactly, but I presented every time when I was picked.\nHow many times have you asked your team for help while presenting the daily warm-up? About 2-3 times.\nHow often have you learned something new from a teammate’s presentation of the daily warm-up? I learned something new for most of the times.\nHow often have you helped a teammate during the daily warm-up presentation? One or two times.\n\n\n\n\n\nHow often have you attended Student Hours or Peer Help? I have attended student hours or peer help more often at the beginning of the semester, about once a week. I haven’t been attending for the past few weeks, though.\nHow often have you asked for or received help from your fellow students? A few times a week.\nHave you been regularly participating in a study group outside class? Yes, about once a week.\nHow often have you posted questions or answers in Slack? I haven’t posted questions or answers in Slack.\n\n\n\n\n\nHow many blog posts have you submitted? I have submitted four blogposts.\nHow many of your submitted blog posts are at each of the following feedback stages?\n\nNo revisions suggested: 4\nRevisions useful: 0\nRevisions encouraged: 0\nIncomplete: 0\n\nRoughly how many hours per week have you spent on this course outside of class? About 6-10 hours per week.\n\n\n\n\n\nAt the beginning of the course, you may have expressed an interest in focusing a little extra on one or two of the following four categories:\n\nTheory: mathematical descriptions of frameworks and algorithms.\nImplementation: effective coding and use of tools in order to implement efficient machine learning algorithms.\nExperimentation: performing experiments to assess the performance of algorithms and clearly communicating about the results.\nSocial responsibility: critical analysis of sources of bias and harm in machine learning algorithms; theoretical formulations of fairness and bias\n\nDid you choose to focus on any of these categories? If so, what have you done in order to pursue your interest?\nI have chosen to focus on experimentation and social responsibility. For experimentation, I have put more effort into experimentation sections in the blogposts, such as experimenting the linear regression model on the bikeshare dataset, and the penguin classification blogpost. I have thought carefully about the experimentation results and written down my interpretations or raised additional questions regarding the process. I also tried to create my writings and graphs in a way that communicates insights clearly to potential readers.\nFor social responsibility, I have read and thought about the readings on algorithmic bias carefully, and am applying what I have learned from these readings and our classes to a real-world dataset in the auditing allocative bias blogpost.\n\n\n\nFor each of the categories below, replace the “[your response here]” cell with 1-2 paragraphs in which you reflect on the following questions:\n\nIn what ways are you on track to meet your goals from the beginning of the course? Be specific: explain what the goal is and what you are doing in order to meet it.\nIn what ways are you not on track to meet your goals from the beginning of the course? Be specific: explain what the goal is and what gap you see between where you are and your goal.\nIf there’s any context you want to share about how you are faring relative to your goals, please do!\n\n\n\nI am on track for the goals of submitting a blog post in most weeks during the semester, and submitting the first draft of no more than two blog post after the “best-by” date. I submitted a blog post each week except for Week 5 and Week 6, and all of them before the “best-by” date. I am also working on the blog post for auditing bias. I am also in good progress for the goal of having at least five blog posts at the “No revisions suggested” level because I currently have four.\nI am a little behind on my last goal of “Go above and beyond in at least two blog posts by performing experiments/exploring visualization choices to communicate results/discussing implications significantly beyond what is required.” I have gone beyond in the penguins blog post by discussing some questions I had about the different performance of different models, but I wished I did more, especially for coming up with my own experimentations and trying new visualization approaches to communicate my results. I think that I haven’t been able to put enough time and effort on writing a blog post significantly beyond the requirements because I have been rushing to get them done before the best-by dates.\n\n\n\nI am on track for my goals on preparation before class. For example, I did complete all core and optional readings before class, and prepare for all warmup activities. I have also been working often with classmates after class as I stated in my goals.\nFor my last goal of often attending student hours or peer help, I am behind since I haven’t attended student hours or peer help sessions recently. I felt that I didn’t have emergent questions that I needed to discuss in these sessions, and sometimes they did not fit with my schedule.\n\n\n\nWe have just started the project, but I think we are on track for submitting our proposal on time.\n\n\n\nIs there anything else that you want to share with me about what you have learned, how you have participated, or what you have achieved in CSCI 0451?\n\n\n\nFrom your experience in CSCI 0451 and your other classes this semester, you may feel moved to make modifications to your goals. Are they still feasible? Too ambitious? Not ambitious enough? If you would like to revise any of your goals from your reflective goal-setting, you can do so below. For each goal you want to modify:\n\nClearly state what the goal was.\nClearly state how you’ve done on that goal so far.\nClearly state your proposed revised goal for the remainder of the course.\n\nI would like to revise my goal of often attending student hours and peer help. I have attended student hours or peer help regularly during the first few weeks of the semester, but haven’t attended much recently. I wanted to modify this goal to “Attend student hours and peer help when I have questions that are not resolved by self effort and discussion with classmates,” because I feel that it is often more efficient for me to find time on my own or talk to classmates to work on questions.\n\n\n\n\nTake 15 minutes to look back on your responses in each of the sections above. Then, state the letter grade that you feel reflects your learning, participation, and achievement in CSCI 0451 so far. Here are some soundbytes to help guide your thinking:\n\n\nAn A sounds like:\n\n“I am very proud of my time in this course.”\n“I have grown significantly in multiple ways that matter to me.”\n“I am ready to take the theory, techniques, and ideas of this course into my future classes, projects, hobbies, or career.”\n\nA B sounds like:\n\n“I had some opportunities to learn more, overall I feel good about my time in this course.”\n“I am able to explain some new things or achieve new tasks.”\n“I can see a few ideas from this course that will be relevant for my future classes, projects, hobbies, or career.”\n\nA C sounds like:\n\n“I often made a good effort, but I missed many opportunities to get more out of my time in this course.”\n“I might be able to complete some new tasks related to the course content, but only with significant further guidance.”\n“I don’t see any ways to take the contents of this course into my future classes, projects, hobbies, or career.”\n\nYou might find that some of these soundbytes resonate and other’s don’t! Take some time, see what feels right, and don’t be afraid to celebrate your achievements.\n\nUpon reflection, I feel that my learning, participation, and achievement in CSCI 0451 (so far) are best reflected by a grade of A-\n\n\n\n\n\nYou may feel disappointed by your reflection. Sometimes we don’t achieve all our goals – it happens and it’s normal! If you are feeling disappointed by how you’ve learned, participated, or achieved in CSCI 0451, then feel free to write something about that below. Feel free to just write your feelings. If you have ideas for how to move forward, include those too! We’ll talk."
  },
  {
    "objectID": "posts/timnit-gebru/timnit-gebru.html",
    "href": "posts/timnit-gebru/timnit-gebru.html",
    "title": "Learning from Timnit Gebru",
    "section": "",
    "text": "Part 2: Reflection Dr. Gebru’s Talk\nDr. Gebru’s talk presents a critical perspective on the development of artificial general intelligence (AGI) by large corporations today, who are contributing to a hype for an omnipotent model and racing to build it. Dr. Gebru introduced the concept of “TESCREALists,” groups of people who believe that AGI can radically enhance human capabilities and thoroughly transform the society. Members of these groups include influential and wealthy people who provide tremendous resources for the development of AGI.\nDr. Gebru argued that the TESCREALists have close connections with the idea of eugenics. They represent the second wave of eugenics, which supports using technologies such as genetic engineering to select for “desirable” traits in humans. These traits often refer to intelligence, defined and measured by a restricted mainstream standard. Dr. Gebru argued that this is a dangerous direction to take because it can further marginalize groups that do not fit in the standards created by those in power.\nDr. Gebru also discussed a common narrative that TESCREALists talk about around AGI – how AGI has such great power that it will lead to either a utopia or an apocalypse depending on how it is handled. They seem to attempt to convince people that AGI will be used properly to improve public welfare only when they are in charge of creating and managing it. Dr. Gebru questioned this narrative from a few perspectives. First, the idea that an all-knowing model will be able to solve any problem and benefit everyone in the world does not sound realistic. The argument that AGI will make the world a more equitable place, in particular, is not convincing given how companies are exploiting labor in underdeveloped regions to keep large AI models running. Another concern that Dr. Gebru brought up was that the apocalyptic framing of AGI can allow these companies to shift people’s attention from current problems caused by their practices, such as data theft and labor exploitation, that are harming people right now, to a potential apocalypse that might happen in the future. Finally, Dr. Gebru argued that a much more rational strategy is to develop smaller models that specialize on specific fields, rather than putting all resources into one large model controlled by a few powerful organizations. This will allow people to benefit from AI that speak to their specific needs and do the job better because they are developed by people who are more specialized in their field.\nDr. Gebru’s talk helped me to think about the potential consequences of creating such a powerful technology of AGI from a perspective that I haven’t considered before. I very much agree with her call for drawing more attention to the unethical practices behind current AI models than the potential existential crisis that a powerful AGI may pose to humankind. I feel that a large part of public discussion around AI has been focusing on the latter and fewer people are aware of how many AI systems that are in use today are already harming marginalized groups. I think that this is the most important idea that people should learn from Dr. Gebru’s talk because everyone should be more aware of the impacts of AI systems that many of us use frequently. I also agree that centralized control of large AI systems is a dangerous direction. What I had a harder time following was the arguments on the connections between the TESCREAL communities and eugenics. I think that more evidence is needed to justify equating all people in these groups to the type of “eugenists” that she was accusing of. But I do agree that many of the beliefs held by these groups are worth questioning and cautioning.\n\n\nPart 3: Reflection on the Process\nAfter the experience of interacting with Dr. Gebru and her work, I feel that I have developed a much deeper understanding of the ethical implications of AI and have come to realize both the importance of and challenges in grappling with these issues. I find it inspiring to engage in critical perspectives of the ethics of AI, and I think that I will be able to apply these perspectives to thinking about the implications of other powerful technologies. What I am frustrated about is that these issues seem challenging to resolve because of power dynamics in the field of AI. I also feel that the debate around this topic is very polarized and both sides have quite extreme views. This makes me worried that it may deviate from a discussion where both sides actually listen to each others’ arguments, and make it harder to generate insights for improving the field. Overall, however, I feel empowered that we get to hear voices from scientists like Dr. Gebru to stand up for marginalized groups and push for fairer science practices."
  },
  {
    "objectID": "posts/music-genre/music_genre.html",
    "href": "posts/music-genre/music_genre.html",
    "title": "Deep Music Genre Classification",
    "section": "",
    "text": "In this blog post, I will use PyTorch to train neural networks that classify music genre based on lyrics and some engineered features that describe song attributes.\n\n\nTo start off, I will read the data and turn them into Dataset objects that can be accessed by PyTorch.\n\n# download dataset\nimport pandas as pd\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/PIC16B/master/datasets/tcc_ceds_music.csv\"\ndf = pd.read_csv(url)\nengineered_features = ['dating', 'violence', 'world/life', 'night/time','shake the audience','family/gospel', 'romantic', 'communication','obscene', 'music', 'movement/places', 'light/visual perceptions','family/spiritual', 'like/girls', 'sadness', 'feelings', 'danceability','loudness', 'acousticness', 'instrumentalness', 'valence', 'energy'] \n\n\ndf.head()\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      Unnamed: 0\n      artist_name\n      track_name\n      release_date\n      genre\n      lyrics\n      len\n      dating\n      violence\n      world/life\n      ...\n      sadness\n      feelings\n      danceability\n      loudness\n      acousticness\n      instrumentalness\n      valence\n      energy\n      topic\n      age\n    \n  \n  \n    \n      0\n      0\n      mukesh\n      mohabbat bhi jhoothi\n      1950\n      pop\n      hold time feel break feel untrue convince spea...\n      95\n      0.000598\n      0.063746\n      0.000598\n      ...\n      0.380299\n      0.117175\n      0.357739\n      0.454119\n      0.997992\n      0.901822\n      0.339448\n      0.137110\n      sadness\n      1.0\n    \n    \n      1\n      4\n      frankie laine\n      i believe\n      1950\n      pop\n      believe drop rain fall grow believe darkest ni...\n      51\n      0.035537\n      0.096777\n      0.443435\n      ...\n      0.001284\n      0.001284\n      0.331745\n      0.647540\n      0.954819\n      0.000002\n      0.325021\n      0.263240\n      world/life\n      1.0\n    \n    \n      2\n      6\n      johnnie ray\n      cry\n      1950\n      pop\n      sweetheart send letter goodbye secret feel bet...\n      24\n      0.002770\n      0.002770\n      0.002770\n      ...\n      0.002770\n      0.225422\n      0.456298\n      0.585288\n      0.840361\n      0.000000\n      0.351814\n      0.139112\n      music\n      1.0\n    \n    \n      3\n      10\n      pérez prado\n      patricia\n      1950\n      pop\n      kiss lips want stroll charm mambo chacha merin...\n      54\n      0.048249\n      0.001548\n      0.001548\n      ...\n      0.225889\n      0.001548\n      0.686992\n      0.744404\n      0.083935\n      0.199393\n      0.775350\n      0.743736\n      romantic\n      1.0\n    \n    \n      4\n      12\n      giorgos papadopoulos\n      apopse eida oneiro\n      1950\n      pop\n      till darling till matter know till dream live ...\n      48\n      0.001350\n      0.001350\n      0.417772\n      ...\n      0.068800\n      0.001350\n      0.291671\n      0.646489\n      0.975904\n      0.000246\n      0.597073\n      0.394375\n      romantic\n      1.0\n    \n  \n\n5 rows × 31 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nI will look at three genres: hip hop, country, and rock. Since the genre labels are strings, I will encode them with integers.\n\ngenres = {\n    \"hip hop\": 0, \n    \"country\": 1, \n    \"rock\": 2\n}\n\ndf = df[df[\"genre\"].apply(lambda x: x in genres.keys())]\ndf[\"genre\"] = df[\"genre\"].apply(genres.get)\ndf.head()\n\nSettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df[\"genre\"] = df[\"genre\"].apply(genres.get)\n\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      Unnamed: 0\n      artist_name\n      track_name\n      release_date\n      genre\n      lyrics\n      len\n      dating\n      violence\n      world/life\n      ...\n      sadness\n      feelings\n      danceability\n      loudness\n      acousticness\n      instrumentalness\n      valence\n      energy\n      topic\n      age\n    \n  \n  \n    \n      7042\n      20290\n      lefty frizzell\n      if you've got the money i've got the time\n      1950\n      1\n      money time honky tonkin time night spot dance ...\n      59\n      0.022813\n      0.001074\n      0.001074\n      ...\n      0.001074\n      0.001074\n      0.523448\n      0.655488\n      0.833333\n      0.000095\n      0.955688\n      0.392373\n      night/time\n      1.0\n    \n    \n      7043\n      20292\n      lefty frizzell\n      i want to be with you always\n      1950\n      1\n      lose blue heart stay go sing song wanna dear n...\n      24\n      0.002288\n      0.002288\n      0.002288\n      ...\n      0.205663\n      0.091285\n      0.705405\n      0.594980\n      0.777108\n      0.000229\n      0.717642\n      0.226202\n      music\n      1.0\n    \n    \n      7044\n      20293\n      lefty frizzell\n      how long will it take (to stop loving you)\n      1950\n      1\n      long count star long climb mar long world stan...\n      34\n      0.001595\n      0.001595\n      0.119458\n      ...\n      0.001595\n      0.001595\n      0.780136\n      0.583109\n      0.892570\n      0.000052\n      0.706307\n      0.180155\n      night/time\n      1.0\n    \n    \n      7045\n      20297\n      lefty frizzell\n      look what thoughts will do\n      1950\n      1\n      think love think love look thoughts today wear...\n      44\n      0.001253\n      0.001253\n      0.308536\n      ...\n      0.001253\n      0.039916\n      0.716235\n      0.609697\n      0.734939\n      0.000000\n      0.703215\n      0.249226\n      world/life\n      1.0\n    \n    \n      7046\n      20300\n      lefty frizzell\n      treasure untold\n      1950\n      1\n      dream eye blue love forever long dear want nea...\n      36\n      0.001698\n      0.001698\n      0.140714\n      ...\n      0.001698\n      0.001698\n      0.703238\n      0.648848\n      0.685743\n      0.000000\n      0.384790\n      0.219195\n      romantic\n      1.0\n    \n  \n\n5 rows × 31 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n# create Dataset class using features of interest\nfrom torch.utils.data import Dataset, DataLoader\nimport torch\n\nengineered_feature_indices = [df.columns.get_loc(col_name) for col_name in engineered_features]\n\nclass TextDataFromDF(Dataset):\n    def __init__(self, df):\n        self.df = df\n    \n    def __getitem__(self, index): # get lyrics, engineered features, and genre labels\n        lyrics = self.df.iloc[index, 5]\n        engineered = self.df.iloc[index, engineered_feature_indices].tolist()\n        labels = self.df.iloc[index, 4]\n        return lyrics, engineered, labels\n\n    def __len__(self):\n        return len(self.df)                \n\n\n# train-test split\nfrom sklearn.model_selection import train_test_split\n\ndf_train, df_test = train_test_split(df,shuffle = True, test_size = 0.2)\ntrain_data = TextDataFromDF(df_train)\ntest_data  = TextDataFromDF(df_test)\n\nEach element of the dataset is a tuple containing the lyrics, engineered features, and integer labels.\n\ntrain_data[0]\n\n('feet feet fight missiles spear thirtyone seventeen soldier thousand years catholic hindu atheist buddhist baptist know shouldn kill know kill friend fight fight fight fight russians fight japan think fight democracy fight reds say peace decide live see write wall hitler condemn dachau stand give body weapon kill universal soldier blame order come away come brothers',\n [0.0015037595085359,\n  0.7092493819987846,\n  0.0015037595507576,\n  0.0015037594168285,\n  0.0015037594030349,\n  0.0015037594155051,\n  0.0015037594066574,\n  0.177928468124219,\n  0.001503759427317,\n  0.0015037594637539,\n  0.0015037594607704,\n  0.0015037594349133,\n  0.0502747402292334,\n  0.0015037594141537,\n  0.0399910181175007,\n  0.0015037594017228,\n  0.4877071374417849,\n  0.5141142989000845,\n  0.780120261164921,\n  0.0,\n  0.414674361088211,\n  0.208183478803342],\n 2)\n\n\n\n\nNext, I will vectorize the lyrics text using similar approaches as our text classification lecture notes.\n\n# create tokenizer and vocabulary\nfrom torchtext.data.utils import get_tokenizer\nfrom torchtext.vocab import build_vocab_from_iterator\n\ntokenizer = get_tokenizer('basic_english')\n\ndef yield_tokens(data_iter): \n  '''\n  loop through our dataset and tokenize lyrics\n  '''\n  for lyrics, _, _ in data_iter:\n      yield tokenizer(lyrics)\n\nvocab = build_vocab_from_iterator(yield_tokens(train_data), specials=[\"<unk>\"], min_freq = 50) # only include tokens that appeared at least 50 times\nvocab.set_default_index(vocab[\"<unk>\"])\n\nTo check if the tokenizer and vocabulary are working correctly:\n\ntokenized = tokenizer(train_data[100][0])\nprint(tokenized)\nprint(vocab(tokenized))\n\n['peepbo', 'peachblow', 'pandour', 'pompadour', 'pale', 'leaf', 'pink', 'sweet', 'deer', 'peepbo', 'animal', 'peepbo', 'deer', 'peepbo', 'animal', 'peepbo', 'deer', 'peepbo', 'animal', 'peepbo', 'deer', 'peepbo', 'animal', 'peepbo', 'peepbo', 'peachblow', 'pandour', 'pompadour', 'pale', 'leaf', 'pink', 'sweet', 'deer', 'peepbo', 'animal', 'peepbo', 'deer', 'peepbo', 'animal', 'peepbo', 'deer', 'peepbo', 'animal', 'peepbo', 'deer', 'peepbo', 'animal', 'peepbo', 'peepbo', 'peachblow', 'pandour', 'pompadour', 'pale', 'leaf', 'pink', 'sweet', 'deer', 'peepbo', 'animal', 'peepbo', 'deer', 'peepbo', 'animal', 'peepbo', 'deer', 'peepbo', 'animal', 'peepbo', 'deer', 'peepbo', 'animal', 'peepbo', 'predaria', 'predo', 'pradari', 'peepbo', 'peachblow', 'pandour', 'pompadour', 'pale', 'leaf', 'pink', 'sweet', 'peepbo', 'peachblow', 'pandour', 'pompadour', 'pale', 'leaf', 'pink', 'sweet', 'deer', 'peepbo', 'animal', 'peepbo', 'deer', 'peepbo', 'animal', 'peepbo', 'deer', 'peepbo', 'animal', 'peepbo', 'deer', 'peepbo', 'animal', 'peepbo', 'peepbo', 'peachblow', 'pandour']\n[0, 0, 0, 0, 0, 0, 0, 66, 0, 0, 798, 0, 0, 0, 798, 0, 0, 0, 798, 0, 0, 0, 798, 0, 0, 0, 0, 0, 0, 0, 0, 66, 0, 0, 798, 0, 0, 0, 798, 0, 0, 0, 798, 0, 0, 0, 798, 0, 0, 0, 0, 0, 0, 0, 0, 66, 0, 0, 798, 0, 0, 0, 798, 0, 0, 0, 798, 0, 0, 0, 798, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 66, 0, 0, 0, 0, 0, 0, 0, 66, 0, 0, 798, 0, 0, 0, 798, 0, 0, 0, 798, 0, 0, 0, 798, 0, 0, 0, 0]\n\n\n\n\n\nFor the last step of data preparation, I will create DataLoaders that pass batches of data to the training loop.\nBefore creating the DataLoaders, I will represent the lyrics with integers, and pad them to make them have uniform lengths.\n\n# determine max length of the lyrics\nmax_len = 0\nfor data in train_data:\n  length = len(data[0].split())\n  if length > max_len:\n    max_len = length\n\n\n# represent lyrics with integers and pad them\nnum_tokens = len(vocab.get_itos())\ndef text_pipeline(x):\n    tokens = vocab(tokenizer(x))\n    y = torch.zeros(max_len, dtype=torch.int64) + num_tokens\n    y[0:len(tokens)] = torch.tensor(tokens,dtype=torch.int64)\n    return y\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\ndef collate_batch(batch):\n    lyrics_list, label_list = [], [],\n    engineered_tuple = ()\n    for (_lyrics, _engineered, _label) in batch:\n      # add lyrics to list\n      processed_lyrics = text_pipeline(_lyrics)\n      lyrics_list.append(processed_lyrics)\n\n      # add engineered features to tuple\n      engineered_tuple += (_engineered, )\n\n      # add label to list\n      label_list.append(_label)\n\n\n    lyrics_tensor = torch.stack(lyrics_list)\n    engineered_tensor = torch.tensor(engineered_tuple, dtype=torch.float32)\n    label_tensor = torch.tensor(label_list, dtype=torch.int64)\n    \n    return lyrics_tensor.to(device), engineered_tensor.to(device), label_tensor.to(device)\n\n\ntrain_loader = DataLoader(train_data, batch_size=16, shuffle=True, collate_fn=collate_batch)\ntest_loader = DataLoader(test_data, batch_size=16, shuffle=True, collate_fn=collate_batch)\n\nEach batch of data returns three tensors: the lyrics, engineered features, and labels.\n\nnext(iter(train_loader))\n\n(tensor([[ 190,    5,  190,  ..., 1372, 1372, 1372],\n         [  35,  137,   62,  ..., 1372, 1372, 1372],\n         [  84,   27,   27,  ..., 1372, 1372, 1372],\n         ...,\n         [   0,   27,  257,  ..., 1372, 1372, 1372],\n         [  13,    3,    0,  ..., 1372, 1372, 1372],\n         [  36,   57, 1041,  ..., 1372, 1372, 1372]], device='cuda:0'),\n tensor([[1.2563e-01, 8.2237e-04, 8.2237e-04, 7.9477e-02, 7.4051e-02, 8.2237e-04,\n          1.8140e-02, 1.6188e-01, 8.2237e-04, 8.2237e-04, 8.2237e-04, 8.2237e-04,\n          8.2237e-04, 8.2237e-04, 7.4127e-02, 4.5682e-01, 5.1045e-01, 7.6297e-01,\n          7.2289e-01, 1.3158e-05, 7.5371e-01, 6.0359e-01],\n         [1.3158e-03, 1.3158e-03, 1.2600e-01, 1.3158e-03, 1.3158e-03, 1.3158e-03,\n          2.4707e-01, 1.3158e-03, 1.3158e-03, 1.3158e-03, 1.3158e-03, 1.3158e-03,\n          1.3158e-03, 1.3158e-03, 6.0587e-01, 1.3158e-03, 3.4907e-01, 7.5292e-01,\n          2.6606e-01, 2.6923e-05, 8.4852e-01, 6.2661e-01],\n         [5.9137e-04, 5.4297e-01, 5.9137e-04, 5.9137e-04, 1.5789e-01, 4.0467e-02,\n          5.9137e-04, 5.9137e-04, 5.9137e-04, 5.9137e-04, 5.9137e-04, 5.9137e-04,\n          5.9137e-04, 5.9137e-04, 9.2487e-02, 5.9137e-04, 4.7688e-01, 6.6959e-01,\n          1.1144e-02, 6.1741e-05, 3.2296e-01, 8.5185e-01],\n         [6.2902e-02, 3.0960e-03, 3.0960e-03, 3.0960e-03, 3.0960e-03, 3.0960e-03,\n          3.2338e-01, 3.0960e-03, 3.0960e-03, 3.0960e-03, 3.0960e-03, 8.1015e-02,\n          1.3279e-01, 3.0960e-03, 1.8003e-01, 3.0960e-03, 3.2091e-01, 6.1941e-01,\n          9.1767e-01, 4.1093e-03, 4.3941e-01, 3.2430e-01],\n         [1.8149e-03, 1.9740e-01, 1.8149e-03, 1.8149e-03, 1.8149e-03, 1.8149e-03,\n          1.8149e-03, 1.8149e-03, 1.8149e-03, 1.8149e-03, 3.3243e-01, 1.8149e-03,\n          3.8736e-02, 1.8149e-03, 4.0422e-01, 1.8149e-03, 4.0214e-01, 4.4807e-01,\n          7.0884e-01, 9.7368e-03, 1.7972e-01, 6.6265e-01],\n         [5.7208e-04, 5.7208e-04, 5.7208e-04, 5.7208e-04, 5.7208e-04, 1.2899e-02,\n          1.3940e-02, 7.4754e-02, 4.6015e-01, 5.7208e-04, 3.5104e-02, 5.5986e-02,\n          5.7208e-04, 5.7208e-04, 2.9759e-01, 5.7208e-04, 6.7075e-01, 7.0010e-01,\n          7.0481e-02, 0.0000e+00, 8.1451e-01, 7.1971e-01],\n         [1.8797e-03, 3.7041e-01, 1.8797e-03, 8.4183e-02, 1.8797e-03, 1.8797e-03,\n          3.2794e-02, 1.5335e-01, 1.8797e-03, 1.8797e-03, 1.8797e-03, 1.8233e-01,\n          1.8797e-03, 1.8797e-03, 1.8797e-03, 6.3712e-02, 3.2308e-01, 6.6167e-01,\n          9.1456e-03, 6.7510e-03, 2.1476e-01, 9.4995e-01],\n         [2.5063e-03, 2.5063e-03, 2.5063e-03, 2.5063e-03, 2.5063e-03, 2.5063e-03,\n          7.8521e-02, 4.6253e-01, 2.5063e-03, 2.5063e-03, 2.5063e-03, 2.5063e-03,\n          6.0003e-02, 2.5063e-03, 3.1478e-01, 2.5063e-03, 6.0576e-01, 5.0617e-01,\n          8.9859e-01, 3.4312e-05, 4.8784e-01, 1.3311e-01],\n         [8.9206e-04, 8.9206e-04, 8.9206e-04, 8.9206e-04, 8.9206e-04, 8.2017e-02,\n          9.4692e-02, 6.1993e-02, 5.8138e-02, 8.9206e-04, 2.1777e-01, 8.9206e-04,\n          8.9206e-04, 8.9206e-04, 4.2052e-01, 8.9206e-04, 4.4330e-01, 7.4281e-01,\n          7.7711e-05, 1.2348e-01, 6.3108e-01, 7.3573e-01],\n         [1.0121e-03, 1.0121e-03, 1.0121e-03, 1.0121e-03, 1.0121e-03, 1.0121e-03,\n          1.0121e-03, 1.0121e-03, 4.1138e-01, 1.0121e-03, 3.1253e-01, 1.0121e-03,\n          1.0121e-03, 1.0121e-03, 1.5268e-01, 1.0823e-01, 4.3247e-01, 5.3480e-01,\n          2.3494e-01, 0.0000e+00, 7.2692e-01, 6.5364e-01],\n         [2.3923e-03, 2.3923e-03, 2.3923e-03, 2.3923e-03, 4.7847e-02, 9.7432e-02,\n          2.3923e-03, 2.3923e-03, 5.6130e-02, 2.3923e-03, 2.3923e-03, 9.4145e-02,\n          2.3923e-03, 2.3923e-03, 5.7453e-01, 2.3923e-03, 6.7183e-01, 7.2410e-01,\n          4.9196e-02, 2.0445e-02, 9.7012e-01, 8.3983e-01],\n         [1.5949e-03, 1.5949e-03, 1.5949e-03, 1.5949e-03, 1.5949e-03, 1.5949e-03,\n          6.0752e-02, 1.5949e-03, 1.5949e-03, 4.3576e-01, 3.8856e-01, 1.5949e-03,\n          1.5949e-03, 1.5949e-03, 1.5949e-03, 2.9081e-02, 3.2850e-01, 7.1048e-01,\n          1.3253e-01, 8.6943e-03, 4.5486e-01, 7.1971e-01],\n         [1.4620e-03, 3.9148e-01, 1.4620e-03, 1.4620e-03, 1.4620e-03, 1.4620e-03,\n          1.4620e-03, 1.4620e-03, 1.4620e-03, 1.4620e-03, 1.4620e-03, 1.1630e-01,\n          7.2153e-02, 1.4620e-03, 3.9813e-01, 1.4620e-03, 4.9637e-01, 6.0718e-01,\n          9.1968e-01, 0.0000e+00, 2.1373e-01, 1.4512e-01],\n         [1.5480e-03, 1.5480e-03, 1.5480e-03, 8.1524e-02, 1.5480e-03, 1.5480e-03,\n          3.1767e-02, 9.7475e-02, 1.5480e-03, 1.5480e-03, 1.5480e-03, 1.5480e-03,\n          1.5480e-03, 1.5480e-03, 7.0748e-01, 1.5480e-03, 5.7327e-01, 5.8839e-01,\n          8.8153e-01, 2.1559e-02, 5.8574e-01, 2.8927e-01],\n         [1.7544e-03, 1.7544e-03, 3.9870e-01, 2.8864e-01, 1.7544e-03, 1.7544e-03,\n          1.7544e-03, 1.7544e-03, 1.7544e-03, 1.7544e-03, 1.3114e-01, 1.5521e-01,\n          1.7544e-03, 1.7544e-03, 1.7544e-03, 1.7544e-03, 4.5197e-01, 6.3493e-01,\n          8.0321e-01, 1.3462e-04, 5.8883e-01, 3.7335e-01],\n         [1.0965e-03, 1.0965e-03, 5.9253e-02, 1.0965e-03, 1.0965e-03, 8.3288e-02,\n          1.0965e-03, 1.0965e-03, 1.0965e-03, 1.0965e-03, 1.2537e-01, 1.0965e-03,\n          1.0965e-03, 1.1992e-01, 4.6606e-01, 1.3185e-01, 5.4186e-01, 7.6104e-01,\n          2.7107e-02, 4.5445e-03, 4.8475e-01, 9.7097e-01]], device='cuda:0'),\n tensor([1, 1, 2, 2, 2, 1, 2, 1, 2, 2, 2, 1, 1, 1, 1, 2], device='cuda:0'))\n\n\n\n\n\n\nI will train three neural networks on the data: 1. A model that only uses the lyrics as features. 2. A model that only uses the engineered features. 3. A model taht uses both the lyrics and the engineered features.\nBefore building the models, I will define the training and testing loops.\n\n# training loop\nlearning_rate = 0.01\n\ndef train(dataloader, feature_choice = \"l\", k_epochs = 1, print_every = 50):\n    # select model based on feature choice\n    if feature_choice == \"l\": # lyrics only model\n      model = lyrics_model\n    elif feature_choice == \"e\": # engineered features only\n      model = engineered_model\n    elif feature_choice == \"b\": # both lyrics and engineered features\n      model = both_model\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n    loss_fn = torch.nn.CrossEntropyLoss()\n\n    log_interval = 300\n\n    for epoch in range(k_epochs): \n      running_loss = 0.0\n      for idx, (lyrics, engineered, label) in enumerate(dataloader):\n          # for calculating accuracy\n          correct, total = 0, 0\n\n          # zero gradients\n          optimizer.zero_grad()\n          # form prediction on batch, using chosen features and models\n          if feature_choice == \"l\": # lyrics only model\n            predicted_label = model(lyrics)\n          elif feature_choice == \"e\": # engineered features only\n            predicted_label = model(engineered)\n          elif feature_choice == \"b\": # both lyrics and engineered features\n            predicted_label = model(lyrics, engineered)\n          \n          # evaluate loss on prediction\n          loss = loss_fn(predicted_label, label)\n          # compute gradient\n          loss.backward()\n          # take an optimization step\n          optimizer.step()\n\n          # update running loss\n          running_loss += loss.item()\n\n          # for printing accuracy\n          correct += (predicted_label.argmax(1) == label).sum().item()\n          total   += label.size(0)\n\n          if idx % print_every == print_every - 1:    \n            print(f'[epoch: {epoch + 1}, batches: {idx + 1:5d}], loss: {running_loss / print_every:.3f}, accuracy:{correct/total:.3f}')\n            running_loss = 0.0\n\n    print(\"Finished Training\") \n    \n    \n\n\n# testing loop\ndef test(dataloader, feature_choice = \"l\"):\n    correct, total = 0, 0\n\n    with torch.no_grad():\n        for idx, (lyrics, engineered, label) in enumerate(dataloader):\n            # form prediction on batch, using chosen features and models\n            if feature_choice == \"l\": # lyrics only model\n              predicted_label = lyrics_model(lyrics)\n            elif feature_choice == \"e\": # engineered features only\n              predicted_label = engineered_model(engineered)\n            elif feature_choice == \"b\": # both lyrics and engineered features\n              predicted_label = both_model(lyrics, engineered)\n\n            correct += (predicted_label.argmax(1) == label).sum().item()\n            total   += label.size(0)\n\n    print(f'Test accuracy: {100 * correct // total} %')\n\n\n\nI will use a simple model with a word embedding layer to classify music based on only the lyrics.\n\nfrom torch import nn\n\n# define the model\nclass LyricsModel(nn.Module):\n    \n    def __init__(self, vocab_size, embedding_dim, max_len, num_class):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size+1, embedding_dim)\n        self.fc = nn.Linear(max_len*embedding_dim, num_class)\n        \n    def forward(self, x):\n        x = self.embedding(x)\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n        return(x)\n\n\n# instantiate the model\nembedding_dim = 3\n\nlyrics_model = LyricsModel(len(vocab), embedding_dim, max_len, 3).to(device)\n\n\n# train the model\nk_epochs = 10\n\ntrain(train_loader, \"l\", k_epochs, 200)\n\n[epoch: 1, batches:   200], loss: 1.192, accuracy:0.500\n[epoch: 1, batches:   400], loss: 1.039, accuracy:0.312\n[epoch: 2, batches:   200], loss: 0.818, accuracy:0.562\n[epoch: 2, batches:   400], loss: 0.797, accuracy:0.688\n[epoch: 3, batches:   200], loss: 0.628, accuracy:0.688\n[epoch: 3, batches:   400], loss: 0.649, accuracy:0.812\n[epoch: 4, batches:   200], loss: 0.509, accuracy:0.500\n[epoch: 4, batches:   400], loss: 0.542, accuracy:0.938\n[epoch: 5, batches:   200], loss: 0.436, accuracy:0.750\n[epoch: 5, batches:   400], loss: 0.497, accuracy:0.812\n[epoch: 6, batches:   200], loss: 0.409, accuracy:0.750\n[epoch: 6, batches:   400], loss: 0.456, accuracy:0.750\n[epoch: 7, batches:   200], loss: 0.365, accuracy:0.875\n[epoch: 7, batches:   400], loss: 0.410, accuracy:0.688\n[epoch: 8, batches:   200], loss: 0.346, accuracy:0.875\n[epoch: 8, batches:   400], loss: 0.383, accuracy:0.812\n[epoch: 9, batches:   200], loss: 0.312, accuracy:0.875\n[epoch: 9, batches:   400], loss: 0.382, accuracy:0.938\n[epoch: 10, batches:   200], loss: 0.326, accuracy:0.875\n[epoch: 10, batches:   400], loss: 0.363, accuracy:0.750\nFinished Training\n\n\n\n# test accuracy\ntest(test_loader, \"l\")\n\nTest accuracy: 65 %\n\n\nThe test accuracy is lower than the training accuracy. This suggests overfitting.\nI will add two dropout layers to the network and see if this reduces overfitting.\n\nclass LyricsModelDropout(nn.Module):\n    \n    def __init__(self, vocab_size, embedding_dim, max_len, num_class):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size+1, embedding_dim)\n        self.dropout1 = nn.Dropout(0.2)\n        self.fc = nn.Linear(3, num_class)\n        self.dropout2 = nn.Dropout(0.2)\n        \n    def forward(self, x):\n        x = self.embedding(x)\n        x = self.dropout1(x)\n        x = x.mean(axis = 1) # take the average across tokens for each embedding dimension\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n        x = self.dropout2(x)\n        return(x)\n\n\nlyrics_model = LyricsModelDropout(len(vocab), embedding_dim, max_len, 3).to(device)\n\n\n# train the model\nk_epochs = 10\n\ntrain(train_loader, \"l\", k_epochs, 200)\n\n[epoch: 1, batches:   200], loss: 0.953, accuracy:0.562\n[epoch: 1, batches:   400], loss: 0.896, accuracy:0.562\n[epoch: 2, batches:   200], loss: 0.854, accuracy:0.500\n[epoch: 2, batches:   400], loss: 0.825, accuracy:0.625\n[epoch: 3, batches:   200], loss: 0.743, accuracy:0.750\n[epoch: 3, batches:   400], loss: 0.732, accuracy:0.500\n[epoch: 4, batches:   200], loss: 0.691, accuracy:0.625\n[epoch: 4, batches:   400], loss: 0.676, accuracy:0.750\n[epoch: 5, batches:   200], loss: 0.643, accuracy:0.688\n[epoch: 5, batches:   400], loss: 0.645, accuracy:0.688\n[epoch: 6, batches:   200], loss: 0.617, accuracy:0.875\n[epoch: 6, batches:   400], loss: 0.630, accuracy:0.750\n[epoch: 7, batches:   200], loss: 0.608, accuracy:0.812\n[epoch: 7, batches:   400], loss: 0.612, accuracy:0.875\n[epoch: 8, batches:   200], loss: 0.600, accuracy:0.562\n[epoch: 8, batches:   400], loss: 0.598, accuracy:0.750\n[epoch: 9, batches:   200], loss: 0.591, accuracy:0.875\n[epoch: 9, batches:   400], loss: 0.585, accuracy:0.812\n[epoch: 10, batches:   200], loss: 0.574, accuracy:0.750\n[epoch: 10, batches:   400], loss: 0.583, accuracy:0.688\nFinished Training\n\n\n\n# test accuracy\ntest(test_loader, \"l\")\n\nTest accuracy: 67 %\n\n\nThe dropout layers did reduce overfitting, and the test accuracy improved by 2%.\nThe base rate for our classification across four genres is 33.3%, so the accuracy of 67% suggests that the model is at least doing one time better than random guessing.\n\n\n\nNext, I will train some models that take in only the engineered features of the songs.\n\nimport torch.nn.functional as F\n\nclass EngineeredModel(nn.Module):\n    \n    def __init__(self, num_class):\n        super().__init__()\n        self.fc1 = nn.Linear(22, 64)\n        self.fc2 = nn.Linear(64, 32)\n        self.fc3 = nn.Linear(32, num_class)\n        \n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return(x)\n\n\nengineered_model = EngineeredModel(3).to(device)\n\n\nlearning_rate = 0.0001\nk_epochs = 10\ntrain(train_loader, \"e\", k_epochs, 200)\n\n[epoch: 1, batches:   200], loss: 1.096, accuracy:0.250\n[epoch: 1, batches:   400], loss: 0.988, accuracy:0.812\n[epoch: 2, batches:   200], loss: 0.860, accuracy:0.750\n[epoch: 2, batches:   400], loss: 0.802, accuracy:0.750\n[epoch: 3, batches:   200], loss: 0.749, accuracy:0.625\n[epoch: 3, batches:   400], loss: 0.713, accuracy:0.688\n[epoch: 4, batches:   200], loss: 0.711, accuracy:0.562\n[epoch: 4, batches:   400], loss: 0.677, accuracy:0.688\n[epoch: 5, batches:   200], loss: 0.671, accuracy:0.562\n[epoch: 5, batches:   400], loss: 0.653, accuracy:0.750\n[epoch: 6, batches:   200], loss: 0.648, accuracy:0.875\n[epoch: 6, batches:   400], loss: 0.643, accuracy:0.750\n[epoch: 7, batches:   200], loss: 0.623, accuracy:0.875\n[epoch: 7, batches:   400], loss: 0.624, accuracy:0.562\n[epoch: 8, batches:   200], loss: 0.608, accuracy:0.688\n[epoch: 8, batches:   400], loss: 0.614, accuracy:0.875\n[epoch: 9, batches:   200], loss: 0.612, accuracy:0.625\n[epoch: 9, batches:   400], loss: 0.608, accuracy:0.562\n[epoch: 10, batches:   200], loss: 0.603, accuracy:0.875\n[epoch: 10, batches:   400], loss: 0.607, accuracy:0.750\nFinished Training\n\n\n\ntest(test_loader, \"e\")\n\nTest accuracy: 74 %\n\n\nThe accuracy that we got using engineered feature is higher than what we got with the lyrics. However, it seems that the model was struggling to improve accuracy during training.\nAdding another fully connected layer improved the test accuracy for 2%:\n\nclass EngineeredModelMoreFc(nn.Module):\n    \n    def __init__(self, num_class):\n        super().__init__()\n        self.fc1 = nn.Linear(22, 128)\n        self.fc2 = nn.Linear(128, 64)\n        self.fc3 = nn.Linear(64, 32)\n        self.fc4 = nn.Linear(32, num_class)\n        \n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = F.relu(self.fc3(x))\n        x = self.fc4(x)\n\n        return(x)\n\n\nengineered_model = EngineeredModelMoreFc(3).to(device)\n\n\ntrain(train_loader, \"e\", k_epochs, 200)\n\n[epoch: 1, batches:   200], loss: 1.099, accuracy:0.562\n[epoch: 1, batches:   400], loss: 0.925, accuracy:0.750\n[epoch: 2, batches:   200], loss: 0.792, accuracy:0.688\n[epoch: 2, batches:   400], loss: 0.715, accuracy:0.938\n[epoch: 3, batches:   200], loss: 0.672, accuracy:0.875\n[epoch: 3, batches:   400], loss: 0.661, accuracy:0.938\n[epoch: 4, batches:   200], loss: 0.645, accuracy:0.750\n[epoch: 4, batches:   400], loss: 0.614, accuracy:0.812\n[epoch: 5, batches:   200], loss: 0.611, accuracy:0.750\n[epoch: 5, batches:   400], loss: 0.616, accuracy:0.938\n[epoch: 6, batches:   200], loss: 0.589, accuracy:0.812\n[epoch: 6, batches:   400], loss: 0.599, accuracy:0.750\n[epoch: 7, batches:   200], loss: 0.600, accuracy:0.562\n[epoch: 7, batches:   400], loss: 0.588, accuracy:0.688\n[epoch: 8, batches:   200], loss: 0.587, accuracy:0.688\n[epoch: 8, batches:   400], loss: 0.595, accuracy:0.688\n[epoch: 9, batches:   200], loss: 0.568, accuracy:0.875\n[epoch: 9, batches:   400], loss: 0.589, accuracy:0.750\n[epoch: 10, batches:   200], loss: 0.578, accuracy:0.688\n[epoch: 10, batches:   400], loss: 0.584, accuracy:0.688\nFinished Training\n\n\n\ntest(test_loader, \"e\")\n\nTest accuracy: 76 %\n\n\nUsing only one layer, on the other hand, decreased test accuracy:\n\n# one hidden layer with 32 outputs\nclass EngineeredModel1Fc(nn.Module):\n    \n    def __init__(self, num_class):\n        super().__init__()\n        self.fc1 = nn.Linear(22, 32)\n        self.fc2 = nn.Linear(32, num_class)\n        \n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return(x)\n\n\nengineered_model = EngineeredModel1Fc(3).to(device)\n\n\ntrain(train_loader, \"e\", k_epochs, 200) \n\n[epoch: 1, batches:   200], loss: 1.119, accuracy:0.562\n[epoch: 1, batches:   400], loss: 1.056, accuracy:0.438\n[epoch: 2, batches:   200], loss: 0.964, accuracy:0.625\n[epoch: 2, batches:   400], loss: 0.923, accuracy:0.500\n[epoch: 3, batches:   200], loss: 0.889, accuracy:0.438\n[epoch: 3, batches:   400], loss: 0.853, accuracy:0.500\n[epoch: 4, batches:   200], loss: 0.837, accuracy:0.625\n[epoch: 4, batches:   400], loss: 0.811, accuracy:0.688\n[epoch: 5, batches:   200], loss: 0.785, accuracy:0.688\n[epoch: 5, batches:   400], loss: 0.785, accuracy:0.875\n[epoch: 6, batches:   200], loss: 0.744, accuracy:0.750\n[epoch: 6, batches:   400], loss: 0.753, accuracy:0.562\n[epoch: 7, batches:   200], loss: 0.715, accuracy:0.438\n[epoch: 7, batches:   400], loss: 0.726, accuracy:0.688\n[epoch: 8, batches:   200], loss: 0.703, accuracy:0.688\n[epoch: 8, batches:   400], loss: 0.704, accuracy:0.375\n[epoch: 9, batches:   200], loss: 0.681, accuracy:0.688\n[epoch: 9, batches:   400], loss: 0.690, accuracy:0.812\n[epoch: 10, batches:   200], loss: 0.671, accuracy:0.812\n[epoch: 10, batches:   400], loss: 0.666, accuracy:0.688\nFinished Training\n\n\n\ntest(test_loader, \"e\")\n\nTest accuracy: 72 %\n\n\nIt looks like accuracy increased as the number of layers increased. I will try another model with even more layers.\n\nclass EngineeredModelMoreFc(nn.Module):\n    \n    def __init__(self, num_class):\n        super().__init__()\n        self.fc1 = nn.Linear(22, 128)\n        self.fc2 = nn.Linear(128, 64)\n        self.fc3 = nn.Linear(64, 32)\n        self.fc4 = nn.Linear(32, 16)\n        self.fc5 = nn.Linear(16, num_class)\n        \n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = F.relu(self.fc3(x))\n        x = F.relu(self.fc4(x))\n        x = self.fc5(x)\n\n        return(x)\n\n\nengineered_model = EngineeredModelMoreFc(3).to(device)\n\n\ntrain(train_loader, \"e\", k_epochs, 200) \n\n[epoch: 1, batches:   200], loss: 1.137, accuracy:0.250\n[epoch: 1, batches:   400], loss: 0.958, accuracy:0.625\n[epoch: 2, batches:   200], loss: 0.845, accuracy:0.562\n[epoch: 2, batches:   400], loss: 0.769, accuracy:0.750\n[epoch: 3, batches:   200], loss: 0.733, accuracy:0.562\n[epoch: 3, batches:   400], loss: 0.669, accuracy:0.625\n[epoch: 4, batches:   200], loss: 0.643, accuracy:0.938\n[epoch: 4, batches:   400], loss: 0.636, accuracy:0.625\n[epoch: 5, batches:   200], loss: 0.627, accuracy:0.875\n[epoch: 5, batches:   400], loss: 0.626, accuracy:0.812\n[epoch: 6, batches:   200], loss: 0.605, accuracy:0.625\n[epoch: 6, batches:   400], loss: 0.624, accuracy:0.625\n[epoch: 7, batches:   200], loss: 0.613, accuracy:0.812\n[epoch: 7, batches:   400], loss: 0.612, accuracy:0.562\n[epoch: 8, batches:   200], loss: 0.595, accuracy:0.812\n[epoch: 8, batches:   400], loss: 0.594, accuracy:0.688\n[epoch: 9, batches:   200], loss: 0.589, accuracy:0.812\n[epoch: 9, batches:   400], loss: 0.594, accuracy:0.688\n[epoch: 10, batches:   200], loss: 0.579, accuracy:0.688\n[epoch: 10, batches:   400], loss: 0.591, accuracy:0.875\nFinished Training\n\n\n\ntest(test_loader, \"e\")\n\nTest accuracy: 75 %\n\n\nWe didn’t get a better accuracy with one more layer. It seems that the benefit of making the model more complex has leveled out.\nIn conclusion, the best performing model is the one with four fully connected layers, with the test accuracy of 76%.\n\n\n\nFinally, I will try a model that takes in both the lyrics and the engineered features. This model takes in the lyrics data and engineered features separately. The lyrics are passed through an embedding layer and a dropout layer. The engineered features go through three fully-connected layers. Finally, the lyrics and engineered feature outputs are combined into one tensor, and passed through two fully-connected layers and one dropout layer in between.\n\nclass CombinedNet(nn.Module):\n    \n    def __init__(self, vocab_size, embedding_dim, num_class):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size+1, embedding_dim)\n        self.fc1 = nn.Linear(22, 64)\n        self.fc2 = nn.Linear(64, 32)\n        self.fc3 = nn.Linear(32, 16)\n        self.fc4 = nn.Linear(613, 32)\n        self.fc5 = nn.Linear(32, num_class)\n        self.dropout = nn.Dropout(0.2)\n    \n    def forward(self, x_1, x_2):\n        \n        # text pipeline\n        x_1 = self.embedding(x_1)\n        x_1 = self.dropout(x_1)\n        x_1 = torch.flatten(x_1, 1)\n\n        # engineered features\n        x_2 = F.relu(self.fc1(x_2))\n        x_2 = F.relu(self.fc2(x_2))\n        x_2 = self.fc3(x_2)\n\n        # ensure that both x_1 and x_2 are 2-d tensors, flattening if necessary\n        # then, combine them with: \n        x = torch.cat((x_1, x_2), 1)\n        # pass x through a couple more fully-connected layers and return output\n        x = F.relu(self.fc4(x))\n        x = self.dropout(x)\n        x = self.fc5(x)\n        return(x)\n\n\nboth_model = CombinedNet(len(vocab), embedding_dim, 3).to(device)\n\n\nlearning_rate = 0.001\nk_epochs = 10\ntrain(train_loader, \"b\", k_epochs, 200)\n\n[epoch: 1, batches:   200], loss: 0.857, accuracy:0.562\n[epoch: 1, batches:   400], loss: 0.727, accuracy:0.750\n[epoch: 2, batches:   200], loss: 0.702, accuracy:0.750\n[epoch: 2, batches:   400], loss: 0.665, accuracy:0.812\n[epoch: 3, batches:   200], loss: 0.666, accuracy:0.625\n[epoch: 3, batches:   400], loss: 0.652, accuracy:0.812\n[epoch: 4, batches:   200], loss: 0.617, accuracy:0.938\n[epoch: 4, batches:   400], loss: 0.586, accuracy:0.750\n[epoch: 5, batches:   200], loss: 0.562, accuracy:0.812\n[epoch: 5, batches:   400], loss: 0.562, accuracy:0.688\n[epoch: 6, batches:   200], loss: 0.536, accuracy:0.812\n[epoch: 6, batches:   400], loss: 0.548, accuracy:0.625\n[epoch: 7, batches:   200], loss: 0.539, accuracy:0.938\n[epoch: 7, batches:   400], loss: 0.527, accuracy:0.750\n[epoch: 8, batches:   200], loss: 0.521, accuracy:0.688\n[epoch: 8, batches:   400], loss: 0.518, accuracy:0.750\n[epoch: 9, batches:   200], loss: 0.491, accuracy:0.812\n[epoch: 9, batches:   400], loss: 0.506, accuracy:0.812\n[epoch: 10, batches:   200], loss: 0.489, accuracy:1.000\n[epoch: 10, batches:   400], loss: 0.501, accuracy:0.812\nFinished Training\n\n\n\ntest(test_loader, \"b\")\n\nTest accuracy: 74 %\n\n\nThe combined model seems to have similar testing performance as the model with only engineered features. Although I added some dropout layers, there is still some overfitting. In the next model, I will try to add more dropout layers.\n\nclass CombinedNetMoreDropout(nn.Module):\n    \n    def __init__(self, vocab_size, embedding_dim, num_class):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size+1, embedding_dim)\n        self.fc1 = nn.Linear(22, 64)\n        self.fc2 = nn.Linear(64, 32)\n        self.fc3 = nn.Linear(32, 16)\n        self.fc4 = nn.Linear(613, 32)\n        self.fc5 = nn.Linear(32, num_class)\n        self.dropout = nn.Dropout(0.2)\n    \n    def forward(self, x_1, x_2):\n        \n        # text pipeline\n        x_1 = self.embedding(x_1)\n        x_1 = self.dropout(x_1)\n        x_1 = torch.flatten(x_1, 1)\n\n        # engineered features\n        x_2 = F.relu(self.fc1(x_2))\n        x_2 = self.dropout(x_2)\n        x_2 = F.relu(self.fc2(x_2))\n        x_2 = self.dropout(x_2)\n        x_2 = self.fc3(x_2)\n\n        # ensure that both x_1 and x_2 are 2-d tensors, flattening if necessary\n        # then, combine them with: \n        x = torch.cat((x_1, x_2), 1)\n        # pass x through a couple more fully-connected layers and return output\n        x = F.relu(self.fc4(x))\n        x = self.dropout(x)\n        x = self.fc5(x)\n        return(x)\n\n\nboth_model = CombinedNet(len(vocab), embedding_dim, 3).to(device)\n\n\nk_epochs = 10\ntrain(train_loader, \"b\", k_epochs, 200)\n\n[epoch: 1, batches:   200], loss: 0.826, accuracy:0.875\n[epoch: 1, batches:   400], loss: 0.701, accuracy:0.750\n[epoch: 2, batches:   200], loss: 0.640, accuracy:0.750\n[epoch: 2, batches:   400], loss: 0.607, accuracy:0.750\n[epoch: 3, batches:   200], loss: 0.568, accuracy:0.875\n[epoch: 3, batches:   400], loss: 0.568, accuracy:0.688\n[epoch: 4, batches:   200], loss: 0.523, accuracy:0.625\n[epoch: 4, batches:   400], loss: 0.530, accuracy:0.812\n[epoch: 5, batches:   200], loss: 0.521, accuracy:0.625\n[epoch: 5, batches:   400], loss: 0.504, accuracy:0.688\n[epoch: 6, batches:   200], loss: 0.487, accuracy:0.750\n[epoch: 6, batches:   400], loss: 0.493, accuracy:0.625\n[epoch: 7, batches:   200], loss: 0.450, accuracy:0.875\n[epoch: 7, batches:   400], loss: 0.475, accuracy:0.812\n[epoch: 8, batches:   200], loss: 0.449, accuracy:0.625\n[epoch: 8, batches:   400], loss: 0.467, accuracy:0.875\n[epoch: 9, batches:   200], loss: 0.435, accuracy:0.750\n[epoch: 9, batches:   400], loss: 0.459, accuracy:0.750\n[epoch: 10, batches:   200], loss: 0.424, accuracy:0.750\n[epoch: 10, batches:   400], loss: 0.431, accuracy:0.812\nFinished Training\n\n\n\ntest(test_loader, \"b\")\n\nTest accuracy: 74 %\n\n\n\n\n\nIn summary, the best test accuracy achieved by our three types of models are: - Lyrics only: 67% - Engineered features only: 76% - Lyrics and engineered features: 74%\nAll of these models perform better than the base rate of 33.3%, but there is still room for improvement. In general, the engineered features are better predictors of genres than the lyrics. Looking at lyrics in addition to engineered features did not seem to help boost accuracy compared to using the features alone.\n\n\n\n\nIn this section, I will visualize the word embeddings learned by my lyrics model.\n\n# extract the embedding matrix from the lyrics model\nembedding_matrix = lyrics_model.embedding.cpu().weight.data.numpy()\n\n\n# extract words from vocab\ntokens = vocab.get_itos()\ntokens.append(\" \")\n\n\n# represent embedding in two dimensions\nfrom sklearn.decomposition import PCA\npca = PCA(n_components = 2)\nweights = pca.fit_transform(embedding_matrix)\nweights\n\narray([[ 1.6995059 ,  0.86947083],\n       [ 1.9406745 ,  2.9961193 ],\n       [-0.12173966,  2.8156223 ],\n       ...,\n       [ 2.7143688 , -0.7970013 ],\n       [-0.44954112, -2.4305842 ],\n       [ 0.16685873,  0.17623016]], dtype=float32)\n\n\n\n# turn into dataframe\nembedding_df = pd.DataFrame({\n    \"word\": tokens, \n    \"x0\": weights[:,0], \n    \"x1\": weights[:,1]\n})\n\n\n# visualize\nimport plotly.express as px \nimport plotly.io as pio\npio.renderers.default = \"plotly_mimetype+notebook_connected\"\nimport numpy as np\n\nfig = px.scatter(embedding_df, \n                 x = \"x0\", \n                 y = \"x1\", \n                 size = list(np.ones(len(embedding_df))),\n                 size_max = 10,\n                 hover_name = \"word\")\n\nfig.show()\n\n\n                                                \n\n\nThe lyrics model had training accuracies of around 67%, which suggests that the word embeddings may not be learning as well as we expect.\nHowever, we can still see a dinstinction among the three genres:\n\nWords at the top corner are typical for hip hop lyrics.\nWords at the bottom corner, around 0 at the x0 axis, seem to be more related to rock, and have a negative valence, as typical rock music does (e.g., band, pain, desperate, nightmare).\nWords at the left corner appear more in country music (e.g., cowboy, road, Texas).\n\nFinally, I will try to visualize any clusterings that the embedding model learned by doing a k-mean clustering analysis.\n\nfrom sklearn.cluster import KMeans\n\n# perform k-means clustering on the weights matrix\nkmeans = KMeans(n_clusters=3, random_state=42).fit(weights)\nlabels = kmeans.labels_\n\n/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n\n\n\n# scatter plot with cluster information\nembedding_df = pd.DataFrame({\n    \"word\": tokens, \n    \"x0\": weights[:,0], \n    \"x1\": weights[:,1], \n    \"cluster\": labels\n})\n\nfig = px.scatter(embedding_df, \n                 x = \"x0\", \n                 y = \"x1\", \n                 color = \"cluster\",\n                 size = list(np.ones(len(embedding_df))),\n                 size_max = 10,\n                 hover_name = \"word\")\n\nfig.show()\n\n\n                                                \n\n\nLooking into the words in the three clusters, it makes sense that the red cluster includes more “country” lyrics, while the blue cluster is hip hop and the yellow cluster is rock.\n\n\n\nTo conclude, I have experimented with using different features and models to classify music by genre. The best performing model used only engineered features about the audio characteristics of the music and achieved a 76% test accuracy.\nAlthough the models that used lyrics data did not perform better, we see that they have learned valid word embeddings that reflected the styles of the lyrics across the three genres.\nFor this attempt, I have chosen three genres that are quite distinctive from each other in terms of lyrics and music style. The models may face more challenges with data that include more genres that differ in a more subtle way."
  }
]