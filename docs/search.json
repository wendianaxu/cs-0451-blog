[
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CSCI 0451 Blog - Diana Xu",
    "section": "",
    "text": "Classifying Palmer penguins.\n\n\n\n\n\n\nMar 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing logistic degression with gradient descent.\n\n\n\n\n\n\nMar 1, 2023\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/perceptron/perceptron-blogpost.html",
    "href": "posts/perceptron/perceptron-blogpost.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Source code\n\n\nI implemented the perceptron update with the following steps:\n\nModifying the feature matrix \\(X\\) and vector of labels \\(y\\). This includes appending a column of 1s to \\(X\\) and turning \\(y\\) into a vector of -1s and 1s.\nInitializing a random weight vector \\(w\\).\nIn a for-loop that breaks either when accuracy reaches 1 or when the specified max_steps is reached,\n\nPick a random data point with index \\(i\\) and access its features \\(x_{i}\\) and label \\(y_{i}\\);\nCompute the predicted label \\(\\hat{y}\\) using the current \\(w\\);\nPerform update if \\(y\\) * \\(\\hat{y}\\) < 0:\n\n\n\nself.w = self.w + (y_i*y_hat < 0) * (y_i * x_)\n\nWhere only when y_i * y_hat < 0 is (y_i * x_) added to the current w.\nFinally, the accuracy of the current \\(w\\) is computed by using \\(w\\) to predict labels for all data points, and then calculating the number of correct predictions compared to \\(y\\). This accuracy is then appended to the history array.\n\n\n\n\n\n\nfrom perceptron import Perceptron\nimport numpy as np\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\n\nn = 100\np_features = 3\n\n# generate random data\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\n# fit perceptron to data\np1 = Perceptron()\np1.fit(X, y, max_steps = 1000)\n\n# visualizations\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize = (8, 3)) \n\n# visualize accuracy history\nax1.plot(p1.history)\nax1.set(xlabel = \"Iteration\",\nylabel = \"Accuracy\")\n\n# visualize data points and perceptron line\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nax2.set(xlabel = \"Feature 1\",\nylabel = \"Feature 2\")\n\nax2 = plt.scatter(X[:,0], X[:,1], c = y)\nax2 = draw_line(p1.w, -2, 2)\n\n\n\n\nHere, the data is linearly separable. The accuracy is able to reach 1.0 after 125+ iterations. The perceptron line generated is able to separate data with labels -1 and 1.\n\n\n\n\n# generate random data\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\n# fit perceptron to data\np2 = Perceptron()\np2.fit(X, y, max_steps = 1000)\n\n# visualizations\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize = (8, 3)) \n\n# visualize accuracy history\nax1.plot(p2.history)\nax1.set(xlabel = \"Iteration\",\nylabel = \"Accuracy\")\n\n# visualize data points and perceptron line\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nax2.set(xlabel = \"Feature 1\",\nylabel = \"Feature 2\")\n\nax2 = plt.scatter(X[:,0], X[:,1], c = y)\nax2 = draw_line(p2.w, -2, 2)\n\n\n\n\nHere, the data is not linearly separable as there are overlapping points. The accuracy fails to reach 1.0 after 1000 steps. The perceptron line does not perfectly separate the two groups of data.\n\n\n\nHere I generate random data with 6 features and feed it to the perceptron algorithm.\n\n# change the number of features\np_features = 7\n\n# generate random data\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\n# fit perceptron to data\np3 = Perceptron()\np3.fit(X, y, max_steps = 1000)\n\n# visualize accuracy history\nfig = plt.plot(p3.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nSince the accuracy has reached 1.0, the data should be linearly separable.\n\n\n\n\nThe runtime complexity of a single iteration of the perceptron update should be \\(O(p)\\). It would depend on the number of features but not the number of data points."
  },
  {
    "objectID": "posts/reflective-goal-setting/goal-setting.html",
    "href": "posts/reflective-goal-setting/goal-setting.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Wen (Diana) Xu\n\n\nThe knowledge we’ll develop in CSCI 0451 can be broadly divided into four main areas:\n\nTheory: mathematical descriptions of frameworks and algorithms.\nImplementation: effective coding and use of tools in order to implement efficient machine learning algorithms.\nExperimentation: performing experiments to assess the performance of algorithms and clearly communicating about the results.\nSocial responsibility: critical analysis of sources of bias and harm in machine learning algorithms; theoretical formulations of fairness and bias\n\nEvery student should grow toward each of these areas, but you can choose to specialize if you’d like! If there are one or two of these categories on which you’d especially like to focus, list them below. Feel free to include any details that you’d like – this can help me tailor the course content to your interests.\nI would like to focus more on experimentation and social responsibility. I am interested in the applications and implications of machine learning algorithms – how to make use of machine learning to solve problems in the real world, how to assess the performance of the algorithms in relation to the specific problem, and how we can effectively communicate machine learning to a broader audience. I also hope to look more into bias in machine learning algorithms – what causes them and how to identify and reduce sources of bias.\n\n\n\n\n\nMost blog posts will require around 5-8 hours on average to complete, plus time for revisions in response to feedback. Blog posts will most frequently involve a mix of mathematical problem-solving, coding, experimentation, and written discussion. Some blog posts will ask you to critically discuss recent readings in an essay-like format.\n\nSubmit a blog post in most weeks during the semester.\nSubmit the first draft of no more than two blog post after the “best-by” date.\nRevise at least five blog posts to the “No Revisions Suggested” level.\nGo above and beyond in at least two blog posts by performing experiments/exploring visualization choices to communicate results/discussing implications significantly beyond what is required.\n\n\n\n\nYou make a choice each day about how to show up for class: whether you’ll be prepared, whether you’ll engage with me and your peers in a constructive manner; and whether you’ll be active during lecture and discussions. We will also have a special opportunity this semester to engage with a renowned expert in machine learning, algorithmic bias, and the ethics of artificial intelligence.\nAn especially important form of course presence is the daily warmup. We’ll spend the first 10-15 minutes of most class periods on warmup activities. You’re expected to have prepared the warmup activity ahead of time (this means you’ll need to have completed the readings as well). Each time, we’ll sort into groups of 5-6 students, and one of you (randomly selected) will be responsible for presenting the activity on the whiteboard. If you’re not feeling prepared to present the activity, you can “pass” to the next person, or ask for help along the way.\n\nComplete all core readings prior to each class periods.\nComplete the optional readings that correspond to my areas of specialization.\nBe prepared for most warmup activities even on days when I am not the leader.\n“Pass” at most once when asked to lead the warmup activity for my group.\nPropose questions ahead of time for our guest speaker.\nOften work with classmates together on blog posts or other course work outside of class time.\nOften attend Peer Help or Student Hours (after preparing questions and working examples).\n\n\n\n\nTo finish off the course, you’ll complete a long-term project that showcases your interests and skills. You’ll be free to propose and pursue a topic. My expectation is that most projects will move significantly beyond the content covered in class in some way: you might implement a new algorithm, study a complex data set in depth, or conduct a series of experiments related to assessing algorithmic bias in a certain class of algorithm. You’ll be expected to complete this project in small groups (of your choosing), and update us at a few milestones along the way.\nPlease share a bit about what kind of topic might excite you, and set a few goals about how you plan to show up as a constructive team-member and co-inquirer (see the ideas for some inspiration).\n\nSubmit all project milestones (proposal, progress report, etc) on time.\nSet regular time each week to work with project partners.\nCommunicate with my group in a clear and timely manner.\nComplete all my designated work on time.\nDraft designated sections of the project report.\nRevise sections of the project report in response to feedback.\nTake the lead in creating and delivering part of the final project presentation.\nTake the lead in checking project figures for accuracy and clear labeling."
  },
  {
    "objectID": "posts/reflective-goal-setting/goal-setting.html#what-youll-learn",
    "href": "posts/reflective-goal-setting/goal-setting.html#what-youll-learn",
    "title": "Reflective Goal-Setting",
    "section": "What You’ll Learn",
    "text": "What You’ll Learn\nThe knowledge we’ll develop in CSCI 0451 can be broadly divided into four main areas:\n\nTheory: mathematical descriptions of frameworks and algorithms.\nImplementation: effective coding and use of tools in order to implement efficient machine learning algorithms.\nExperimentation: performing experiments to assess the performance of algorithms and clearly communicating about the results.\nSocial responsibility: critical analysis of sources of bias and harm in machine learning algorithms; theoretical formulations of fairness and bias\n\nEvery student should grow toward each of these areas, but you can choose to specialize if you’d like! If there are one or two of these categories on which you’d especially like to focus, list them below. Feel free to include any details that you’d like – this can help me tailor the course content to your interests.\nI would like to focus more on experimentation and social responsibility. I am interested in the applications and implications of machine learning algorithms – how to make use of machine learning to solve problems in the real world, how to assess the performance of the algorithms in relation to the specific problem, and how we can effectively communicate machine learning to a broader audience. I also hope to look more into bias in machine learning algorithms – what causes them and how to identify and reduce sources of bias."
  },
  {
    "objectID": "posts/reflective-goal-setting/goal-setting.html#what-youll-achieve",
    "href": "posts/reflective-goal-setting/goal-setting.html#what-youll-achieve",
    "title": "Reflective Goal-Setting",
    "section": "What You’ll Achieve",
    "text": "What You’ll Achieve\n\nBlog Posts\nMost blog posts will require around 5-8 hours on average to complete, plus time for revisions in response to feedback. Blog posts will most frequently involve a mix of mathematical problem-solving, coding, experimentation, and written discussion. Some blog posts will ask you to critically discuss recent readings in an essay-like format.\n\nSubmit a blog post in most weeks during the semester.\nSubmit the first draft of no more than two blog post after the “best-by” date.\nRevise at least five blog posts to the “No Revisions Suggested” level.\nGo above and beyond in at least two blog posts by performing experiments/exploring visualization choices to communicate results/discussing implications significantly beyond what is required.\n\n\n\nCourse Presence (Participation)\nYou make a choice each day about how to show up for class: whether you’ll be prepared, whether you’ll engage with me and your peers in a constructive manner; and whether you’ll be active during lecture and discussions. We will also have a special opportunity this semester to engage with a renowned expert in machine learning, algorithmic bias, and the ethics of artificial intelligence.\nAn especially important form of course presence is the daily warmup. We’ll spend the first 10-15 minutes of most class periods on warmup activities. You’re expected to have prepared the warmup activity ahead of time (this means you’ll need to have completed the readings as well). Each time, we’ll sort into groups of 5-6 students, and one of you (randomly selected) will be responsible for presenting the activity on the whiteboard. If you’re not feeling prepared to present the activity, you can “pass” to the next person, or ask for help along the way.\n\nComplete all core readings prior to each class periods.\nComplete the optional readings that correspond to my areas of specialization.\nBe prepared for most warmup activities even on days when I am not the leader.\n“Pass” at most once when asked to lead the warmup activity for my group.\nPropose questions ahead of time for our guest speaker.\nOften work with classmates together on blog posts or other course work outside of class time.\nOften attend Peer Help or Student Hours (after preparing questions and working examples).\n\n\n\nProject\nTo finish off the course, you’ll complete a long-term project that showcases your interests and skills. You’ll be free to propose and pursue a topic. My expectation is that most projects will move significantly beyond the content covered in class in some way: you might implement a new algorithm, study a complex data set in depth, or conduct a series of experiments related to assessing algorithmic bias in a certain class of algorithm. You’ll be expected to complete this project in small groups (of your choosing), and update us at a few milestones along the way.\nPlease share a bit about what kind of topic might excite you, and set a few goals about how you plan to show up as a constructive team-member and co-inquirer (see the ideas for some inspiration).\n\nSubmit all project milestones (proposal, progress report, etc) on time.\nSet regular time each week to work with project partners.\nCommunicate with my group in a clear and timely manner.\nComplete all my designated work on time.\nDraft designated sections of the project report.\nRevise sections of the project report in response to feedback.\nTake the lead in creating and delivering part of the final project presentation.\nTake the lead in checking project figures for accuracy and clear labeling."
  },
  {
    "objectID": "posts/logistic-regression/logistic-regression-blogpost.html",
    "href": "posts/logistic-regression/logistic-regression-blogpost.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "Source code\n\n\n\n\nI implemented gradient descent in a for-loop where the maximum number of iterations is the max_epoch specified in the fit() method.\nIn each iteration (epoch), \\(w\\) is first updated:\n\nself.w -= alpha*self.gradient(self.w, X_, y)\n\nwhere the gradient() function computes the gradient of logistic loss associated with the previous \\(w\\).\nThen, we compute the loss and accuracy of the current \\(w\\) and pass them to corresponding variables.\nFinally, a if-statement within the for-loop checks if the current loss is close to the previous loss and breaks the loop if they are close (i.e. the minimum loss is found).\n\n\n\nFor fit_stochastic(), an update is performed in each iteration within the for-loop where a different set of random points is picked each time:\n\nfor batch in np.array_split(order, n // batch_size + 1):\n    ...\n    # gradient step\n    self.w -= alpha*self.gradient(self.w, x_batch, y_batch)\n\nThis for-loop is nested within another for-loop that represents epochs. After updates are performed in all batches, loss and accuracy for the current epoch is calculated and the current loss is checked against the previous loss to see if the minimum is reached.\n\n\n\n\n\n\nIn this example, gradient descent with \\(\\alpha\\) = 0.1 converged before the 1000 steps. However, using the same data set with \\(\\alpha\\) increased to 10, the loss kept oscillating and never converged. This demonstrates how gradient descent would fail to converge when the learning rate is too large.\n\nfrom logistic_regression import LogisticRegression\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\n# generate data\nX, y = make_blobs(n_samples = 200, n_features = 2, centers = [(0, 0), (1, 1)])\n\n# alpha = 0.1\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 0.1, max_epochs = 1000)\n\n# plot\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient (alpha = 0.1)\")\n\n# alpha = 10\nLR.fit(X, y, alpha = 10, max_epochs = 1000)\n\n# plot\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient (alpha = 10)\")\n\nplt.loglog()\n\nxlab = plt.xlabel(\"Number of steps\")\nylab = plt.ylabel(\"Loss\")\nlegend = plt.legend() \n\n\n\n\n\n\n\nHere, we perform stochastic gradient descent with 4 different batch sizes on a data set with 10 features. The smaller the batch size, the quicker the algorithm converges.\n\n# generate data\nX, y = make_blobs(n_samples = 200, n_features = 10, centers = [(0, 0), (1, 1)])\n\n# batch_size = 5\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 1000, batch_size = 5)\n\n# plot\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"batch_size = 5\")\n\n# batch_size = 10\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 1000, batch_size = 10)\n\n# plot\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"batch_size = 10\")\n\n# batch_size = 20\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 1000, batch_size = 10)\n\n# plot\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"batch_size = 20\")\n\n# batch_size = 50\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 1000, batch_size = 50)\n\n# plot\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"batch_size = 50\")\n\nplt.loglog()\n\nxlab = plt.xlabel(\"Number of steps\")\nylab = plt.ylabel(\"Loss\")\nlegend = plt.legend()"
  },
  {
    "objectID": "posts/penguins/penguins.html",
    "href": "posts/penguins/penguins.html",
    "title": "Penguins",
    "section": "",
    "text": "In this blog post, I will be training some machine learning models on the Palmer Penguins data set. The goal is to find a model trained with three of the features in the data set that achieves 100% testing accuracy.\n\n\nHere is an overview of the Palmer Penguins data set:\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\ntrain.head()\n\n\n\n\n\n  \n    \n      \n      studyName\n      Sample Number\n      Species\n      Region\n      Island\n      Stage\n      Individual ID\n      Clutch Completion\n      Date Egg\n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Flipper Length (mm)\n      Body Mass (g)\n      Sex\n      Delta 15 N (o/oo)\n      Delta 13 C (o/oo)\n      Comments\n    \n  \n  \n    \n      0\n      PAL0708\n      27\n      Gentoo penguin (Pygoscelis papua)\n      Anvers\n      Biscoe\n      Adult, 1 Egg Stage\n      N46A1\n      Yes\n      11/29/07\n      44.5\n      14.3\n      216.0\n      4100.0\n      NaN\n      7.96621\n      -25.69327\n      NaN\n    \n    \n      1\n      PAL0708\n      22\n      Gentoo penguin (Pygoscelis papua)\n      Anvers\n      Biscoe\n      Adult, 1 Egg Stage\n      N41A2\n      Yes\n      11/27/07\n      45.1\n      14.5\n      215.0\n      5000.0\n      FEMALE\n      7.63220\n      -25.46569\n      NaN\n    \n    \n      2\n      PAL0910\n      124\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Torgersen\n      Adult, 1 Egg Stage\n      N67A2\n      Yes\n      11/16/09\n      41.4\n      18.5\n      202.0\n      3875.0\n      MALE\n      9.59462\n      -25.42621\n      NaN\n    \n    \n      3\n      PAL0910\n      146\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Dream\n      Adult, 1 Egg Stage\n      N82A2\n      Yes\n      11/16/09\n      39.0\n      18.7\n      185.0\n      3650.0\n      MALE\n      9.22033\n      -26.03442\n      NaN\n    \n    \n      4\n      PAL0708\n      24\n      Chinstrap penguin (Pygoscelis antarctica)\n      Anvers\n      Dream\n      Adult, 1 Egg Stage\n      N85A2\n      No\n      11/28/07\n      50.6\n      19.4\n      193.0\n      3800.0\n      MALE\n      9.28153\n      -24.97134\n      NaN\n    \n  \n\n\n\n\nUpon intuition, I am guessing that the species of a penguin might be related to which island they live on and their physiological measures. To test my guess, I will make some tables and figures using these variables.\n\n\n\ntrain.groupby([\"Species\", \"Island\"])[[\"Island\"]].count()\n\n\n\n\n\n  \n    \n      \n      \n      Island\n    \n    \n      Species\n      Island\n      \n    \n  \n  \n    \n      Adelie Penguin (Pygoscelis adeliae)\n      Biscoe\n      35\n    \n    \n      Dream\n      41\n    \n    \n      Torgersen\n      42\n    \n    \n      Chinstrap penguin (Pygoscelis antarctica)\n      Dream\n      56\n    \n    \n      Gentoo penguin (Pygoscelis papua)\n      Biscoe\n      101\n    \n  \n\n\n\n\nThere is a clear pattern in the island that different species of penguins inhabit. Adelie is found on all three islands, whereas Chinstrap is only found on Dream and Gentoo is only found on Biscoe.\n\n\n\n\nimport seaborn as sns\n\nsns.set_theme()\n\nsns.relplot(data = train, \n            x = \"Culmen Length (mm)\", \n            y = \"Culmen Depth (mm)\", \n            hue = \"Flipper Length (mm)\",\n            style = \"Species\")\n\n<seaborn.axisgrid.FacetGrid at 0x7fd323a6f310>\n\n\n\n\n\nThis figure shows the relationship between three of the physiological measures of a penguin (culmen depth, culmen length, flipper length) and their species.\nThe three species form nice clusters on the graph. This tells us that we can do a good job predicting them using culmen depth and length, as we can separate species using the x and y coordinates easily.\nThere is also a pattern in the color of the clusters: Gentoo has the deepest colors, followed by Chinstrap. Therefore, flipper length is also likely correlated with species.\n\n\n\n\n\n\nI start by preparing the training data using the code provided by Professor Phil:\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\nX_train\n\n\n\n\n\n  \n    \n      \n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Flipper Length (mm)\n      Body Mass (g)\n      Delta 15 N (o/oo)\n      Delta 13 C (o/oo)\n      Island_Biscoe\n      Island_Dream\n      Island_Torgersen\n      Stage_Adult, 1 Egg Stage\n      Clutch Completion_No\n      Clutch Completion_Yes\n      Sex_FEMALE\n      Sex_MALE\n    \n  \n  \n    \n      1\n      45.1\n      14.5\n      215.0\n      5000.0\n      7.63220\n      -25.46569\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n    \n    \n      2\n      41.4\n      18.5\n      202.0\n      3875.0\n      9.59462\n      -25.42621\n      0\n      0\n      1\n      1\n      0\n      1\n      0\n      1\n    \n    \n      3\n      39.0\n      18.7\n      185.0\n      3650.0\n      9.22033\n      -26.03442\n      0\n      1\n      0\n      1\n      0\n      1\n      0\n      1\n    \n    \n      4\n      50.6\n      19.4\n      193.0\n      3800.0\n      9.28153\n      -24.97134\n      0\n      1\n      0\n      1\n      1\n      0\n      0\n      1\n    \n    \n      5\n      33.1\n      16.1\n      178.0\n      2900.0\n      9.04218\n      -26.15775\n      0\n      1\n      0\n      1\n      0\n      1\n      1\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      269\n      41.1\n      17.5\n      190.0\n      3900.0\n      8.94365\n      -26.06943\n      0\n      1\n      0\n      1\n      0\n      1\n      0\n      1\n    \n    \n      270\n      45.4\n      14.6\n      211.0\n      4800.0\n      8.24515\n      -25.46782\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n    \n    \n      271\n      36.2\n      17.2\n      187.0\n      3150.0\n      9.04296\n      -26.19444\n      0\n      0\n      1\n      1\n      1\n      0\n      1\n      0\n    \n    \n      272\n      50.0\n      15.9\n      224.0\n      5350.0\n      8.20042\n      -26.39677\n      1\n      0\n      0\n      1\n      0\n      1\n      0\n      1\n    \n    \n      273\n      48.2\n      14.3\n      210.0\n      4600.0\n      7.68870\n      -25.50811\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n    \n  \n\n256 rows × 14 columns\n\n\n\n\n\n\n\n\nIn the following for-loops, I train and score all possible combinations of 1 qualitative + 2 quantitative features using cross-validation. I choose to start with the logistic regression model.\n\nfrom warnings import simplefilter\nfrom sklearn.exceptions import ConvergenceWarning\nsimplefilter(\"ignore\", category=ConvergenceWarning)\n\nfrom itertools import combinations\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nimport numpy as np\n\n# these are not actually all the columns: you'll \n# need to add any of the other ones you want to search for\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island\", \"Stage\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)', \n                 'Delta 15 N (o/oo)', 'Delta 13 C (o/oo)']\n\ncv_max = 0;\nbest_features = [];\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = list(pair) + qual_cols\n    \n    # fit model\n    LR = LogisticRegression()\n    LR.fit(X_train[cols], y_train)\n    \n    # cross-validation\n    cv_scores = cross_val_score(LR, X_train[cols], y_train, cv = 5)\n    cv_mean = cv_scores.mean()\n    \n    # update top 3 cv scores and best features list\n    if cv_mean > cv_max:\n        cv_max = cv_mean\n        best_features = cols;\n    \n    \nprint(\"Best CV score = \", cv_max)\nprint(\"Best features = \", best_features)\n\nBest CV score =  0.996078431372549\nBest features =  ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n\n\nIt seems that the best cross-validation score we can get using logistic regression is 0.996.\nNext, I will try a few other machine learning models and see if we can get even higher scores. #### Decision tree\n\nfrom sklearn.tree import DecisionTreeClassifier\n\ncv_max = 0\nbest_features = []\nbest_depth = 0\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = list(pair) + qual_cols\n    \n    for d in range(2, 30):\n        # fit model\n        DTC = DecisionTreeClassifier(max_depth = d)\n        DTC.fit(X_train[cols], y_train)\n\n        # cross-validation\n        cv_scores = cross_val_score(DTC, X_train[cols], y_train, cv = 5)\n        cv_mean = cv_scores.mean()\n\n        # update top 3 cv scores and best features list\n        if cv_mean > cv_max:\n            cv_max = cv_mean\n            best_features = cols\n            best_depth = d\n    \n    \nprint(\"Best CV score = \", cv_max)\nprint(\"Best features = \", best_features)\nprint(\"Best max depth = \", best_depth)\n\nBest CV score =  0.9803921568627452\nBest features =  ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\nBest max depth =  4\n\n\nUsing the decision tree method, the best CV score we can achieve is 0.98, a little lower than with logistic regression. The best features are the same as what we got from searching through logistic regression models. The max depth that would allow the best CV score turns out to be 4.\n\n\n\n\nfrom sklearn.ensemble import RandomForestClassifier\n\ncv_max = 0;\nbest_features = [];\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = list(pair) + qual_cols\n    \n    # fit model\n    RF = RandomForestClassifier()\n    RF.fit(X_train[cols], y_train)\n    \n    # cross-validation\n    cv_scores = cross_val_score(RF, X_train[cols], y_train, cv = 5)\n    cv_mean = cv_scores.mean()\n    \n    # update top 3 cv scores and best features list\n    if cv_mean > cv_max:\n        cv_max = cv_mean\n        best_features = cols;\n    \n    \nprint(\"Best CV score = \", cv_max)\nprint(\"Best features = \", best_features)\n\nBest CV score =  0.9843137254901961\nBest features =  ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n\n\nWe again get the same set of best features from searching all possible random forest models. The best CV score is very close to what we got from the decision tree classifiers.\n\n\n\n\nfrom sklearn.svm import SVC\n\ncv_max = 0\nbest_features = []\nbest_gamma = 0\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = list(pair) + qual_cols\n    \n    for g in np.float_power(10, np.arange(-5, 5)):\n        # fit model\n        SVC_ = SVC(gamma = g)\n        SVC_.fit(X_train[cols], y_train)\n\n        # cross-validation\n        cv_scores = cross_val_score(SVC_, X_train[cols], y_train, cv = 5)\n        cv_mean = cv_scores.mean()\n\n        # update top 3 cv scores and best features list\n        if cv_mean > cv_max:\n            cv_max = cv_mean\n            best_features = cols\n            best_gamma = g\n    \n    \nprint(\"Best CV score = \", cv_max)\nprint(\"Best features = \", best_features)\nprint(\"Best gamma = \", best_gamma)\n\nBest CV score =  0.984389140271493\nBest features =  ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\nBest gamma =  0.1\n\n\nFinally, I search through all possible feature combinations and gamma values from \\(10^{-5}\\) to \\(10^5\\). The best model turns out to be trained with the same set of features as the rest of the models, with gamma = 0.1.\n\n\n\nIt seems that with any of the four models, the most predictive features are always culmen length, culmen depth, and island. I wonder if this applies to other machine learning models in general – is there always a set of “best” features to use regardless of the model choice?\nComparing the cross-validation scores of all four models, it looks like they have similar performance. Logistic regression is slightly outperforming the rest. Next, I will try applying the best candidates from each of the four model types to testing data and see if they have similar testing accuracy as well.\n\n\n\n\n\ntest_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\n\nLR = LogisticRegression()\nLR.fit(X_train[best_features], y_train)\nprint(\"Logistic regression: \", LR.score(X_test[best_features], y_test))\n      \nDTC = DecisionTreeClassifier(max_depth = best_depth)\nDTC.fit(X_train[best_features], y_train)\nprint(\"Decision tree: \", DTC.score(X_test[best_features], y_test))\n\nRF = RandomForestClassifier()\nRF.fit(X_train[best_features], y_train)\nprint(\"Random forest: \", RF.score(X_test[best_features], y_test))\n      \nSVC_ = SVC(gamma = best_gamma)\nSVC_.fit(X_train[best_features], y_train)\nprint(\"SVC: \", SVC_.score(X_test[best_features], y_test))\n\nLogistic regression:  1.0\nDecision tree:  0.9852941176470589\nRandom forest:  0.9852941176470589\nSVC:  0.9558823529411765\n\n\nWith the testing data, we reached 1.0 accuracy using logistic regression.\nThe decision tree and random forest models also had pretty good accuracy, close to what the cross-validation scores suggest.\nThe accuracy from SVC is a little lower than anticipated since the CV score from training data is higher by ~0.03.\n\n\n\n\nFinally, I will plot the decision regions of all four selected models. I will compare them and try to understand what makes logistic regression the best of them with this data set.\n\nfrom matplotlib import pyplot as plt\n\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\n\n\n\nplot_regions(LR, X_train[best_features], y_train)\nplt.show()\n\n\n\n\n\n\n\n\nplot_regions(DTC, X_train[best_features], y_train)\nplt.show()\n\n\n\n\n\n\n\n\nplot_regions(RF, X_train[best_features], y_train)\nplt.show()\n\n\n\n\n\n\n\n\nplot_regions(SVC_, X_train[best_features], y_train)\nplt.show()\n\n\n\n\n\n\n\nComparing the decision regions of different models, it seems that the errors of the last three models almost exclusively come from the separation between the two species on the second island. There are a few green points right next to the blue ones, which were only correctly classified by logistic regression.\nI can see how this creates a problem for decision tree and random forest – since they only create separation lines at two directions (along the x axis or y axis), it would be hard for them to be precise where the two clusters are extremely close to each other.\nFor the support vector classifier, however, I am curious about what makes it the least accurate, at least for this data set. Is this by nature of some characteristics of this model or just by chance?"
  }
]