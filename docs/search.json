[
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CSCI 0451 Blog - Diana Xu",
    "section": "",
    "text": "We reflect on our learning, engagement, and achievement in the first part of the semester.\n\n\n\n\n\n\nApr 5, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCreating a machine learning model to predict an individual characteristic based on demographic variables and performing a bias audit on the model.\n\n\n\n\n\n\nMar 29, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing linear regression.\n\n\n\n\n\n\nMar 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClassifying Palmer penguins.\n\n\n\n\n\n\nMar 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing logistic degression with gradient descent.\n\n\n\n\n\n\nMar 1, 2023\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/perceptron/perceptron-blogpost.html",
    "href": "posts/perceptron/perceptron-blogpost.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Source code\n\n\nI implemented the perceptron update with the following steps:\n\nModifying the feature matrix \\(X\\) and vector of labels \\(y\\). This includes appending a column of 1s to \\(X\\) and turning \\(y\\) into a vector of -1s and 1s.\nInitializing a random weight vector \\(w\\).\nIn a for-loop that breaks either when accuracy reaches 1 or when the specified max_steps is reached,\n\nPick a random data point with index \\(i\\) and access its features \\(x_{i}\\) and label \\(y_{i}\\);\nCompute the predicted label \\(\\hat{y}\\) using the current \\(w\\);\nPerform update if \\(y\\) * \\(\\hat{y}\\) < 0:\n\n\n\nself.w = self.w + (y_i*y_hat < 0) * (y_i * x_)\n\nWhere only when y_i * y_hat < 0 is (y_i * x_) added to the current w.\nFinally, the accuracy of the current \\(w\\) is computed by using \\(w\\) to predict labels for all data points, and then calculating the number of correct predictions compared to \\(y\\). This accuracy is then appended to the history array.\n\n\n\n\n\n\nfrom perceptron import Perceptron\nimport numpy as np\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\n\nn = 100\np_features = 3\n\n# generate random data\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\n# fit perceptron to data\np1 = Perceptron()\np1.fit(X, y, max_steps = 1000)\n\n# visualizations\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize = (8, 3)) \n\n# visualize accuracy history\nax1.plot(p1.history)\nax1.set(xlabel = \"Iteration\",\nylabel = \"Accuracy\")\n\n# visualize data points and perceptron line\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nax2.set(xlabel = \"Feature 1\",\nylabel = \"Feature 2\")\n\nax2 = plt.scatter(X[:,0], X[:,1], c = y)\nax2 = draw_line(p1.w, -2, 2)\n\n\n\n\nHere, the data is linearly separable. The accuracy is able to reach 1.0 after 125+ iterations. The perceptron line generated is able to separate data with labels -1 and 1.\n\n\n\n\n# generate random data\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\n# fit perceptron to data\np2 = Perceptron()\np2.fit(X, y, max_steps = 1000)\n\n# visualizations\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize = (8, 3)) \n\n# visualize accuracy history\nax1.plot(p2.history)\nax1.set(xlabel = \"Iteration\",\nylabel = \"Accuracy\")\n\n# visualize data points and perceptron line\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nax2.set(xlabel = \"Feature 1\",\nylabel = \"Feature 2\")\n\nax2 = plt.scatter(X[:,0], X[:,1], c = y)\nax2 = draw_line(p2.w, -2, 2)\n\n\n\n\nHere, the data is not linearly separable as there are overlapping points. The accuracy fails to reach 1.0 after 1000 steps. The perceptron line does not perfectly separate the two groups of data.\n\n\n\nHere I generate random data with 6 features and feed it to the perceptron algorithm.\n\n# change the number of features\np_features = 7\n\n# generate random data\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\n# fit perceptron to data\np3 = Perceptron()\np3.fit(X, y, max_steps = 1000)\n\n# visualize accuracy history\nfig = plt.plot(p3.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nSince the accuracy has reached 1.0, the data should be linearly separable.\n\n\n\n\nThe runtime complexity of a single iteration of the perceptron update should be \\(O(p)\\). It would depend on the number of features but not the number of data points."
  },
  {
    "objectID": "posts/reflective-goal-setting/goal-setting.html",
    "href": "posts/reflective-goal-setting/goal-setting.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Wen (Diana) Xu\n\n\nThe knowledge we’ll develop in CSCI 0451 can be broadly divided into four main areas:\n\nTheory: mathematical descriptions of frameworks and algorithms.\nImplementation: effective coding and use of tools in order to implement efficient machine learning algorithms.\nExperimentation: performing experiments to assess the performance of algorithms and clearly communicating about the results.\nSocial responsibility: critical analysis of sources of bias and harm in machine learning algorithms; theoretical formulations of fairness and bias\n\nEvery student should grow toward each of these areas, but you can choose to specialize if you’d like! If there are one or two of these categories on which you’d especially like to focus, list them below. Feel free to include any details that you’d like – this can help me tailor the course content to your interests.\nI would like to focus more on experimentation and social responsibility. I am interested in the applications and implications of machine learning algorithms – how to make use of machine learning to solve problems in the real world, how to assess the performance of the algorithms in relation to the specific problem, and how we can effectively communicate machine learning to a broader audience. I also hope to look more into bias in machine learning algorithms – what causes them and how to identify and reduce sources of bias.\n\n\n\n\n\nMost blog posts will require around 5-8 hours on average to complete, plus time for revisions in response to feedback. Blog posts will most frequently involve a mix of mathematical problem-solving, coding, experimentation, and written discussion. Some blog posts will ask you to critically discuss recent readings in an essay-like format.\n\nSubmit a blog post in most weeks during the semester.\nSubmit the first draft of no more than two blog post after the “best-by” date.\nRevise at least five blog posts to the “No Revisions Suggested” level.\nGo above and beyond in at least two blog posts by performing experiments/exploring visualization choices to communicate results/discussing implications significantly beyond what is required.\n\n\n\n\nYou make a choice each day about how to show up for class: whether you’ll be prepared, whether you’ll engage with me and your peers in a constructive manner; and whether you’ll be active during lecture and discussions. We will also have a special opportunity this semester to engage with a renowned expert in machine learning, algorithmic bias, and the ethics of artificial intelligence.\nAn especially important form of course presence is the daily warmup. We’ll spend the first 10-15 minutes of most class periods on warmup activities. You’re expected to have prepared the warmup activity ahead of time (this means you’ll need to have completed the readings as well). Each time, we’ll sort into groups of 5-6 students, and one of you (randomly selected) will be responsible for presenting the activity on the whiteboard. If you’re not feeling prepared to present the activity, you can “pass” to the next person, or ask for help along the way.\n\nComplete all core readings prior to each class periods.\nComplete the optional readings that correspond to my areas of specialization.\nBe prepared for most warmup activities even on days when I am not the leader.\n“Pass” at most once when asked to lead the warmup activity for my group.\nPropose questions ahead of time for our guest speaker.\nOften work with classmates together on blog posts or other course work outside of class time.\nOften attend Peer Help or Student Hours (after preparing questions and working examples).\n\n\n\n\nTo finish off the course, you’ll complete a long-term project that showcases your interests and skills. You’ll be free to propose and pursue a topic. My expectation is that most projects will move significantly beyond the content covered in class in some way: you might implement a new algorithm, study a complex data set in depth, or conduct a series of experiments related to assessing algorithmic bias in a certain class of algorithm. You’ll be expected to complete this project in small groups (of your choosing), and update us at a few milestones along the way.\nPlease share a bit about what kind of topic might excite you, and set a few goals about how you plan to show up as a constructive team-member and co-inquirer (see the ideas for some inspiration).\n\nSubmit all project milestones (proposal, progress report, etc) on time.\nSet regular time each week to work with project partners.\nCommunicate with my group in a clear and timely manner.\nComplete all my designated work on time.\nDraft designated sections of the project report.\nRevise sections of the project report in response to feedback.\nTake the lead in creating and delivering part of the final project presentation.\nTake the lead in checking project figures for accuracy and clear labeling."
  },
  {
    "objectID": "posts/reflective-goal-setting/goal-setting.html#what-youll-learn",
    "href": "posts/reflective-goal-setting/goal-setting.html#what-youll-learn",
    "title": "Reflective Goal-Setting",
    "section": "What You’ll Learn",
    "text": "What You’ll Learn\nThe knowledge we’ll develop in CSCI 0451 can be broadly divided into four main areas:\n\nTheory: mathematical descriptions of frameworks and algorithms.\nImplementation: effective coding and use of tools in order to implement efficient machine learning algorithms.\nExperimentation: performing experiments to assess the performance of algorithms and clearly communicating about the results.\nSocial responsibility: critical analysis of sources of bias and harm in machine learning algorithms; theoretical formulations of fairness and bias\n\nEvery student should grow toward each of these areas, but you can choose to specialize if you’d like! If there are one or two of these categories on which you’d especially like to focus, list them below. Feel free to include any details that you’d like – this can help me tailor the course content to your interests.\nI would like to focus more on experimentation and social responsibility. I am interested in the applications and implications of machine learning algorithms – how to make use of machine learning to solve problems in the real world, how to assess the performance of the algorithms in relation to the specific problem, and how we can effectively communicate machine learning to a broader audience. I also hope to look more into bias in machine learning algorithms – what causes them and how to identify and reduce sources of bias."
  },
  {
    "objectID": "posts/reflective-goal-setting/goal-setting.html#what-youll-achieve",
    "href": "posts/reflective-goal-setting/goal-setting.html#what-youll-achieve",
    "title": "Reflective Goal-Setting",
    "section": "What You’ll Achieve",
    "text": "What You’ll Achieve\n\nBlog Posts\nMost blog posts will require around 5-8 hours on average to complete, plus time for revisions in response to feedback. Blog posts will most frequently involve a mix of mathematical problem-solving, coding, experimentation, and written discussion. Some blog posts will ask you to critically discuss recent readings in an essay-like format.\n\nSubmit a blog post in most weeks during the semester.\nSubmit the first draft of no more than two blog post after the “best-by” date.\nRevise at least five blog posts to the “No Revisions Suggested” level.\nGo above and beyond in at least two blog posts by performing experiments/exploring visualization choices to communicate results/discussing implications significantly beyond what is required.\n\n\n\nCourse Presence (Participation)\nYou make a choice each day about how to show up for class: whether you’ll be prepared, whether you’ll engage with me and your peers in a constructive manner; and whether you’ll be active during lecture and discussions. We will also have a special opportunity this semester to engage with a renowned expert in machine learning, algorithmic bias, and the ethics of artificial intelligence.\nAn especially important form of course presence is the daily warmup. We’ll spend the first 10-15 minutes of most class periods on warmup activities. You’re expected to have prepared the warmup activity ahead of time (this means you’ll need to have completed the readings as well). Each time, we’ll sort into groups of 5-6 students, and one of you (randomly selected) will be responsible for presenting the activity on the whiteboard. If you’re not feeling prepared to present the activity, you can “pass” to the next person, or ask for help along the way.\n\nComplete all core readings prior to each class periods.\nComplete the optional readings that correspond to my areas of specialization.\nBe prepared for most warmup activities even on days when I am not the leader.\n“Pass” at most once when asked to lead the warmup activity for my group.\nPropose questions ahead of time for our guest speaker.\nOften work with classmates together on blog posts or other course work outside of class time.\nOften attend Peer Help or Student Hours (after preparing questions and working examples).\n\n\n\nProject\nTo finish off the course, you’ll complete a long-term project that showcases your interests and skills. You’ll be free to propose and pursue a topic. My expectation is that most projects will move significantly beyond the content covered in class in some way: you might implement a new algorithm, study a complex data set in depth, or conduct a series of experiments related to assessing algorithmic bias in a certain class of algorithm. You’ll be expected to complete this project in small groups (of your choosing), and update us at a few milestones along the way.\nPlease share a bit about what kind of topic might excite you, and set a few goals about how you plan to show up as a constructive team-member and co-inquirer (see the ideas for some inspiration).\n\nSubmit all project milestones (proposal, progress report, etc) on time.\nSet regular time each week to work with project partners.\nCommunicate with my group in a clear and timely manner.\nComplete all my designated work on time.\nDraft designated sections of the project report.\nRevise sections of the project report in response to feedback.\nTake the lead in creating and delivering part of the final project presentation.\nTake the lead in checking project figures for accuracy and clear labeling."
  },
  {
    "objectID": "posts/logistic-regression/logistic-regression-blogpost.html",
    "href": "posts/logistic-regression/logistic-regression-blogpost.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "Source code\n\n\n\n\nI implemented gradient descent in a for-loop where the maximum number of iterations is the max_epoch specified in the fit() method.\nIn each iteration (epoch), \\(w\\) is first updated:\n\nself.w -= alpha*self.gradient(self.w, X_, y)\n\nwhere the gradient() function computes the gradient of logistic loss associated with the previous \\(w\\).\nThen, we compute the loss and accuracy of the current \\(w\\) and pass them to corresponding variables.\nFinally, a if-statement within the for-loop checks if the current loss is close to the previous loss and breaks the loop if they are close (i.e. the minimum loss is found).\n\n\n\nFor fit_stochastic(), an update is performed in each iteration within the for-loop where a different set of random points is picked each time:\n\nfor batch in np.array_split(order, n // batch_size + 1):\n    ...\n    # gradient step\n    self.w -= alpha*self.gradient(self.w, x_batch, y_batch)\n\nThis for-loop is nested within another for-loop that represents epochs. After updates are performed in all batches, loss and accuracy for the current epoch is calculated and the current loss is checked against the previous loss to see if the minimum is reached.\n\n\n\n\n\n\nFirst, I will test my implementation on a simple dataset, using the code from the demo on the assignment page, to make sure it works as we expect.\n\nfrom mlxtend.plotting import plot_decision_regions\n\nnp.seterr(all='ignore') \n\n# make the data\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n# fit the model\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 0.1, max_epochs = 1000)\n\n# show score\nprint(LR.score(X, y))\n\n# plot decision regions\nplot_decision_regions(X, y, clf = LR)\n\n0.93\n\n\n<AxesSubplot:xlabel='Feature 1', ylabel='Feature 2'>\n\n\n\n\n\nThe model has a score of 0.93 and classifies the data quite accurately according to the graph.\n\n\n\n\nIn this example, gradient descent with \\(\\alpha\\) = 0.1 converged before the 1000 steps. However, using the same data set with \\(\\alpha\\) increased to 10, the loss kept oscillating and never converged. This demonstrates how gradient descent would fail to converge when the learning rate is too large.\n\nfrom logistic_regression import LogisticRegression\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\n# generate data\nX, y = make_blobs(n_samples = 200, n_features = 2, centers = [(0, 0), (1, 1)])\n\n# alpha = 0.1\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 0.1, max_epochs = 1000)\n\n# plot\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient (alpha = 0.1)\")\n\n# alpha = 10\nLR.fit(X, y, alpha = 10, max_epochs = 1000)\n\n# plot\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient (alpha = 10)\")\n\nplt.loglog()\n\nxlab = plt.xlabel(\"Number of steps\")\nylab = plt.ylabel(\"Loss\")\nlegend = plt.legend() \n\n\n\n\n\n\n\nHere, we perform stochastic gradient descent with 4 different batch sizes on a data set with 10 features. The smaller the batch size, the quicker the algorithm converges.\n\n# generate data\nX, y = make_blobs(n_samples = 200, n_features = 10, centers = [(0, 0), (1, 1)])\n\n# batch_size = 5\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 1000, batch_size = 5)\n\n# plot\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"batch_size = 5\")\n\n# batch_size = 10\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 1000, batch_size = 10)\n\n# plot\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"batch_size = 10\")\n\n# batch_size = 20\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 1000, batch_size = 10)\n\n# plot\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"batch_size = 20\")\n\n# batch_size = 50\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 1000, batch_size = 50)\n\n# plot\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"batch_size = 50\")\n\nplt.loglog()\n\nxlab = plt.xlabel(\"Number of steps\")\nylab = plt.ylabel(\"Loss\")\nlegend = plt.legend()"
  },
  {
    "objectID": "posts/penguins/penguins.html",
    "href": "posts/penguins/penguins.html",
    "title": "Penguins",
    "section": "",
    "text": "In this blog post, I will be training some machine learning models on the Palmer Penguins data set. The goal is to find a model trained with three of the features in the data set that achieves 100% testing accuracy.\n\n\nHere is an overview of the Palmer Penguins data set:\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\ntrain.head()\n\n\n\n\n\n  \n    \n      \n      studyName\n      Sample Number\n      Species\n      Region\n      Island\n      Stage\n      Individual ID\n      Clutch Completion\n      Date Egg\n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Flipper Length (mm)\n      Body Mass (g)\n      Sex\n      Delta 15 N (o/oo)\n      Delta 13 C (o/oo)\n      Comments\n    \n  \n  \n    \n      0\n      PAL0708\n      27\n      Gentoo penguin (Pygoscelis papua)\n      Anvers\n      Biscoe\n      Adult, 1 Egg Stage\n      N46A1\n      Yes\n      11/29/07\n      44.5\n      14.3\n      216.0\n      4100.0\n      NaN\n      7.96621\n      -25.69327\n      NaN\n    \n    \n      1\n      PAL0708\n      22\n      Gentoo penguin (Pygoscelis papua)\n      Anvers\n      Biscoe\n      Adult, 1 Egg Stage\n      N41A2\n      Yes\n      11/27/07\n      45.1\n      14.5\n      215.0\n      5000.0\n      FEMALE\n      7.63220\n      -25.46569\n      NaN\n    \n    \n      2\n      PAL0910\n      124\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Torgersen\n      Adult, 1 Egg Stage\n      N67A2\n      Yes\n      11/16/09\n      41.4\n      18.5\n      202.0\n      3875.0\n      MALE\n      9.59462\n      -25.42621\n      NaN\n    \n    \n      3\n      PAL0910\n      146\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Dream\n      Adult, 1 Egg Stage\n      N82A2\n      Yes\n      11/16/09\n      39.0\n      18.7\n      185.0\n      3650.0\n      MALE\n      9.22033\n      -26.03442\n      NaN\n    \n    \n      4\n      PAL0708\n      24\n      Chinstrap penguin (Pygoscelis antarctica)\n      Anvers\n      Dream\n      Adult, 1 Egg Stage\n      N85A2\n      No\n      11/28/07\n      50.6\n      19.4\n      193.0\n      3800.0\n      MALE\n      9.28153\n      -24.97134\n      NaN\n    \n  \n\n\n\n\nUpon intuition, I am guessing that the species of a penguin might be related to which island they live on and their physiological measures. To test my guess, I will make some tables and figures using these variables.\n\n\n\ntrain.groupby([\"Species\", \"Island\"])[[\"Island\"]].count()\n\n\n\n\n\n  \n    \n      \n      \n      Island\n    \n    \n      Species\n      Island\n      \n    \n  \n  \n    \n      Adelie Penguin (Pygoscelis adeliae)\n      Biscoe\n      35\n    \n    \n      Dream\n      41\n    \n    \n      Torgersen\n      42\n    \n    \n      Chinstrap penguin (Pygoscelis antarctica)\n      Dream\n      56\n    \n    \n      Gentoo penguin (Pygoscelis papua)\n      Biscoe\n      101\n    \n  \n\n\n\n\nThere is a clear pattern in the island that different species of penguins inhabit. Adelie is found on all three islands, whereas Chinstrap is only found on Dream and Gentoo is only found on Biscoe.\n\n\n\n\nimport seaborn as sns\n\nsns.set_theme()\n\nsns.relplot(data = train, \n            x = \"Culmen Length (mm)\", \n            y = \"Culmen Depth (mm)\", \n            hue = \"Flipper Length (mm)\",\n            style = \"Species\")\n\n<seaborn.axisgrid.FacetGrid at 0x7fd323a6f310>\n\n\n\n\n\nThis figure shows the relationship between three of the physiological measures of a penguin (culmen depth, culmen length, flipper length) and their species.\nThe three species form nice clusters on the graph. This tells us that we can do a good job predicting them using culmen depth and length, as we can separate species using the x and y coordinates easily.\nThere is also a pattern in the color of the clusters: Gentoo has the deepest colors, followed by Chinstrap. Therefore, flipper length is also likely correlated with species.\n\n\n\n\n\n\nI start by preparing the training data using the code provided by Professor Phil:\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\nX_train\n\n\n\n\n\n  \n    \n      \n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Flipper Length (mm)\n      Body Mass (g)\n      Delta 15 N (o/oo)\n      Delta 13 C (o/oo)\n      Island_Biscoe\n      Island_Dream\n      Island_Torgersen\n      Stage_Adult, 1 Egg Stage\n      Clutch Completion_No\n      Clutch Completion_Yes\n      Sex_FEMALE\n      Sex_MALE\n    \n  \n  \n    \n      1\n      45.1\n      14.5\n      215.0\n      5000.0\n      7.63220\n      -25.46569\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n    \n    \n      2\n      41.4\n      18.5\n      202.0\n      3875.0\n      9.59462\n      -25.42621\n      0\n      0\n      1\n      1\n      0\n      1\n      0\n      1\n    \n    \n      3\n      39.0\n      18.7\n      185.0\n      3650.0\n      9.22033\n      -26.03442\n      0\n      1\n      0\n      1\n      0\n      1\n      0\n      1\n    \n    \n      4\n      50.6\n      19.4\n      193.0\n      3800.0\n      9.28153\n      -24.97134\n      0\n      1\n      0\n      1\n      1\n      0\n      0\n      1\n    \n    \n      5\n      33.1\n      16.1\n      178.0\n      2900.0\n      9.04218\n      -26.15775\n      0\n      1\n      0\n      1\n      0\n      1\n      1\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      269\n      41.1\n      17.5\n      190.0\n      3900.0\n      8.94365\n      -26.06943\n      0\n      1\n      0\n      1\n      0\n      1\n      0\n      1\n    \n    \n      270\n      45.4\n      14.6\n      211.0\n      4800.0\n      8.24515\n      -25.46782\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n    \n    \n      271\n      36.2\n      17.2\n      187.0\n      3150.0\n      9.04296\n      -26.19444\n      0\n      0\n      1\n      1\n      1\n      0\n      1\n      0\n    \n    \n      272\n      50.0\n      15.9\n      224.0\n      5350.0\n      8.20042\n      -26.39677\n      1\n      0\n      0\n      1\n      0\n      1\n      0\n      1\n    \n    \n      273\n      48.2\n      14.3\n      210.0\n      4600.0\n      7.68870\n      -25.50811\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n    \n  \n\n256 rows × 14 columns\n\n\n\n\n\n\n\n\nIn the following for-loops, I train and score all possible combinations of 1 qualitative + 2 quantitative features using cross-validation. I choose to start with the logistic regression model.\n\nfrom warnings import simplefilter\nfrom sklearn.exceptions import ConvergenceWarning\nsimplefilter(\"ignore\", category=ConvergenceWarning)\n\nfrom itertools import combinations\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nimport numpy as np\n\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island\", \"Stage\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)', \n                 'Delta 15 N (o/oo)', 'Delta 13 C (o/oo)']\n\ncv_max = 0;\nbest_features = [];\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = list(pair) + qual_cols\n    \n    # fit model\n    LR = LogisticRegression()\n    LR.fit(X_train[cols], y_train)\n    \n    # cross-validation\n    cv_scores = cross_val_score(LR, X_train[cols], y_train, cv = 5)\n    cv_mean = cv_scores.mean()\n    \n    # update best cv scores and best features list\n    if cv_mean > cv_max:\n        cv_max = cv_mean\n        best_features = cols\n    \n    \nprint(\"Best CV score = \", cv_max)\nprint(\"Best features = \", best_features)\n\nBest CV score =  0.996078431372549\nBest features =  ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n\n\nIt seems that the best cross-validation score we can get using logistic regression is 0.996.\nNext, I will try a few other machine learning models and see if we can get even higher scores. #### Decision tree\n\nfrom sklearn.tree import DecisionTreeClassifier\n\ncv_max = 0\nbest_features = []\nbest_depth = 0\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = list(pair) + qual_cols\n    \n    for d in range(2, 30):\n        # fit model\n        DTC = DecisionTreeClassifier(max_depth = d)\n        DTC.fit(X_train[cols], y_train)\n\n        # cross-validation\n        cv_scores = cross_val_score(DTC, X_train[cols], y_train, cv = 5)\n        cv_mean = cv_scores.mean()\n\n        # update top 3 cv scores and best features list\n        if cv_mean > cv_max:\n            cv_max = cv_mean\n            best_features = cols\n            best_depth = d\n    \n    \nprint(\"Best CV score = \", cv_max)\nprint(\"Best features = \", best_features)\nprint(\"Best max depth = \", best_depth)\n\nBest CV score =  0.9803921568627452\nBest features =  ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\nBest max depth =  4\n\n\nUsing the decision tree method, the best CV score we can achieve is 0.98, a little lower than with logistic regression. The best features are the same as what we got from searching through logistic regression models. The max depth that would allow the best CV score turns out to be 4.\n\n\n\n\nfrom sklearn.ensemble import RandomForestClassifier\n\ncv_max = 0;\nbest_features = [];\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = list(pair) + qual_cols\n    \n    # fit model\n    RF = RandomForestClassifier()\n    RF.fit(X_train[cols], y_train)\n    \n    # cross-validation\n    cv_scores = cross_val_score(RF, X_train[cols], y_train, cv = 5)\n    cv_mean = cv_scores.mean()\n    \n    # update top 3 cv scores and best features list\n    if cv_mean > cv_max:\n        cv_max = cv_mean\n        best_features = cols;\n    \n    \nprint(\"Best CV score = \", cv_max)\nprint(\"Best features = \", best_features)\n\nBest CV score =  0.9843137254901961\nBest features =  ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n\n\nWe again get the same set of best features from searching all possible random forest models. The best CV score is very close to what we got from the decision tree classifiers.\n\n\n\n\nfrom sklearn.svm import SVC\n\ncv_max = 0\nbest_features = []\nbest_gamma = 0\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = list(pair) + qual_cols\n    \n    for g in np.float_power(10, np.arange(-5, 5)):\n        # fit model\n        SVC_ = SVC(gamma = g)\n        SVC_.fit(X_train[cols], y_train)\n\n        # cross-validation\n        cv_scores = cross_val_score(SVC_, X_train[cols], y_train, cv = 5)\n        cv_mean = cv_scores.mean()\n\n        # update top 3 cv scores and best features list\n        if cv_mean > cv_max:\n            cv_max = cv_mean\n            best_features = cols\n            best_gamma = g\n    \n    \nprint(\"Best CV score = \", cv_max)\nprint(\"Best features = \", best_features)\nprint(\"Best gamma = \", best_gamma)\n\nBest CV score =  0.984389140271493\nBest features =  ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\nBest gamma =  0.1\n\n\nFinally, I search through all possible feature combinations and gamma values from \\(10^{-5}\\) to \\(10^5\\). The best model turns out to be trained with the same set of features as the rest of the models, with gamma = 0.1.\n\n\n\nIt seems that with any of the four models, the most predictive features are always culmen length, culmen depth, and island. I wonder if this applies to other machine learning models in general – is there always a set of “best” features to use regardless of the model choice?\nComparing the cross-validation scores of all four models, it looks like they have similar performance. Logistic regression is slightly outperforming the rest. Next, I will try applying the best candidates from each of the four model types to testing data and see if they have similar testing accuracy as well.\n\n\n\n\n\ntest_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\n\nLR = LogisticRegression()\nLR.fit(X_train[best_features], y_train)\nprint(\"Logistic regression: \", LR.score(X_test[best_features], y_test))\n      \nDTC = DecisionTreeClassifier(max_depth = best_depth)\nDTC.fit(X_train[best_features], y_train)\nprint(\"Decision tree: \", DTC.score(X_test[best_features], y_test))\n\nRF = RandomForestClassifier()\nRF.fit(X_train[best_features], y_train)\nprint(\"Random forest: \", RF.score(X_test[best_features], y_test))\n      \nSVC_ = SVC(gamma = best_gamma)\nSVC_.fit(X_train[best_features], y_train)\nprint(\"SVC: \", SVC_.score(X_test[best_features], y_test))\n\nLogistic regression:  1.0\nDecision tree:  0.9852941176470589\nRandom forest:  0.9852941176470589\nSVC:  0.9558823529411765\n\n\nWith the testing data, we reached 1.0 accuracy using logistic regression.\nThe decision tree and random forest models also had pretty good accuracy, close to what the cross-validation scores suggest.\nThe accuracy from SVC is a little lower than anticipated since the CV score from training data is higher by ~0.03.\n\n\n\n\nFinally, I will plot the decision regions of all four selected models. I will compare them and try to understand what makes logistic regression the best of them with this data set.\n\nfrom matplotlib import pyplot as plt\n\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\n\n\n\nplot_regions(LR, X_train[best_features], y_train)\nplt.show()\n\n\n\n\n\n\n\n\nplot_regions(DTC, X_train[best_features], y_train)\nplt.show()\n\n\n\n\n\n\n\n\nplot_regions(RF, X_train[best_features], y_train)\nplt.show()\n\n\n\n\n\n\n\n\nplot_regions(SVC_, X_train[best_features], y_train)\nplt.show()\n\n\n\n\n\n\n\nComparing the decision regions of different models, it seems that the errors of the last three models almost exclusively come from the separation between the two species on the second island. There are a few green points right next to the blue ones, which were only correctly classified by logistic regression.\nI can see how this creates a problem for decision tree and random forest – since they only create separation lines at two directions (along the x axis or y axis), it would be hard for them to be precise where the two clusters are extremely close to each other.\nFor the support vector classifier, however, I am curious about what makes it the least accurate, at least for this data set. Is this by nature of some characteristics of this model or just by chance?"
  },
  {
    "objectID": "posts/linear-regression/linear-regression-blogpost.html",
    "href": "posts/linear-regression/linear-regression-blogpost.html",
    "title": "Linear Regression",
    "section": "",
    "text": "Source code\n\n\n\n\nThe first way to implement linear regression is by using the analytical formula that gives us the \\(\\hat{w}\\) to minimize \\(L(w)\\):\n\nself.w = np.linalg.inv(X_.T@X_)@X_.T@y\n\n\n\n\nWe could also use the regular gradient descent to compute \\(\\hat{w}\\). To reduce time complexity, we precompute P = X_.T@X and q = X.T@y, and then pass them to gradient() in the for-loop where we update the gradient:\n\nself.w -= 2*alpha*self.gradient(P, q, self.w)\n\n\n\n\n\n\n\nIn this experiment, I explore how the training score and validation score change as the number of features increases.\n\nfrom linear_regression import LinearRegression\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\n# functions for data generation\ndef pad(X):\n    return np.append(X, np.ones((X.shape[0], 1)), 1)\n\ndef LR_data(n_train = 100, n_val = 100, p_features = 1, noise = .1, w = None):\n    if w is None: \n        w = np.random.rand(p_features + 1) + .2\n    \n    X_train = np.random.rand(n_train, p_features)\n    y_train = pad(X_train)@w + noise*np.random.randn(n_train)\n\n    X_val = np.random.rand(n_val, p_features)\n    y_val = pad(X_val)@w + noise*np.random.randn(n_val)\n    \n    return X_train, y_train, X_val, y_val\n\nI increase the number of features used in the model from 1 to n_train-1 and plot the change in training and validation scores.\n\n# generate data\nn_train = 100\nn_val = 100\nnoise = 0.2\ntrain_scores = []\nval_scores = []\n\n# increase p_features from 1 to n_train-1 and calculate training and validation scores for each\nfor p_features in np.arange(1, n_train): \n    # create data\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n    LR = LinearRegression()\n    LR.fit(X_train, y_train)\n    train_scores.append(LR.score(X_train, y_train))\n    val_scores.append(LR.score(X_val, y_val))\n\n\n# plot\nplt.plot(np.arange(1, n_train), train_scores, label = \"Training scores\")\n\n# plot\nplt.plot(np.arange(1, n_train), val_scores, label = \"Validation scores\")\n\nxlab = plt.xlabel(\"Number of features\")\nylab = plt.ylabel(\"Score\")\nlegend = plt.legend() \n\n\n\n\nWe can observe from the chart that the training score increased all the way to 1.0 as the number of features increases. The validation score, however, has been fluctuating and forms a slightly downward trend, and dramatically decreased to almost 0 when the number of features reached ~99.\nThis is a demonstration of overfitting. With too many features, the model becomes increasingly accurate in describing the trend in the training data, but at the same time takes into account more noise from the training data that doesn’t generate to the rest of the data. As a result, validation scores decrease.\n\n\n\nTo fix overfitting, I experiment with LASSO regularization:\n\nfrom sklearn.linear_model import Lasso\n\n# generate data\nn_train = 100\nn_val = 100\nnoise = 0.2\ntrain_scores = []\nval_scores = []\n\n# increase p_features from 1 to n_train-1 and calculate training and validation scores for each\nfor p_features in np.arange(1, n_train + 10): \n    # create data\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n    L = Lasso(alpha = 0.001)\n    L.fit(X_train, y_train)\n    train_scores.append(L.score(X_train, y_train))\n    val_scores.append(L.score(X_val, y_val))\n\n\n# plot\nplt.plot(np.arange(1, n_train + 10), train_scores, label = \"Training scores\")\nplt.plot(np.arange(1, n_train + 10), val_scores, label = \"Validation scores\")\n\nxlab = plt.xlabel(\"Number of features\")\nylab = plt.ylabel(\"Score\")\nlegend = plt.legend() \n\n\n\n\nUsing LASSO regularization, the validation scores still drops as the number of features increases, but there is no dramatical decrease as the number of features approaches, or even exceeds the number of data points. This is because LASSO is able to force entries of the weight vector to zero, which can help eliminate the effect of features that act as noise to the model.\n\n\n\n\nIn this section, I train my linear regression model to a bikeshare data set.\n\n# import data\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nbikeshare = pd.read_csv(\"https://philchodrow.github.io/PIC16A/datasets/Bike-Sharing-Dataset/day.csv\")\n\nbikeshare.head()\n\n\n\n\n\n  \n    \n      \n      instant\n      dteday\n      season\n      yr\n      mnth\n      holiday\n      weekday\n      workingday\n      weathersit\n      temp\n      atemp\n      hum\n      windspeed\n      casual\n      registered\n      cnt\n    \n  \n  \n    \n      0\n      1\n      2011-01-01\n      1\n      0\n      1\n      0\n      6\n      0\n      2\n      0.344167\n      0.363625\n      0.805833\n      0.160446\n      331\n      654\n      985\n    \n    \n      1\n      2\n      2011-01-02\n      1\n      0\n      1\n      0\n      0\n      0\n      2\n      0.363478\n      0.353739\n      0.696087\n      0.248539\n      131\n      670\n      801\n    \n    \n      2\n      3\n      2011-01-03\n      1\n      0\n      1\n      0\n      1\n      1\n      1\n      0.196364\n      0.189405\n      0.437273\n      0.248309\n      120\n      1229\n      1349\n    \n    \n      3\n      4\n      2011-01-04\n      1\n      0\n      1\n      0\n      2\n      1\n      1\n      0.200000\n      0.212122\n      0.590435\n      0.160296\n      108\n      1454\n      1562\n    \n    \n      4\n      5\n      2011-01-05\n      1\n      0\n      1\n      0\n      3\n      1\n      1\n      0.226957\n      0.229270\n      0.436957\n      0.186900\n      82\n      1518\n      1600\n    \n  \n\n\n\n\n\n# plot the number of casual users over time\nfig, ax = plt.subplots(1, figsize = (7, 3))\nax.plot(pd.to_datetime(bikeshare['dteday']), bikeshare['casual'])\nax.set(xlabel = \"Day\", ylabel = \"# of casual users\")\nl = plt.tight_layout()\n\n\n\n\n\n# transforming data\ncols = [\"casual\", \n        \"mnth\", \n        \"weathersit\", \n        \"workingday\",\n        \"yr\",\n        \"temp\", \n        \"hum\", \n        \"windspeed\",\n        \"holiday\"]\n\nbikeshare = bikeshare[cols]\n\nbikeshare = pd.get_dummies(bikeshare, columns = ['mnth'], drop_first = \"if_binary\")\nbikeshare\n\n\n\n\n\n  \n    \n      \n      casual\n      weathersit\n      workingday\n      yr\n      temp\n      hum\n      windspeed\n      holiday\n      mnth_2\n      mnth_3\n      mnth_4\n      mnth_5\n      mnth_6\n      mnth_7\n      mnth_8\n      mnth_9\n      mnth_10\n      mnth_11\n      mnth_12\n    \n  \n  \n    \n      0\n      331\n      2\n      0\n      0\n      0.344167\n      0.805833\n      0.160446\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      1\n      131\n      2\n      0\n      0\n      0.363478\n      0.696087\n      0.248539\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      2\n      120\n      1\n      1\n      0\n      0.196364\n      0.437273\n      0.248309\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      3\n      108\n      1\n      1\n      0\n      0.200000\n      0.590435\n      0.160296\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      4\n      82\n      1\n      1\n      0\n      0.226957\n      0.436957\n      0.186900\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      726\n      247\n      2\n      1\n      1\n      0.254167\n      0.652917\n      0.350133\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n    \n    \n      727\n      644\n      2\n      1\n      1\n      0.253333\n      0.590000\n      0.155471\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n    \n    \n      728\n      159\n      2\n      0\n      1\n      0.253333\n      0.752917\n      0.124383\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n    \n    \n      729\n      364\n      1\n      0\n      1\n      0.255833\n      0.483333\n      0.350754\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n    \n    \n      730\n      439\n      2\n      1\n      1\n      0.215833\n      0.577500\n      0.154846\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n    \n  \n\n731 rows × 19 columns\n\n\n\n\n# train-test split\ntrain, test = train_test_split(bikeshare, test_size = .2, shuffle = False)\n\nX_train = train.drop([\"casual\"], axis = 1)\ny_train = train[\"casual\"]\n\nX_test = test.drop([\"casual\"], axis = 1)\ny_test = test[\"casual\"]\n\n\n# fit and score the model\nLR = LinearRegression()\nLR.fit(X_train, y_train)\nLR.score(X_train, y_train)\n\n0.7318355359284503\n\n\nThe model has a score of 0.73.\n\n# compute predictions and visualize in comparison to actual test data\ny_hat = LR.predict(X_test)\n\nplt.plot(np.arange(len(y_hat)), y_hat, label = \"Predictions\")\nplt.plot(np.arange(len(y_hat)), y_test, label = \"Test data\")\n         \nxlab = plt.xlabel(\"Day\")\nylab = plt.ylabel(\"Number of casual users\")\nlegend = plt.legend() \n\n\n\n\nIt seems that the model does a good job in predicting the general trend in the data – the overall decreasing number of users and the timing of peaks. However, it tends to underestimate the minimum and maximum values.\nFinally, I will look into the weight vector and see what it reveals about people’s preference of when to use bikeshare.\n\n# compare weight vector to list of features\nfeature_weights = pd.DataFrame(LR.w[:-1], X_train.columns)\nfeature_weights\n\n\n\n\n\n  \n    \n      \n      0\n    \n  \n  \n    \n      weathersit\n      -108.371136\n    \n    \n      workingday\n      -791.690549\n    \n    \n      yr\n      280.586927\n    \n    \n      temp\n      1498.715113\n    \n    \n      hum\n      -490.100340\n    \n    \n      windspeed\n      -1242.800381\n    \n    \n      holiday\n      -235.879349\n    \n    \n      mnth_2\n      -3.354397\n    \n    \n      mnth_3\n      369.271956\n    \n    \n      mnth_4\n      518.408753\n    \n    \n      mnth_5\n      537.301886\n    \n    \n      mnth_6\n      360.807998\n    \n    \n      mnth_7\n      228.881481\n    \n    \n      mnth_8\n      241.316412\n    \n    \n      mnth_9\n      371.503854\n    \n    \n      mnth_10\n      437.600848\n    \n    \n      mnth_11\n      252.433004\n    \n    \n      mnth_12\n      90.821460\n    \n  \n\n\n\n\nWe can observe a couple of interesting trends from the magnitude and direction of the weights: 1. People use bikeshare more when the weather is nicer – especially when it is warmer and there is less wind. It is also interesting, though, that the weathersit variable is slightly negatively correlated with the number of users. I wonder what exactly this variable is measuring. 2. People use bikeshare the most in April and May. This makes sense because weather is the best during this time of the year, while summer is too hot and winter is too cold. 3. People use bikeshare more on weekends but less on holidays."
  },
  {
    "objectID": "posts/auditing-allocative-bias/auditing-allocative-bias-blogpost.html",
    "href": "posts/auditing-allocative-bias/auditing-allocative-bias-blogpost.html",
    "title": "Auditing Allocative Bias",
    "section": "",
    "text": "In this blogpost, I will train a model to predict income based on demographic variables excluding sex, and perform audit for gender bias on the model.\n\n\nThe data I choose to look into is the PUMS data in for Illinois in 2019.\n\nfrom folktables import ACSDataSource, ACSEmployment, BasicProblem, adult_filter\nimport numpy as np\n\nSTATE = \"IL\"\n\ndata_source = ACSDataSource(survey_year='2019', \n                            horizon='1-Year', \n                            survey='person')\n\nacs_data = data_source.get_data(states=[STATE], download=True)\npossible_features=['AGEP', 'SCHL', 'MAR', 'DIS', 'ESP', 'CIT', 'MIG', 'MIL', 'ANC', 'NATIVITY', 'DEAR', 'DEYE', 'DREM', 'SEX', 'RAC1P', 'ESR', \"PINCP\"]\nacs_data[possible_features].head()\n\n\n\n\n\n  \n    \n      \n      AGEP\n      SCHL\n      MAR\n      DIS\n      ESP\n      CIT\n      MIG\n      MIL\n      ANC\n      NATIVITY\n      DEAR\n      DEYE\n      DREM\n      SEX\n      RAC1P\n      ESR\n      PINCP\n    \n  \n  \n    \n      0\n      19\n      19.0\n      5\n      2\n      NaN\n      1\n      1.0\n      4.0\n      1\n      1\n      2\n      2\n      2.0\n      2\n      1\n      6.0\n      0.0\n    \n    \n      1\n      18\n      18.0\n      5\n      2\n      NaN\n      1\n      3.0\n      4.0\n      4\n      1\n      2\n      2\n      2.0\n      2\n      1\n      6.0\n      800.0\n    \n    \n      2\n      22\n      16.0\n      5\n      2\n      NaN\n      1\n      3.0\n      1.0\n      2\n      1\n      2\n      2\n      2.0\n      1\n      1\n      4.0\n      35000.0\n    \n    \n      3\n      64\n      16.0\n      5\n      2\n      NaN\n      1\n      1.0\n      4.0\n      1\n      1\n      2\n      2\n      2.0\n      2\n      1\n      6.0\n      0.0\n    \n    \n      4\n      53\n      16.0\n      5\n      2\n      NaN\n      5\n      1.0\n      4.0\n      1\n      2\n      2\n      2\n      2.0\n      1\n      1\n      1.0\n      31200.0\n    \n  \n\n\n\n\nI would like to predict whether an individual’s income is over $50K based on all the variables in possible_features except for sex. I will create a feature matrix, a label vector, and a group label vector using the relevant variables, and split them into training data and test data.\n\nfeatures_to_use = [f for f in possible_features if f not in [\"PINCP\", \"SEX\"]]\n\nIncomeProblem = BasicProblem(\n    features=features_to_use,\n    target='PINCP',\n    target_transform=lambda x: (x > 50000) * 1, # transform income variable into binary category\n    group='SEX',\n    preprocess=lambda x: x,\n    postprocess=lambda x: np.nan_to_num(x, -1),\n)\n\nfeatures, label, group = IncomeProblem.df_to_numpy(acs_data)\n\n# train-test split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n    features, label, group, test_size=0.2, random_state=0)\n\n\n\nBefore training any models, I will first explore the basic descriptives of the dataset.\n\nimport pandas as pd\ndf = pd.DataFrame(X_train, columns = features_to_use)\ndf[\"group\"] = group_train\ndf[\"label\"] = y_train\n\n\nsize = len(df)\nsize\n\n100005\n\n\nThere are 100005 individuals in the dataset.\n\n(df[\"label\"] == 1).sum()/size\n\n0.25900704964751764\n\n\nAround 25.9% of these individuals earn over $50K.\n\ndf.groupby(\"group\").size()\n\ngroup\n1    49135\n2    50870\ndtype: int64\n\n\n49,135 of these individuals are male and 50,870 are female.\n\ndf.groupby(\"group\")[\"label\"].mean().reset_index()\n\n\n\n\n\n  \n    \n      \n      group\n      label\n    \n  \n  \n    \n      0\n      1\n      0.326712\n    \n    \n      1\n      2\n      0.193611\n    \n  \n\n\n\n\nAround 32.7% of male individuals in the dataset has income over 50k dollars, but only 19.4% of women has income over $50k.\n\nimport seaborn as sns\nproportions = df.groupby([\"group\", \"RAC1P\"])[\"label\"].mean().reset_index()\nax = sns.barplot(data = proportions, x = \"RAC1P\", y = \"label\", hue = \"group\")\n\n\n\n\nIt seems that for almost all race groups, the proportion of men with income over $50k is greater than that of women. The only exception is African Americans, for whom this proportion doesn’t differ very much across sex.\n\n\n\n\nI choose to use a decision tree classifier for my prediction. I will use cross-validation to find out the best max depth for the model.\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_val_score\n\ncv_max = 0\nbest_depth = 0\ncvs = []\n\nfor d in range(2, 30):\n    # fit model\n    DTC = DecisionTreeClassifier(max_depth = d)\n    DTC.fit(X_train, y_train)\n\n    # cross-validation\n    cv_scores = cross_val_score(DTC, X_train, y_train, cv = 5)\n    cv_mean = cv_scores.mean()\n    cvs.append(cv_mean)\n\n    # update best cv score and best degree\n    if cv_mean > cv_max:\n        cv_max = cv_mean\n        best_depth = d\n\nprint(\"Best cv score = \", cv_max)\nprint(\"Best max depth = \", best_depth)\n\n# draw a line chart for cv scores at varying degrees\nimport matplotlib.pyplot as plt\nfig = plt.plot(range(2, 30), cvs)\nxlab = plt.xlabel(\"Max depth\")\nylab = plt.ylabel(\"CV score\")\n\nBest cv score =  0.8123893805309734\nBest max depth =  8\n\n\n\n\n\nAs shown by the graph, the best performance is reached at a max depth of 8.\n\n\n\nNext, I will use the model on the test data and perform an audit for bias.\n\n# train model and predict using test data\nDTC = DecisionTreeClassifier(max_depth = best_depth)\nDTC.fit(X_train, y_train)\ny_pred = DTC.predict(X_test)\n\n\n\nFirst, we will take a look at the measures for the whole dataset.\n\nfrom sklearn.metrics import confusion_matrix\n\n# get confusion matrix values\ncm = confusion_matrix(y_test, y_pred)\nTP, FP, FN, TN = cm[0][0], cm[0][1], cm[1][0], cm[1][1]\n\ntest_score = DTC.score(X_test, y_test)\nPPV = TP / (TP + FP)\nFNR = FN / (TP + FN)\nFPR = FP / (TN + FP)\n\nprint(\"Overall accuracy = \", test_score)\nprint(\"PPV = \", PPV)\nprint(\"FNR = \", FNR)\nprint(\"FPR = \", FPR)\n\nOverall accuracy =  0.8155747540196784\nPPV =  0.9173833584175446\nFNR =  0.15262400079439947\nFPR =  0.316190084344785\n\n\nThe overall accuracy and PPV show that the model is quite accurate. The FPR is one time higher than the FPR, suggesting that this model is more likely to wrongly predict that someone earns over $50K when they does not in reality, compared to wrongly predict that someone earns less than 50K.\n\n\n\nThen we will compute the same measures by each group (i.e., male and female) and see if there are any differences.\n\nmale = group_test == 1\nfemale = group_test == 2\n\ncorrect = (y_test == y_pred)\n\naccuracy_male = correct[male].mean()\naccuracy_female = correct[female].mean()\n\naccuracy_male, accuracy_female\n\n(0.7849171180042672, 0.8447253433208489)\n\n\nThe accuracy for male and female is similar, and a little higher for female.\n\n#X_test_df = pd.DataFrame(X_test, columns = [features_to_use])\ncm_male = confusion_matrix(y_test[male],y_pred[male])\ncm_female = confusion_matrix(y_test[female],y_pred[female])\n\n# male\nTP, FP, FN, TN = cm_male[0][0], cm_male[0][1], cm_male[1][0], cm_male[1][1]\nPPV_male = TP / (TP + FP)\nFNR_male = FN / (TP + FN)\nFPR_male = FP / (TN + FP)\n\n# female\nTP, FP, FN, TN = cm_female[0][0], cm_female[0][1], cm_female[1][0], cm_female[1][1]\nPPV_female = TP / (TP + FP)\nFNR_female = FN / (TP + FN)\nFPR_female = FP / (TN + FP)\n\nPPV_male, PPV_female\n\n(0.9252971137521222, 0.9110832206989766)\n\n\nThe PPV for male and female is similar too.\n\nFNR_male, FNR_female\n\n(0.20809548521017124, 0.10175138016371597)\n\n\nThe false negative rate is higher for male than for female. This shows that if a man earns more than $50K, the model is more likely to make a mistake and say he earns less, compared to the same situation for a woman.\n\nFPR_male, FPR_female\n\n(0.24147393179145432, 0.3987012987012987)\n\n\nThe false positive rate is higher for female than for male. This means that the model is more likely to predict that a woman earns more than $50K when she actually earns less.\n\n\n\nFinally, we will calculate the three different bias measures discussed by Chouldechova (2017).\nThe first measure is calibration. Good calibration means that a man and a woman who received the same score (in this case, either 0 or 1) have the same probability of earning over $50K.\n\n# Calibration\nprint(\"% positive for male predicted positive = \", y_test[male][y_pred[male] == 1].mean())\nprint(\"% positive for female predicted positive = \", y_test[female][y_pred[female] == 1].mean())\nprint(\"% positive for male predicted negative = \", y_test[male][y_pred[male] == 0].mean())\nprint(\"% positive for female predicted negative = \", y_test[female][y_pred[female] == 0].mean())\n\n% positive for male predicted positive =  0.7585260682085456\n% positive for female predicted positive =  0.6012987012987013\n% positive for male predicted negative =  0.20809548521017124\n% positive for female predicted negative =  0.10175138016371597\n\n\nThe probabilities are around 0.1-0.15 higher for men than for women. It looks like the model is roughly calibrated, but still, there is a slight difference in what the same score means for the two groups.\n\n\n\n\nIn this section, I train my linear regression model to a bikeshare data set.\n\n# import data\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nbikeshare = pd.read_csv(\"https://philchodrow.github.io/PIC16A/datasets/Bike-Sharing-Dataset/day.csv\")\n\nbikeshare.head()\n\n\n\n\n\n  \n    \n      \n      instant\n      dteday\n      season\n      yr\n      mnth\n      holiday\n      weekday\n      workingday\n      weathersit\n      temp\n      atemp\n      hum\n      windspeed\n      casual\n      registered\n      cnt\n    \n  \n  \n    \n      0\n      1\n      2011-01-01\n      1\n      0\n      1\n      0\n      6\n      0\n      2\n      0.344167\n      0.363625\n      0.805833\n      0.160446\n      331\n      654\n      985\n    \n    \n      1\n      2\n      2011-01-02\n      1\n      0\n      1\n      0\n      0\n      0\n      2\n      0.363478\n      0.353739\n      0.696087\n      0.248539\n      131\n      670\n      801\n    \n    \n      2\n      3\n      2011-01-03\n      1\n      0\n      1\n      0\n      1\n      1\n      1\n      0.196364\n      0.189405\n      0.437273\n      0.248309\n      120\n      1229\n      1349\n    \n    \n      3\n      4\n      2011-01-04\n      1\n      0\n      1\n      0\n      2\n      1\n      1\n      0.200000\n      0.212122\n      0.590435\n      0.160296\n      108\n      1454\n      1562\n    \n    \n      4\n      5\n      2011-01-05\n      1\n      0\n      1\n      0\n      3\n      1\n      1\n      0.226957\n      0.229270\n      0.436957\n      0.186900\n      82\n      1518\n      1600\n    \n  \n\n\n\n\n\n# plot the number of casual users over time\nfig, ax = plt.subplots(1, figsize = (7, 3))\nax.plot(pd.to_datetime(bikeshare['dteday']), bikeshare['casual'])\nax.set(xlabel = \"Day\", ylabel = \"# of casual users\")\nl = plt.tight_layout()\n\n\n\n\n\n# transforming data\ncols = [\"casual\", \n        \"mnth\", \n        \"weathersit\", \n        \"workingday\",\n        \"yr\",\n        \"temp\", \n        \"hum\", \n        \"windspeed\",\n        \"holiday\"]\n\nbikeshare = bikeshare[cols]\n\nbikeshare = pd.get_dummies(bikeshare, columns = ['mnth'], drop_first = \"if_binary\")\nbikeshare\n\n\n\n\n\n  \n    \n      \n      casual\n      weathersit\n      workingday\n      yr\n      temp\n      hum\n      windspeed\n      holiday\n      mnth_2\n      mnth_3\n      mnth_4\n      mnth_5\n      mnth_6\n      mnth_7\n      mnth_8\n      mnth_9\n      mnth_10\n      mnth_11\n      mnth_12\n    \n  \n  \n    \n      0\n      331\n      2\n      0\n      0\n      0.344167\n      0.805833\n      0.160446\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      1\n      131\n      2\n      0\n      0\n      0.363478\n      0.696087\n      0.248539\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      2\n      120\n      1\n      1\n      0\n      0.196364\n      0.437273\n      0.248309\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      3\n      108\n      1\n      1\n      0\n      0.200000\n      0.590435\n      0.160296\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      4\n      82\n      1\n      1\n      0\n      0.226957\n      0.436957\n      0.186900\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      726\n      247\n      2\n      1\n      1\n      0.254167\n      0.652917\n      0.350133\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n    \n    \n      727\n      644\n      2\n      1\n      1\n      0.253333\n      0.590000\n      0.155471\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n    \n    \n      728\n      159\n      2\n      0\n      1\n      0.253333\n      0.752917\n      0.124383\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n    \n    \n      729\n      364\n      1\n      0\n      1\n      0.255833\n      0.483333\n      0.350754\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n    \n    \n      730\n      439\n      2\n      1\n      1\n      0.215833\n      0.577500\n      0.154846\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n    \n  \n\n731 rows × 19 columns\n\n\n\n\n# train-test split\ntrain, test = train_test_split(bikeshare, test_size = .2, shuffle = False)\n\nX_train = train.drop([\"casual\"], axis = 1)\ny_train = train[\"casual\"]\n\nX_test = test.drop([\"casual\"], axis = 1)\ny_test = test[\"casual\"]\n\n\n# fit and score the model\nLR = LinearRegression()\nLR.fit(X_train, y_train)\nLR.score(X_train, y_train)\n\n0.7318355359284503\n\n\nThe model has a score of 0.73.\n\n# compute predictions and visualize in comparison to actual test data\ny_hat = LR.predict(X_test)\n\nplt.plot(np.arange(len(y_hat)), y_hat, label = \"Predictions\")\nplt.plot(np.arange(len(y_hat)), y_test, label = \"Test data\")\n         \nxlab = plt.xlabel(\"Day\")\nylab = plt.ylabel(\"Number of casual users\")\nlegend = plt.legend() \n\n\n\n\nIt seems that the model does a good job in predicting the general trend in the data – the overall decreasing number of users and the timing of peaks. However, it tends to underestimate the minimum and maximum values.\nFinally, I will look into the weight vector and see what it reveals about people’s preference of when to use bikeshare.\n\n# compare weight vector to list of features\nfeature_weights = pd.DataFrame(LR.w[:-1], X_train.columns)\nfeature_weights\n\n\n\n\n\n  \n    \n      \n      0\n    \n  \n  \n    \n      weathersit\n      -108.371136\n    \n    \n      workingday\n      -791.690549\n    \n    \n      yr\n      280.586927\n    \n    \n      temp\n      1498.715113\n    \n    \n      hum\n      -490.100340\n    \n    \n      windspeed\n      -1242.800381\n    \n    \n      holiday\n      -235.879349\n    \n    \n      mnth_2\n      -3.354397\n    \n    \n      mnth_3\n      369.271956\n    \n    \n      mnth_4\n      518.408753\n    \n    \n      mnth_5\n      537.301886\n    \n    \n      mnth_6\n      360.807998\n    \n    \n      mnth_7\n      228.881481\n    \n    \n      mnth_8\n      241.316412\n    \n    \n      mnth_9\n      371.503854\n    \n    \n      mnth_10\n      437.600848\n    \n    \n      mnth_11\n      252.433004\n    \n    \n      mnth_12\n      90.821460\n    \n  \n\n\n\n\nWe can observe a couple of interesting trends from the magnitude and direction of the weights: 1. People use bikeshare more when the weather is nicer – especially when it is warmer and there is less wind. It is also interesting, though, that the weathersit variable is slightly negatively correlated with the number of users. I wonder what exactly this variable is measuring. 2. People use bikeshare the most in April and May. This makes sense because weather is the best during this time of the year, while summer is too hot and winter is too cold. 3. People use bikeshare more on weekends but less on holidays."
  },
  {
    "objectID": "posts/mid-course/mid-course.html",
    "href": "posts/mid-course/mid-course.html",
    "title": "Mid-Course Reflection",
    "section": "",
    "text": "Wen (Diana) Xu\n\n\nIn this section I’ll ask you to fill in some data. You don’t have to give precise numbers – approximate, conversational responses are fine. For example, when I ask “how often have you attended class,” good answers include “almost always,” “I’ve missed three times,” “about 75% of the time,” “not as often as I want,” etc.\n\n\n\nHow often have you attended class? (e.g. “almost always,” “I missed three times,” etc.) I attended all classes.\nHow often have you taken notes on the core readings ahead of the class period? About 2/3 of the times.\nHow often have you been prepared to present the daily warm-up exercise to your team, even if you weren’t actually called? I have been prepared for all warmup activities.\nHow many times have you actually presented the daily warm-up to your team? I don’t remember exactly, but I presented every time when I was picked.\nHow many times have you asked your team for help while presenting the daily warm-up? About 2-3 times.\nHow often have you learned something new from a teammate’s presentation of the daily warm-up? I learned something new for most of the times.\nHow often have you helped a teammate during the daily warm-up presentation? One or two times.\n\n\n\n\n\nHow often have you attended Student Hours or Peer Help? I have attended student hours or peer help more often at the beginning of the semester, about once a week. I haven’t been attending for the past few weeks, though.\nHow often have you asked for or received help from your fellow students? A few times a week.\nHave you been regularly participating in a study group outside class? Yes, about once a week.\nHow often have you posted questions or answers in Slack? I haven’t posted questions or answers in Slack.\n\n\n\n\n\nHow many blog posts have you submitted? I have submitted four blogposts.\nHow many of your submitted blog posts are at each of the following feedback stages?\n\nNo revisions suggested: 4\nRevisions useful: 0\nRevisions encouraged: 0\nIncomplete: 0\n\nRoughly how many hours per week have you spent on this course outside of class? About 6-10 hours per week.\n\n\n\n\n\nAt the beginning of the course, you may have expressed an interest in focusing a little extra on one or two of the following four categories:\n\nTheory: mathematical descriptions of frameworks and algorithms.\nImplementation: effective coding and use of tools in order to implement efficient machine learning algorithms.\nExperimentation: performing experiments to assess the performance of algorithms and clearly communicating about the results.\nSocial responsibility: critical analysis of sources of bias and harm in machine learning algorithms; theoretical formulations of fairness and bias\n\nDid you choose to focus on any of these categories? If so, what have you done in order to pursue your interest?\nI have chosen to focus on experimentation and social responsibility. For experimentation, I have put more effort into experimentation sections in the blogposts, such as experimenting the linear regression model on the bikeshare dataset, and the penguin classification blogpost. I have thought carefully about the experimentation results and written down my interpretations or raised additional questions regarding the process. I also tried to create my writings and graphs in a way that communicates insights clearly to potential readers.\nFor social responsibility, I have read and thought about the readings on algorithmic bias carefully, and am applying what I have learned from these readings and our classes to a real-world dataset in the auditing allocative bias blogpost.\n\n\n\nFor each of the categories below, replace the “[your response here]” cell with 1-2 paragraphs in which you reflect on the following questions:\n\nIn what ways are you on track to meet your goals from the beginning of the course? Be specific: explain what the goal is and what you are doing in order to meet it.\nIn what ways are you not on track to meet your goals from the beginning of the course? Be specific: explain what the goal is and what gap you see between where you are and your goal.\nIf there’s any context you want to share about how you are faring relative to your goals, please do!\n\n\n\nI am on track for the goals of submitting a blog post in most weeks during the semester, and submitting the first draft of no more than two blog post after the “best-by” date. I submitted a blog post each week except for Week 5 and Week 6, and all of them before the “best-by” date. I am also working on the blog post for auditing bias. I am also in good progress for the goal of having at least five blog posts at the “No revisions suggested” level because I currently have four.\nI am a little behind on my last goal of “Go above and beyond in at least two blog posts by performing experiments/exploring visualization choices to communicate results/discussing implications significantly beyond what is required.” I have gone beyond in the penguins blog post by discussing some questions I had about the different performance of different models, but I wished I did more, especially for coming up with my own experimentations and trying new visualization approaches to communicate my results. I think that I haven’t been able to put enough time and effort on writing a blog post significantly beyond the requirements because I have been rushing to get them done before the best-by dates.\n\n\n\nI am on track for my goals on preparation before class. For example, I did complete all core and optional readings before class, and prepare for all warmup activities. I have also been working often with classmates after class as I stated in my goals.\nFor my last goal of often attending student hours or peer help, I am behind since I haven’t attended student hours or peer help sessions recently. I felt that I didn’t have emergent questions that I needed to discuss in these sessions, and sometimes they did not fit with my schedule.\n\n\n\nWe have just started the project, but I think we are on track for submitting our proposal on time.\n\n\n\nIs there anything else that you want to share with me about what you have learned, how you have participated, or what you have achieved in CSCI 0451?\n\n\n\nFrom your experience in CSCI 0451 and your other classes this semester, you may feel moved to make modifications to your goals. Are they still feasible? Too ambitious? Not ambitious enough? If you would like to revise any of your goals from your reflective goal-setting, you can do so below. For each goal you want to modify:\n\nClearly state what the goal was.\nClearly state how you’ve done on that goal so far.\nClearly state your proposed revised goal for the remainder of the course.\n\nI would like to revise my goal of often attending student hours and peer help. I have attended student hours or peer help regularly during the first few weeks of the semester, but haven’t attended much recently. I wanted to modify this goal to “Attend student hours and peer help when I have questions that are not resolved by self effort and discussion with classmates,” because I feel that it is often more efficient for me to find time on my own or talk to classmates to work on questions.\n\n\n\n\nTake 15 minutes to look back on your responses in each of the sections above. Then, state the letter grade that you feel reflects your learning, participation, and achievement in CSCI 0451 so far. Here are some soundbytes to help guide your thinking:\n\n\nAn A sounds like:\n\n“I am very proud of my time in this course.”\n“I have grown significantly in multiple ways that matter to me.”\n“I am ready to take the theory, techniques, and ideas of this course into my future classes, projects, hobbies, or career.”\n\nA B sounds like:\n\n“I had some opportunities to learn more, overall I feel good about my time in this course.”\n“I am able to explain some new things or achieve new tasks.”\n“I can see a few ideas from this course that will be relevant for my future classes, projects, hobbies, or career.”\n\nA C sounds like:\n\n“I often made a good effort, but I missed many opportunities to get more out of my time in this course.”\n“I might be able to complete some new tasks related to the course content, but only with significant further guidance.”\n“I don’t see any ways to take the contents of this course into my future classes, projects, hobbies, or career.”\n\nYou might find that some of these soundbytes resonate and other’s don’t! Take some time, see what feels right, and don’t be afraid to celebrate your achievements.\n\nUpon reflection, I feel that my learning, participation, and achievement in CSCI 0451 (so far) are best reflected by a grade of A-\n\n\n\n\n\nYou may feel disappointed by your reflection. Sometimes we don’t achieve all our goals – it happens and it’s normal! If you are feeling disappointed by how you’ve learned, participated, or achieved in CSCI 0451, then feel free to write something about that below. Feel free to just write your feelings. If you have ideas for how to move forward, include those too! We’ll talk."
  }
]