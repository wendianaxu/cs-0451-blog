{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Learning from Timnit Gebru\"\n",
    "type: \"Blogpost\"\n",
    "date: 2023-04-19\n",
    "description: |\n",
    "    What I have learned from Dr. Timnit Gebru's talk and visit. \n",
    "publish: \"true\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning from Timnit Gebru"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dr. Timnit Gebru is a renowned computer scientist and researcher on algorithmic bias in artificial intelligence (AI). She has contributed immensely to the field of fairness and ethics of AI. She has worked on a project called Gender Shades, where she and colleagues found that common facial recognition algorithms were much less accurate in identifying the faces of Black women compared to White men. During her time on the AI ethics team at Google, she co-authored a paper on the ethics of large language models, pointing out how they can be susceptible to bias. She was reportedly fired by Google after refusing to retract the paper by their request. Later, she founded the Distributed Artificial Intelligence Research Institute (DAIR), with the goal of studying how AI affects marginalized groups in the US. Dr. Gebru’s research and advocacy have led the AI community to look into the fairness and ethical issues arising from the field and have significantly shaped the discussions on such issues. Next Monday, Dr. Gebru will be giving a talk titled *Eugenics and the Promise of Utopia through Artificial General Intelligence* at Middlebury, and will also be virtually visiting the CS 0451 class. This is a valuable opportunity for students to learn about her work and engage in conversations with her. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Talk on Computer Vision\n",
    "In her talk on the ethics of computer graphics, Dr. Gebru discussed many ways in which current computer graphics technologies can reflect and exacerbate existing structural inequalities. She used facial recognition algorithms as a primary example, which is currently used in many fields from emotion detection in job interviews to policing systems. Dr. Gebru illustrated **two main problems** in the ethics of facial recognition technologies: how their algorithms are biased in themselves, and how they are used in ways that harm marginalized groups. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gender Shades is an example of algorithmic biases: because of the lack of diversity in data sets, facial recognition algorithms are much more accurate in identifying White men than Black women. Similar problems exist in the broader field of computer vision, where classification systems are built on social and cultural norms of the Western world. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is more alarming is probably where these computer vision are used, and why they are used in the first place. For example, why do we need binary gender classification systems? Why does the police need to use facial recognition to identify protesters? Dr. Gebru highlighted that for most of the time, computer vision systems are developed and used by people who are not from the marginalized groups that are the most vulnerable to the negative impacts of these systems. This amplifies the existing power imbalance. Therefore, when we talk about diversity and representation in data-driven systems, it is not enough to just include more “diverse” datasets – we need diversity in the humans behind these systems. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tl;dr \n",
    "We need to be cautious about how computer vision technologies are developed and used because they can easily amplify existing structural inequalities by disproportionately harming the most marginalized groups. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proposed Question for Dr. Gebru\n",
    "In her talk, Dr. Gebru mentioned the problem with abstraction: algorithms reduce people into mathematical equations and this is where many ethical problems arise. This makes me think about how any algorithm involves some level of abstraction and cannot capture every piece of nuanced information present in the real world. I am curious about Dr. Gebru’s thoughts on where we should draw the line between what abstraction is acceptable and what abstraction would harm the people that it is representing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Reflection Dr. Gebru's Talk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dr. Gebru’s talk presents a critical perspective on the development of artificial general intelligence (AGI) by large corporations today, who are contributing to a hype for an omnipotent model and racing to build it. Dr. Gebru introduced the concept of \"TESCREALists,\" groups of people who believe that AGI can radically enhance human capabilities and thoroughly transform the society. Members of these groups include influential and wealthy people who provide tremendous resources for the development of AGI. \n",
    "\n",
    "Dr. Gebru argued that the TESCREALists have close connections with the idea of eugenics. They represent the second wave of eugenics, which supports using technologies such as genetic engineering to select for “desirable” traits in humans. These traits often refer to intelligence, defined and measured by a restricted mainstream standard. Dr. Gebru argued that this is a dangerous direction to take because it can further marginalize groups that do not fit in the standards created by those in power. \n",
    "\n",
    "Dr. Gebru also discussed a common narrative that TESCREALists talk about around AGI – how AGI has such great power that it will lead to either a utopia or an apocalypse depending on how it is handled. They seem to attempt to convince people that AGI will be used properly to improve public welfare only when they are in charge of creating and managing it. Dr. Gebru questioned this narrative from a few perspectives. First, the idea that an all-knowing model will be able to solve any problem and benefit everyone in the world does not sound realistic. The argument that AGI will make the world a more equitable place, in particular, is not convincing given how companies are exploiting labor in underdeveloped regions to keep large AI models running. Another concern that Dr. Gebru brought up was that the apocalyptic framing of AGI can allow these companies to shift people's attention from current problems caused by their practices, such as data theft and labor exploitation, that are harming people right now, to a potential apocalypse that might happen in the future. Finally, Dr. Gebru argued that a much more rational strategy is to develop smaller models that specialize on specific fields, rather than putting all resources into one large model controlled by a few powerful organizations. This will allow people to benefit from AI that speak to their specific needs and do the job better because they are developed by people who are more specialized in their field. \n",
    "\n",
    "Dr. Gebru’s talk helped me to think about the potential consequences of creating such a powerful technology of AGI from a perspective that I haven’t considered before. I very much agree with her call for drawing more attention to the unethical practices behind current AI models than the potential existential crisis that a powerful AGI may pose to humankind. I feel that a large part of public discussion around AI has been focusing on the latter and fewer people are aware of how many AI systems that are in use today are already harming marginalized groups. I think that this is the most important idea that people should learn from Dr. Gebru’s talk because everyone should be more aware of the impacts of AI systems that many of us use frequently. I also agree that centralized control of large AI systems is a dangerous direction. What I had a harder time following was the arguments on the connections between the TESCREAL communities and eugenics. I think that more evidence is needed to justify equating all people in these groups to the type of “eugenists” that she was accusing of. But I do agree that many of the beliefs held by these groups are worth questioning and cautioning. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Reflection on the Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the experience of interacting with Dr. Gebru and her work, I feel that I have developed a much deeper understanding of the ethical implications of AI and have come to realize both the importance of and challenges in grappling with these issues. I find it inspiring to engage in critical perspectives of the ethics of AI, and I think that I will be able to apply these perspectives to thinking about the implications of other powerful technologies. What I am frustrated about is that these issues seem challenging to resolve because of power dynamics in the field of AI. I also feel that the debate around this topic is very polarized and both sides have quite extreme views. This makes me worried that it may deviate from a discussion where both sides actually listen to each others’ arguments, and make it harder to generate insights for improving the field. Overall, however, I feel empowered that we get to hear voices from scientists like Dr. Gebru to stand up for marginalized groups and push for fairer science practices. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "debe06cc0f9553f110b64dc3926c05df82dae2145b852c8422b9c04315589dcb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
