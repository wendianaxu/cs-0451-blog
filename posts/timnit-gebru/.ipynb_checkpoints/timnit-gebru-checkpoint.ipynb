{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Learning from Timnit Gebru\"\n",
    "type: \"Blogpost\"\n",
    "date: 2023-04-19\n",
    "description: |\n",
    "    What I have learned from Dr. Timnit Gebru's talk and visit. \n",
    "publish: \"true\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning from Timnit Gebru"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dr. Timnit Gebru is a renowned computer scientist and researcher on algorithmic bias in artificial intelligence (AI). She has contributed immensely to the field of fairness and ethics of AI. She has worked on a project called Gender Shades, where she and colleagues found that common facial recognition algorithms were much less accurate in identifying the faces of Black women compared to White men. During her time on the AI ethics team at Google, she co-authored a paper on the ethics of large language models, pointing out how they can be susceptible to bias. She was reportedly fired by Google after refusing to retract the paper by their request. Later, she founded the Distributed Artificial Intelligence Research Institute (DAIR), with the goal of studying how AI affects marginalized groups in the US. Dr. Gebru’s research and advocacy have led the AI community to look into the fairness and ethical issues arising from the field and have significantly shaped the discussions on such issues. Next Monday, Dr. Gebru will be giving a talk titled *Eugenics and the Promise of Utopia through Artificial General Intelligence* at Middlebury, and will also be virtually visiting the CS 0451 class. This is a valuable opportunity for students to learn about her work and engage in conversations with her. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Talk on Computer Vision\n",
    "In her talk on the ethics of computer graphics, Dr. Gebru discussed many ways in which current computer graphics technologies can reflect and exacerbate existing structural inequalities. She used facial recognition algorithms as a primary example, which is currently used in many fields from emotion detection in job interviews to policing systems. Dr. Gebru illustrated **two main problems** in the ethics of facial recognition technologies: how their algorithms are biased in themselves, and how they are used in ways that harm marginalized groups. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gender Shades is an example of algorithmic biases: because of the lack of diversity in data sets, facial recognition algorithms are much more accurate in identifying White men than Black women. Similar problems exist in the broader field of computer vision, where classification systems are built on social and cultural norms of the Western world. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is more alarming is probably where these computer vision are used, and why they are used in the first place. For example, why do we need binary gender classification systems? Why does the police need to use facial recognition to identify protesters? Dr. Gebru highlighted that for most of the time, computer vision systems are developed and used by people who are not from the marginalized groups that are the most vulnerable to the negative impacts of these systems. This amplifies the existing power imbalance. Therefore, when we talk about diversity and representation in data-driven systems, it is not enough to just include more “diverse” datasets – we need diversity in the humans behind these systems. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tl;dr \n",
    "We need to be cautious about how computer vision technologies are developed and used because they can easily amplify existing structural inequalities by disproportionately harming the most marginalized groups. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proposed Question for Dr. Gebru\n",
    "In her talk, Dr. Gebru mentioned the problem with abstraction: algorithms reduce people into mathematical equations and this is where many ethical problems arise. This makes me think about how any algorithm involves some level of abstraction and cannot capture every piece of nuanced information present in the real world. I am curious about Dr. Gebru’s thoughts on where we should draw the line between what abstraction is acceptable and what abstraction would harm the people that it is representing. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "debe06cc0f9553f110b64dc3926c05df82dae2145b852c8422b9c04315589dcb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
